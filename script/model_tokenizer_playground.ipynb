{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VISIBLE_CUDA_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9430d94ec5f14db380f8d6b568347b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model mistralai/Mistral-7B-v0.1 and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device='cuda:0')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased size: 7241.7M parameters\n",
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|            model.embed_tokens.weight            | 131072000  |\n",
      "|      model.layers.0.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.0.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.0.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.0.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.0.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.0.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.0.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.0.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.0.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.1.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.1.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.1.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.1.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.1.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.1.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.1.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.1.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.1.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.2.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.2.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.2.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.2.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.2.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.2.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.2.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.2.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.2.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.3.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.3.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.3.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.3.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.3.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.3.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.3.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.3.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.3.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.4.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.4.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.4.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.4.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.4.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.4.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.4.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.4.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.4.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.5.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.5.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.5.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.5.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.5.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.5.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.5.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.5.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.5.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.6.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.6.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.6.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.6.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.6.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.6.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.6.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.6.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.6.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.7.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.7.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.7.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.7.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.7.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.7.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.7.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.7.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.7.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.8.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.8.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.8.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.8.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.8.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.8.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.8.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.8.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.8.post_attention_layernorm.weight |    4096    |\n",
      "|      model.layers.9.self_attn.q_proj.weight     |  16777216  |\n",
      "|      model.layers.9.self_attn.k_proj.weight     |  4194304   |\n",
      "|      model.layers.9.self_attn.v_proj.weight     |  4194304   |\n",
      "|      model.layers.9.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.9.mlp.gate_proj.weight       |  58720256  |\n",
      "|        model.layers.9.mlp.up_proj.weight        |  58720256  |\n",
      "|       model.layers.9.mlp.down_proj.weight       |  58720256  |\n",
      "|      model.layers.9.input_layernorm.weight      |    4096    |\n",
      "|  model.layers.9.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.10.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.10.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.10.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.10.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.10.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.10.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.10.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.10.input_layernorm.weight     |    4096    |\n",
      "| model.layers.10.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.11.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.11.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.11.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.11.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.11.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.11.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.11.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.11.input_layernorm.weight     |    4096    |\n",
      "| model.layers.11.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.12.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.12.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.12.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.12.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.12.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.12.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.12.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.12.input_layernorm.weight     |    4096    |\n",
      "| model.layers.12.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.13.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.13.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.13.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.13.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.13.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.13.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.13.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.13.input_layernorm.weight     |    4096    |\n",
      "| model.layers.13.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.14.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.14.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.14.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.14.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.14.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.14.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.14.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.14.input_layernorm.weight     |    4096    |\n",
      "| model.layers.14.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.15.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.15.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.15.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.15.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.15.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.15.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.15.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.15.input_layernorm.weight     |    4096    |\n",
      "| model.layers.15.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.16.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.16.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.16.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.16.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.16.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.16.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.16.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.16.input_layernorm.weight     |    4096    |\n",
      "| model.layers.16.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.17.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.17.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.17.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.17.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.17.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.17.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.17.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.17.input_layernorm.weight     |    4096    |\n",
      "| model.layers.17.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.18.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.18.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.18.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.18.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.18.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.18.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.18.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.18.input_layernorm.weight     |    4096    |\n",
      "| model.layers.18.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.19.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.19.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.19.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.19.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.19.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.19.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.19.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.19.input_layernorm.weight     |    4096    |\n",
      "| model.layers.19.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.20.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.20.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.20.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.20.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.20.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.20.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.20.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.20.input_layernorm.weight     |    4096    |\n",
      "| model.layers.20.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.21.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.21.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.21.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.21.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.21.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.21.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.21.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.21.input_layernorm.weight     |    4096    |\n",
      "| model.layers.21.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.22.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.22.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.22.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.22.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.22.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.22.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.22.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.22.input_layernorm.weight     |    4096    |\n",
      "| model.layers.22.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.23.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.23.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.23.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.23.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.23.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.23.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.23.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.23.input_layernorm.weight     |    4096    |\n",
      "| model.layers.23.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.24.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.24.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.24.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.24.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.24.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.24.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.24.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.24.input_layernorm.weight     |    4096    |\n",
      "| model.layers.24.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.25.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.25.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.25.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.25.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.25.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.25.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.25.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.25.input_layernorm.weight     |    4096    |\n",
      "| model.layers.25.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.26.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.26.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.26.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.26.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.26.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.26.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.26.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.26.input_layernorm.weight     |    4096    |\n",
      "| model.layers.26.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.27.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.27.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.27.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.27.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.27.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.27.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.27.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.27.input_layernorm.weight     |    4096    |\n",
      "| model.layers.27.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.28.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.28.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.28.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.28.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.28.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.28.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.28.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.28.input_layernorm.weight     |    4096    |\n",
      "| model.layers.28.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.29.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.29.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.29.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.29.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.29.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.29.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.29.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.29.input_layernorm.weight     |    4096    |\n",
      "| model.layers.29.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.30.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.30.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.30.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.30.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.30.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.30.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.30.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.30.input_layernorm.weight     |    4096    |\n",
      "| model.layers.30.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.31.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.31.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.31.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.31.self_attn.o_proj.weight     |  16777216  |\n",
      "|       model.layers.31.mlp.gate_proj.weight      |  58720256  |\n",
      "|        model.layers.31.mlp.up_proj.weight       |  58720256  |\n",
      "|       model.layers.31.mlp.down_proj.weight      |  58720256  |\n",
      "|      model.layers.31.input_layernorm.weight     |    4096    |\n",
      "| model.layers.31.post_attention_layernorm.weight |    4096    |\n",
      "|                model.norm.weight                |    4096    |\n",
      "|                  lm_head.weight                 | 131072000  |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 7241732096\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7241732096"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "def model_size_and_parameters(model):\n",
    "    # Create a PrettyTable for displaying module-wise parameter information\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "\n",
    "    # Calculate the total number of parameters in the model\n",
    "    model_size = sum(t.numel() for t in model.parameters())\n",
    "\n",
    "    # Print the total size of the model in megabytes\n",
    "    print(f\"bert-base-uncased size: {model_size/1000**2:.1f}M parameters\")\n",
    "\n",
    "    # Initialize a variable to keep track of the total trainable parameters\n",
    "    total_params = 0\n",
    "\n",
    "    # Iterate through named parameters of the model\n",
    "    for name, parameter in model.named_parameters():\n",
    "        # Check if the parameter requires gradient (i.e., is trainable)\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "\n",
    "        # Get the number of parameters in the current module\n",
    "        params = parameter.numel()\n",
    "\n",
    "        # Add a row to the PrettyTable with module name and number of parameters\n",
    "        table.add_row([name, params])\n",
    "\n",
    "        # Increment the total trainable parameters\n",
    "        total_params += params\n",
    "\n",
    "    # Print the PrettyTable with module-wise parameter information\n",
    "    print(table)\n",
    "\n",
    "    # Print the total number of trainable parameters in the model\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "    # Return the total number of trainable parameters\n",
    "    return total_params\n",
    "\n",
    "model_size_and_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading mistral model with custom config from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1282052096"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "from transformers import MistralConfig\n",
    "ModelParam_M,D_emb,Vocal,D_Head,d_FF,N_Layer,N_Head,KV_Head,Window,GPU_use_MB = 1607.8,4096,50000,128,14336,4,16,4,4096,6532.25\n",
    "custom_config = MistralConfig(\n",
    "        vocab_size=Vocal,\n",
    "        hidden_size=D_emb,\n",
    "        intermediate_size=d_FF,\n",
    "        num_hidden_layers=N_Layer,\n",
    "        num_attention_heads=N_Head,\n",
    "        num_key_value_heads=KV_Head,\n",
    "        hidden_act=\"silu\",\n",
    "        max_position_embeddings=4096 * 32,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        use_cache=True,\n",
    "        pad_token_id=None,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,\n",
    "        sliding_window=Window,\n",
    "        attention_dropout=0.0,\n",
    "    )\n",
    "\n",
    "model_custom = AutoModelForCausalLM.from_config(custom_config)\n",
    "model_custom.num_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MistralForCausalLM, MistralConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_emb,Vocal,d_head,d_FF,N_Layer,N_Head,KV_Head,Window = 4096,50000,128,14336,2,32,8,8192\n",
    "\n",
    "custom_config = MistralConfig(\n",
    "        vocab_size=Vocal,\n",
    "        hidden_size=D_emb,\n",
    "        intermediate_size=d_FF,\n",
    "        num_hidden_layers=N_Layer,\n",
    "        num_attention_heads=N_Head,\n",
    "        num_key_value_heads=KV_Head,\n",
    "        hidden_act=\"silu\",\n",
    "        max_position_embeddings=4096 * 32,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        use_cache=True,\n",
    "        pad_token_id=None,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,\n",
    "        sliding_window=Window,\n",
    "        attention_dropout=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralForCausalLM(custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845.828096"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()/1000**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31dc64451de46179c575993294c02b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8525267c3b904727bd958153e1d89af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a830e84f0324c948182fe7c6232d1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['VISIBLE_CUDA_DEVICES'] = '1'\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "# load model mistralai/Mistral-7B-v0.1 and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-125m\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of PreTrainedTokenizerFast(name_or_path='facebook/galactica-125m', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[START_REF]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[END_REF]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[IMAGE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t7: AddedToken(\"<fragments>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"</fragments>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t9: AddedToken(\"<work>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t10: AddedToken(\"</work>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t11: AddedToken(\"[START_SUP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t12: AddedToken(\"[END_SUP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t13: AddedToken(\"[START_SUB]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t14: AddedToken(\"[END_SUB]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t15: AddedToken(\"[START_DNA]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t16: AddedToken(\"[END_DNA]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t17: AddedToken(\"[START_AMINO]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t18: AddedToken(\"[END_AMINO]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t19: AddedToken(\"[START_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t20: AddedToken(\"[END_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t21: AddedToken(\"[START_I_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t22: AddedToken(\"[END_I_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 0,\n",
       " '<pad>': 1,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " '[START_REF]': 4,\n",
       " '[END_REF]': 5,\n",
       " '[IMAGE]': 6,\n",
       " '<fragments>': 7,\n",
       " '</fragments>': 8,\n",
       " '<work>': 9,\n",
       " '</work>': 10,\n",
       " '[START_SUP]': 11,\n",
       " '[END_SUP]': 12,\n",
       " '[START_SUB]': 13,\n",
       " '[END_SUB]': 14,\n",
       " '[START_DNA]': 15,\n",
       " '[END_DNA]': 16,\n",
       " '[START_AMINO]': 17,\n",
       " '[END_AMINO]': 18,\n",
       " '[START_SMILES]': 19,\n",
       " '[END_SMILES]': 20,\n",
       " '[START_I_SMILES]': 21,\n",
       " '[END_I_SMILES]': 22}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the special tokens\n",
    "tokenizer.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 4: AddedToken(\"[START_REF]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 5: AddedToken(\"[END_REF]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 6: AddedToken(\"[IMAGE]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 7: AddedToken(\"<fragments>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 8: AddedToken(\"</fragments>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 9: AddedToken(\"<work>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 10: AddedToken(\"</work>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 11: AddedToken(\"[START_SUP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 12: AddedToken(\"[END_SUP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 13: AddedToken(\"[START_SUB]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 14: AddedToken(\"[END_SUB]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 15: AddedToken(\"[START_DNA]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 16: AddedToken(\"[END_DNA]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 17: AddedToken(\"[START_AMINO]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 18: AddedToken(\"[END_AMINO]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 19: AddedToken(\"[START_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 20: AddedToken(\"[END_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 21: AddedToken(\"[START_I_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 22: AddedToken(\"[END_I_SMILES]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_2.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m = AutoTokenizer.from_pretrained('/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_2.0%_50000_new')\n",
    "tokenizer_m.get_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>\\x00\\x01\\x02\\x03\\x04\\x05\\x06\\x07\\x08\\t\\n\\x0b\\x0c\\x0e\\x0f']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m.batch_decode([[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[UNK] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m2 = AutoTokenizer.from_pretrained('/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new')\n",
    "tokenizer_m2.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[CLS]': 1,\n",
       " '[SEP]': 2,\n",
       " '[MASK]': 3,\n",
       " '[UNK] ': 4,\n",
       " '[BOS]': 5,\n",
       " '[EOS]': 6}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m2.added_tokens_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 4: AddedToken(\"[UNK] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " 6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m2.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m2.pad_token = tokenizer_m2.added_tokens_decoder[0]\n",
    "tokenizer_m2.cls_token = tokenizer_m2.added_tokens_decoder[1]\n",
    "tokenizer_m2.sep_token = tokenizer_m2.added_tokens_decoder[2]\n",
    "tokenizer_m2.mask_token = tokenizer_m2.added_tokens_decoder[3]\n",
    "tokenizer_m2.unk_token = tokenizer_m2.added_tokens_decoder[4]\n",
    "tokenizer_m2.bos_token = tokenizer_m2.added_tokens_decoder[5]\n",
    "tokenizer_m2.eos_token = tokenizer_m2.added_tokens_decoder[6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_added_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK] ', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[UNK] \", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m2.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m3 = AutoTokenizer.from_pretrained('/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new')\n",
    "tokenizer_m3.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_added_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m3.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_m3.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_m3.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_m3.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_added_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[BOS]', 'eos_token': '[EOS]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"[BOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"[EOS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m3.add_special_tokens({'pad_token': '[PAD]',\n",
    "                                                'cls_token': '[CLS]',\n",
    "                                                'sep_token': '[SEP]',\n",
    "                                                'mask_token': '[MASK]',\n",
    "                                                'unk_token': '[UNK]',\n",
    "                                                'bos_token': '[BOS]',\n",
    "                                                'eos_token': '[EOS]'})\n",
    "tokenizer_m3.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m3.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m4 = AutoTokenizer.from_pretrained('/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new')\n",
    "tokenizer_m4.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_m4.bos_token = \"<bos>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_added_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new', vocab_size=50000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<bos>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m4.get_added_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerFast.get_vocab of PreTrainedTokenizerFast(name_or_path='/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_0.002%_50000_new', vocab_size=15505, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<sep>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_m5 = AutoTokenizer.from_pretrained('/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_0.002%_50000_new')\n",
    "tokenizer_m5.get_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_m5.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading model on multiple GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-30 18:19:22.902259: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-30 18:19:22.942774: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-30 18:19:22.942811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-30 18:19:22.944233: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-30 18:19:22.951425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-30 18:19:23.791072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer\n",
      "Loading and preparing dataset...\n",
      "loading sample dataset of size  100\n",
      "size of dataframe in MB:  6.497421\n",
      "Train dataset size:  85\n",
      "Validation dataset size:  10\n",
      "Train dataset columns:  Index(['text'], dtype='object')\n",
      "Validation dataset columns:  Index(['text'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9273fc689e634b65bdda835b1a7b0619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/85 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by mode='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd54343c8e741f6880917fe7e592158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/home/dosisiddhesh/miniconda3/lib/python3.11/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "device = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0,1,2,3'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]='1'\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from evaluate import load\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForLanguageModeling, \n",
    "    EarlyStoppingCallback, \n",
    "#    WandbCallback,\n",
    ")\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "isf16 = False\n",
    "\n",
    "\n",
    "# metric = load(\"perplexity\")\n",
    "code_path = \"/home/dosisiddhesh/MISTRAL_EXP/mistral-src\"\n",
    "data_path = \"/home/dosisiddhesh/MISTRAL_EXP/data/latex.csv\"\n",
    "model_path = Path(\"/home/dosisiddhesh/MISTRAL_EXP/model/mistral-7B-v0.1\")  # model and tokenizer location\n",
    "# tokenizer_path_sentence_piece_for_mistral_src = '/home/dosisiddhesh/MISTRAL_EXP/model/tokenizer_5.0%_50000_new.model'\n",
    "# tokenizer_path_hf_debertv2 = \"/home/dosisiddhesh/MISTRAL_EXP/model/tokenizer_5.0%_50000_hf.model\"\n",
    "# tokenizer_path_llama = \"hf-internal-testing/llama-tokenizer\" #llama\n",
    "tokenizer_path_hf_our = '/home/dosisiddhesh/MISTRAL_EXP/model/hf_tokenizer_4.0%_50000_new'\n",
    "\n",
    "\n",
    "sys.path.append(code_path)  # append the path where mistral-src was cloned\n",
    "from mistral.tokenizer import Tokenizer\n",
    "from mistral.model import Transformer, ModelArgs\n",
    "from training_utils import Parameter, MyModel, Dataset_Preprocessing, HyperParams\n",
    "#__________________________________________________________________________________________________\n",
    "# +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
    "D_emb = 4096\n",
    "Vocal = 50000\n",
    "d_head = 128\n",
    "d_FF = 7168 #14336\n",
    "N_Layer = 4\n",
    "N_Head = 32\n",
    "KV_Head = 8\n",
    "Window = 4096 #8192\n",
    "data_row = 100\n",
    "value = [D_emb,Vocal,d_head,d_FF,N_Layer,N_Head,KV_Head,Window]\n",
    "#**************************************************************************************************\n",
    "param = Parameter(\"Mistral\", value)\n",
    "hp = HyperParams(\n",
    "    epoch=1, \n",
    "    learning_rate=6e-4, \n",
    "    model_id=\"mistral/dummy\",\n",
    "    weight_decay=0.1,  \n",
    "    warmup_steps=50,\n",
    "    lr_scheduler_type=\"linear\", #['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', 'inverse_sqrt', 'reduce_lr_on_plateau']\n",
    "    BATCH_SIZE=8,\n",
    "    tokenizer_batch_size=16,\n",
    "    eval_steps=50, # Adjust as needed1\n",
    "    logging_steps=50,  # Adjust as needed\n",
    "    save_steps=200,\n",
    "    save_total_limit = 1,\n",
    "    max_seq_length=int(1024*2),\n",
    ")\n",
    "#__________________________________________________________________________________________________\n",
    "# +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
    "\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()\n",
    "# os.environ[\"WANDB_PROJECT\"]=\"Misral\"\n",
    "# WANDB_PROJECT=\"Misral_sci_tex\"\n",
    "# wandb_run_name = \"dummy\"\n",
    "\n",
    "#____________________________________________________________________________________________________________________________\n",
    "# In[]: GPU stats ***********************************************************************************************************\n",
    "from pynvml import *\n",
    "\n",
    "# def print_gpu_utilization():\n",
    "#     nvmlInit()\n",
    "#     handle = nvmlDeviceGetHandleByIndex(int(device))\n",
    "#     info = nvmlDeviceGetMemoryInfo(handle)\n",
    "#     print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "# ___________________________________________________________________________________________________________________________\n",
    "# In[]: preparing the dataset ***********************************************************************************************\n",
    "dataset_obj = Dataset_Preprocessing(data_path, dataset_batch_size=hp.tokenizer_batch_size, max_seq_length=hp.max_seq_length)\n",
    "print(\"Loading tokenizer\")\n",
    "# tokenizer = dataset_obj.load_tokenizer(tok_type=\"mistral_src\", tokenizer_path=tokenizer_path_sentence_piece_for_mistral_src)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "# if not os.path.exists(tokenizer_path_hf_debertv2):\n",
    "#     tokenizer_deberta = DebertaV2Tokenizer(\n",
    "#         vocab_file  = '/home/dosisiddhesh/MISTRAL_EXP/model/tokenizer_5.0%_50000_new.model',\n",
    "#         # max_len = 512,\n",
    "#     )\n",
    "#     tokenizer_deberta.save_pretrained(tokenizer_path)\n",
    "# tokenizer = dataset_obj.load_tokenizer(tok_type=\"debertaV2\", tokenizer_path=tokenizer_path_hf_debertv2)\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "tokenizer = dataset_obj.load_tokenizer(tok_type=\"hf\", tokenizer_path=tokenizer_path_hf_our)\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]',\n",
    "#                               'unk_token': '[UNK]',\n",
    "#                               'mask_token': '[MASK]',\n",
    "#                               'cls_token': '[CLS]',\n",
    "#                               'sep_token': '[SEP]',\n",
    "#                               'bos_token': '[BOS]',\n",
    "#                               'eos_token': '[EOS]',\n",
    "#                               })\n",
    "\n",
    "# print(\"Tokenizer special tokens:\", tokenizer.special_tokens_map)\n",
    "# print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "# print(\"Tokenizer bos token:\", tokenizer.bos_token)\n",
    "# print(\"Tokenizer eos token:\", tokenizer.eos_token)\n",
    "# print(\"Tokenizer pad token:\", tokenizer.pad_token)\n",
    "# print(\"Tokenizer unk token:\", tokenizer.unk_token)\n",
    "# print(\"Tokenizer mask token:\", tokenizer.mask_token)\n",
    "# print(\"Tokenizer cls token:\", tokenizer.cls_token)\n",
    "# print(\"Tokenizer sep token:\", tokenizer.sep_token)\n",
    "# print(\"Tokenizer pad token id:\", tokenizer.pad_token_id)\n",
    "# print(\"Tokenizer unk token id:\", tokenizer.unk_token_id)\n",
    "# print(\"Tokenizer mask token id:\", tokenizer.mask_token_id)\n",
    "# print(\"Tokenizer cls token id:\", tokenizer.cls_token_id)\n",
    "# print(\"Tokenizer sep token id:\", tokenizer.sep_token_id)\n",
    "# print(\"Tokenizer bos token id:\", tokenizer.bos_token_id)\n",
    "# print(\"Tokenizer eos token id:\", tokenizer.eos_token_id)\n",
    "# print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "# decode the token -100 \n",
    "# input(\"Press Enter to continue...\")\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset_obj.generate_dataset(rows=data_row, eval_frac=hp.eval_frac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "MISTRAL model size: 929.7M parameters\n",
      "+------------------------------------------------+------------+\n",
      "|                    Modules                     | Parameters |\n",
      "+------------------------------------------------+------------+\n",
      "|           model.embed_tokens.weight            | 204800000  |\n",
      "|     model.layers.0.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.0.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.0.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.0.self_attn.o_proj.weight     |  16777216  |\n",
      "|      model.layers.0.mlp.gate_proj.weight       |  29360128  |\n",
      "|       model.layers.0.mlp.up_proj.weight        |  29360128  |\n",
      "|      model.layers.0.mlp.down_proj.weight       |  29360128  |\n",
      "|     model.layers.0.input_layernorm.weight      |    4096    |\n",
      "| model.layers.0.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.1.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.1.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.1.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.1.self_attn.o_proj.weight     |  16777216  |\n",
      "|      model.layers.1.mlp.gate_proj.weight       |  29360128  |\n",
      "|       model.layers.1.mlp.up_proj.weight        |  29360128  |\n",
      "|      model.layers.1.mlp.down_proj.weight       |  29360128  |\n",
      "|     model.layers.1.input_layernorm.weight      |    4096    |\n",
      "| model.layers.1.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.2.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.2.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.2.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.2.self_attn.o_proj.weight     |  16777216  |\n",
      "|      model.layers.2.mlp.gate_proj.weight       |  29360128  |\n",
      "|       model.layers.2.mlp.up_proj.weight        |  29360128  |\n",
      "|      model.layers.2.mlp.down_proj.weight       |  29360128  |\n",
      "|     model.layers.2.input_layernorm.weight      |    4096    |\n",
      "| model.layers.2.post_attention_layernorm.weight |    4096    |\n",
      "|     model.layers.3.self_attn.q_proj.weight     |  16777216  |\n",
      "|     model.layers.3.self_attn.k_proj.weight     |  4194304   |\n",
      "|     model.layers.3.self_attn.v_proj.weight     |  4194304   |\n",
      "|     model.layers.3.self_attn.o_proj.weight     |  16777216  |\n",
      "|      model.layers.3.mlp.gate_proj.weight       |  29360128  |\n",
      "|       model.layers.3.mlp.up_proj.weight        |  29360128  |\n",
      "|      model.layers.3.mlp.down_proj.weight       |  29360128  |\n",
      "|     model.layers.3.input_layernorm.weight      |    4096    |\n",
      "| model.layers.3.post_attention_layernorm.weight |    4096    |\n",
      "|               model.norm.weight                |    4096    |\n",
      "|                 lm_head.weight                 | 204800000  |\n",
      "+------------------------------------------------+------------+\n",
      "Total Trainable Params: 929.7306M\n",
      "Total Params: 929730560\n",
      "Original Model Size: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model_obj = MyModel(model_id=hp.model_id, hp=hp)\n",
    "config = model_obj.get_model_config(param)    # huggingface mistral config\n",
    "model = model_obj.get_model(param).to(\"cuda:0\", dtype= torch.float32)\n",
    "print(\"Total Params:\",model_obj.model_size_and_parameters())\n",
    "print(\"Original Model Size:\",model.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parallel = torch.nn.DataParallel(model, device_ids=[0,1,2,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_parallel.device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
