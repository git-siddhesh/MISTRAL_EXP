,text
73,"\importpackages{}
\graphicspath{ {./images/} }


\author[R. Friedman]{Robert Friedman}
\address{Columbia University, Department of Mathematics, New York, NY 10027}
\email{rf@math.columbia.edu}
\author[R. Laza]{Radu Laza}
\address{Stony Brook University, Department of Mathematics, Stony Brook, NY 11794}
\email{radu.laza@stonybrook.edu}
\begin{abstract}
The goal of this paper is to describe certain nonlinear topological obstructions for the existence of first order smoothings of mildly singular Calabi-Yau varieties of dimension at least $4$. For nodal Calabi-Yau threefolds,   a necessary and sufficient linear  topological condition for the existence of a first order smoothing was first given  in \cite{F}. Subsequently, Rollenske-Thomas \cite{RollenskeThomas} generalized this picture to nodal Calabi-Yau varieties of odd dimension, by finding a necessary nonlinear topological condition for the existence of a first order smoothing. In a complementary direction,  in \cite{FL}, the linear necessary and sufficient conditions of \cite{F} were extended to Calabi-Yau varieties in every dimension with $1$-liminal singularities (which are exactly the ordinary double points in dimension $3$ but not in higher dimensions).  In this paper, we give a common formulation of all of these previous results by establishing  analogues of the  nonlinear topological conditions of \cite{RollenskeThomas} for  Calabi-Yau varieties with weighted homogeneous $k$-liminal hypersurface singularities, a broad class of singularities that includes ordinary double points in odd dimensions.
\end{abstract}
\thanks{Research of the second author is supported in part by NSF grant DMS-2101640.}    
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
\providecommand{\MRhref}[2]{
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
{amsalpha}
\maketitle
 \section*{Introduction}
 
The deformation theory of generalized Fano and Calabi-Yau threefolds with ordinary double points (or nodes), or more generally isolated canonical hypersurface singularities, has been extensively studied \cite{F}, \cite{namtop}, \cite{NS}, \cite{namstrata}, \cite{SteenbrinkDB}. In recent work \cite{FL}, the authors have revisited and sharpened these results and explored generalizations to higher dimensions. One surprising aspect of these generalizations is that they do not give any information about smoothing varieties with mild singularities such as ordinary double points in dimension at least $4$. To explain this phenomenon, note that the deformations of a Calabi-Yau manifold $Y$ are unobstructed.  A generalization to the case where $Y$ is allowed to have ordinary double points was proved  independently by Kawamata, Ran, and Tian \cite{Kawamata},   \cite{Ran},  \cite{Tian}. Their result was  recently further extended  in \cite{FL22c} to the case where $Y$ is allowed to have much more general types of singularities (\textsl{$1$-Du Bois} local complete intersection  (lci) singularities in the terminology of Definition~\ref{def0.3}). However, unless we know that there exist smoothings to first order, these unobstructedness results do not directly imply the existence of an actual smoothing.  Unfortunately, the methods of  \cite{FL} give no information about the existence of such first order smoothings for ordinary double point singularities in dimension at least $4$, or more generally other classes of  mild (isolated $1$-rational  lci)  singularities. 
 
 On the other hand, Rollenske-Thomas \cite{RollenskeThomas} studied the deformations of Calabi-Yau varieties with ordinary double points, i.e.\ nodal Calabi-Yau varieties,   in odd dimension and gave a \emph{necessary} condition for the existence of first order smoothings, generalizing the results of \cite{F} in dimension $3$. The purpose of this note is to extend the results of  \cite{RollenskeThomas} to a much larger class of singularities, including certain singularities in even dimensions, and to put their results into a broader framework.
 
 \medskip
 
 To explain these results in more detail,  we begin with a preliminary definition:
 
 \begin{definition}\label{defnCY} A \textsl{canonical Calabi-Yau variety $Y$} is a   compact analytic variety $Y$  with at worst canonical Gorenstein (or equivalently rational Gorenstein) singularities, such that $\omega_Y\cong \scrO_Y$, and such that either $Y$ is a scheme or $Y$ has only isolated singularities and the $\partial\bar\partial$-lemma holds for some resolution of $Y$.
\end{definition} 
 
For a    compact analytic variety $Y$ with at worst ordinary double point singularities, recall that a \textsl{first order deformation of $Y$} is a flat proper morphism $f\colon \mathcal{Y} \to \Spec \Cee[\epsilon]$, together with an isomorphism from the fiber over $0$ to $Y$, and these are classified by $\mathbb{T}_Y^1 = \Ext^1(\Omega^1_Y, \scrO_Y)$. Given a class $\theta \in   \mathbb{T}_Y^1$, its image in $H^0(Y; T^1_Y) = \bigoplus_{x\in Y_{\text{\rm{sing}}}}T^1_{Y,x}$ measures the first order change to the singularities of $Y$, and $\theta$ is a \textsl{first order smoothing of $Y$} if the image of $\theta$ in $T^1_{Y,x}\cong \Cee$ is nonzero for every $x\in Y_{\text{\rm{sing}}}$.  Then by \cite[\S4]{F} (also \cite[Prop. 8.7]{FriedmanSurvey}), we have: 
 
 \begin{theorem}\label{dim3criterion} Suppose that $Y$ is a canonical Calabi-Yau variety of dimension $3$ whose only singularities are ordinary double points.  Let $\pi\colon Y'\to Y$ be a small resolution of the singularities of $Y$, so that $\pi^{-1}(x) =C_x\cong \Pee^1$ for every $x\in Y_{\text{\rm{sing}}}$, and let $[C_x]$ be the fundamental class of $C_x$ in $H^2(Y'; \Omega^2_{Y'})$. Then a first order smoothing of $Y$ exists $\iff$ there exist $a_x\in \Cee$, $a_x \neq 0$ for every $x$, such that  $\sum_{x\in Y_{\text{\rm{sing}}}} a_x[C_x] =0$ in $H^2(Y'; \Omega^2_{Y'})$.
\end{theorem}
Rollenske-Thomas \cite{RollenskeThomas} rephrased and partially extended Theorem~\ref{dim3criterion} to all odd dimensions $n = 2k+1 \ge 3$ as follows. For $n > 3$, there is no small resolution of an ordinary double point. Instead, consider the standard blowup of a node.   The exceptional divisor is an even dimensional quadric, whose primitive cohomology is generated by  the difference $[A]-[B]$, where $A$ and $B$ are two complementary linear spaces of dimension $k$ such that $A\cdot B =1$. For $Y$ a projective variety of dimension $2k+1$ whose only singular points are nodes and $\pi: \hY \to Y$ a standard resolution as above,   for each $x\in Y_{\text{\rm{sing}}}$, there is thus a class $[A_x]-[B_x]\in H^{k+1}(\hY;\Omega^{k+1}_{\hY})$.  The following is equivalent to the necessity part of  Theorem~\ref{dim3criterion} in dimension $3$ and generalizes it   to all odd dimensional nodal canonical Calabi-Yau varieties:   
 
\begin{theorem}[Rollenske-Thomas]\label{RTthm} Suppose that $Y$ is a canonical Calabi-Yau variety of odd dimension $n=2k+1$ whose only singularities are ordinary double points and let $\hY \to Y$ be a standard resolution as above. Then there exist identifications   $T^1_{Y,x}\cong \Cee$  such that the following holds: If $\theta$ is a  first order smoothing of $Y$ with  image in $T^1_{Y,x}$ equal to $\lambda_x\in \Cee$ via the above  isomorphisms $T^1_{Y,x}\cong \Cee$, then, with notation as above,  
 \begin{equation}\label{eqn1}
 \sum_{x\in Y_{\text{\rm{sing}}}} \lambda_x^k ([A_x]-[B_x])=0 
 \end{equation} 
   in $H^{k+1}(\hY;\Omega^{k+1}_{\hY})$.
\end{theorem}
 
 We can interpret Theorem~\ref{RTthm} in the following way. First, if there exists a first order smoothing of $Y$, then the classes $[A_x]-[B_x]$ are not linearly independent in $H^{k+1}(\hY;\Omega^{k+1}_{\hY})$, and in fact satisfy a linear relation whose coefficients are all nonzero. Second, the image of $\mathbb{T}_Y^1$ in $H^0(Y; T^1_Y)$, which is a vector subspace of $H^0(Y; T^1_Y)$, is contained in the subvariety of $H^0(Y; T^1_Y)$ defined by the nonlinear Equation~\ref{eqn1}, which is roughly speaking an intersection of affine varieties of Fermat type.  
 
 \medskip
 
 The goal of this paper is to further generalize \cite{RollenskeThomas}.  To state the result, let $Y$ be as before a compact analytic variety with isolated singularities.  If $x\in Y_{\text{\rm{sing}}}$ is a singular point, let $\pi \colon \hY \to Y$ be some log resolution of $Y$ and let $E_x =\pi^{-1}(x)$ be the exceptional divisor over $x$. In the case of ordinary double points, we have both that $\dim T^1_{Y, x} = 1$ for a singular point and that there are distinguished classes $[A_x]-[B_x] \in H^{k+1}(\hY;\Omega^{k+1}_{\hY})$ which are defined locally around the singular points, and will have to find appropriate substitutes more generally. First, we deal with  the issue of  $T^1_{Y,x}$ by defining the types of smoothings to which our theorem will apply: 
\begin{definition} Let $(X,x)$ be the germ of an isolated hypersurface singularity, so that $T^1_{X,x} = \scrO_{X,x}/J$ is a cyclic $\scrO_{X,x}$-module. Thus $\dim T^1_{X,x}/\mathfrak{m}_xT^1_{X,x} = 1$. Then an element  $\theta_x\in T^1_{X,x}$ is a \textsl{strong first order smoothing} if $\theta_x\notin \mathfrak{m}_xT^1_{X,x}$.  In case $x$ is an ordinary double point, $\theta_x\in T^1_{X,x}$ is a  strong first order smoothing $\iff$ $\theta_x\neq 0$. For a compact $Y$ with only isolated hypersurface singularities, a first order deformation $\theta\in \mathbb{T}^1_Y$ is a \textsl{strong first order smoothing} if the image $\theta_x$ of $\theta$ in $T^1_{X,x}$ is a strong first order smoothing for every $x\in Y_{\text{\rm{sing}}}$. A standard argument (e.g.\ \cite[Lemma 1.9]{FL}) shows that,  if  $f\colon \mathcal{Y}\to \Delta$ is a deformation of $Y$ over the disk, then its Kodaira-Spencer class $\theta$ is a strong first order smoothing $\iff$ $\mathcal{Y}$ is smooth and in particular  the nearby fibers $Y_t =f^{-1}(t)$, $0< |t| \ll 1$, are smooth.
\end{definition} 
 
 To deal with the correct generalization of the class $[A_x]-[B_x]$, recall that, for each $x\in Y_{\text{\rm{sing}}}$ (assumed throughout to be an isolated hypersurface singularity), 
 we have the corresponding link $L_x$ at $x$.  There is a natural mixed Hodge structure on $H^\bullet(L)$ (see e.g.\ \cite[\S6.2]{PS}). Moreover, for all $k$, there is a natural map 
 \begin{equation}\varphi\colon \Gr^{n-k}_FH^n(L_x) \to H^{k+1}(\hY;\Omega^{n-k}_{\hY})\end{equation}  given as the composition
\begin{gather*}
\Gr^{n-k}_FH^n(L_x) =H^k(E_x;\Omega^{n-k}_{\hY}(\log E_x)|E_x) \to \Gr^{n-k}_FH^{n+1}_E(\hY) = H^k(E_x;\Omega^{n-k}_{\hY}(\log E_x)/\Omega^{n-k}_{\hY})\\
\xrightarrow{\partial}  H^{k+1}(\hY;\Omega^{n-k}_{\hY}).
\end{gather*}
In case there is a Hodge decomposition for $\hY$ (for example if $\hY$ is  K\""ahler or more generally satisfies the $\partial\bar\partial$-lemma), the above maps are consistent in the obvious sense with the topological maps
$$H^n(L_x) \to H^{n+1}_{E_x}(\hY) \to H^{n+1}(\hY),$$
where via Poincar\'e duality the map $H^n(L_x) \to  H^{n+1}(\hY)$ is the same as the natural map  $H_{n-1}(L_x) \to  H_{n-1}(\hY)$. In the special case where $x$ is an ordinary double point and $n=2k+1$,  $\dim H^n(L_x) = 1$,  so that $H^n(L_x) =\Cee \varepsilon_x$ for some $\varepsilon_x \in H^n(L_x)$, 
and, for an appropriate choice of  $\varepsilon_x$,
 $\varphi(\varepsilon_x) = [A_x]-[B_x]\in H^{k+1}(\hY;\Omega^{n-k}_{\hY}) =  H^{k+1}(\hY;\Omega^{k+1}_{\hY})$.  
\medskip
In \cite[Definition 6.10]{FL22d}, we have defined a class of isolated hypersurface singularities which we call \textsl{$k$-liminal}, which are a far-reaching  generalization of ordinary double points in  odd dimensions. The technical  definition is given below in Definition~\ref{def0.3}. Suffice it to say here that,   in dimension $3$,  the only $1$-liminal singularities are ordinary double points, and more generally  in odd dimension $2k+1$,  the only $k$-liminal singularities are ordinary double points. However, ordinary double points in even dimensions are not $k$-liminal for any value of $k$.  On the positive side, by Lemma~\ref{ex0.4.1}, for every $n \ge 3$, there exist $k$-liminal singularities of dimension $n$ $\iff$ $1\le k \le \Dis \left[\frac{n-1}{2}\right]$. In particular, for every $n \ge 3$, there exist $k$-liminal singularities of dimension $n$ for some  $k\ge 1$. One reason that   $k$-liminal singularities play  a role similar to that of ordinary double points in odd dimensions, is the following result, essentially due to  Dimca-Saito \cite[\S4.11]{DS} (cf.\ also \cite[Corollary 6.14]{FL22d}):
\begin{theorem}\label{introlink}  If $(X,x)$ is the germ of an isolated $k$-liminal hypersurface singularity and $L$ is the corresponding link, then $\dim \Gr^{n-k}_FH^n(L) =1$.
\end{theorem}
\begin{remark}\label{1DBsmooths} For $k \ge 1$, a  $k$-liminal singularity is in particular $1$-Du Bois. Hence, by \cite[Corollary 1.5]{FL22c}, a canonical Calabi-Yau variety $Y$ with  only isolated $k$-liminal hypersurface singularities has unobstructed deformations. 
 In particular if there exists a strong first order smoothing of $Y$, then $Y$ is smoothable. 
\end{remark}
 For $1$-liminal  singularities, we showed   \cite[Lemma 5.6, Corollary 5.12]{FL}  that there is a necessary and sufficient linear condition   for there to exist a strong first order smoothing of $Y$, and hence an actual smoothing by Remark~\ref{1DBsmooths}. 
  This statement (see Theorem~\ref{1limthm} below for a precise version) can be viewed as a  natural generalization of Theorem~\ref{dim3criterion}.  The main results of this paper, Theorem~\ref{mainthma} and Corollary~\ref{last},  are then   further generalizations which apply to all  weighted homogeneous $k$-liminal singularities. However, as  in Theorem~\ref{RTthm}, we are only able to obtain necessary conditions for $k \ge 2$: 
\begin{theorem}\label{intromain}  Let $Y$ be a canonical Calabi-Yau variety of dimension $n$ with isolated $k$-liminal  weighted homogeneous hypersurface singularities. For each singular point $x\in Y$, let $L_x$ be the link at $x$ and write $\Gr^{n-k}_FH^n(L_x) = H^k(E_x; \Omega^{n-k}_{\hY}(\log E_x)|E_x) =\Cee\cdot \varepsilon_x$ for some choice of a generator $\varepsilon_x$.  Let $\varphi\colon \Gr^{n-k}_FH^n(L) =\bigoplus_{x\in Y_{\text{\rm{sing}}}}\Gr^{n-k}_FH^n(L)\to H^{k+1}(\hY; \Omega^{n-k}_{\hY})$ be the natural map. 
Finally, for  each $x\in Y_{\text{\rm{sing}}}$, fix an identification $T_{Y,x}^1/\mathfrak{m}_x T_{Y,x}^1\cong \Cee$.  Then, for each $x\in Y_{\text{\rm{sing}}}$ there exist $c_x\in \Cee^*$ with the following property: If $\theta \in \mathbb{T}^1_Y$ induces $\lambda_x\in \Cee$, then 
$$\sum_{x\in Y_{\text{\rm{sing}}}} c_x\lambda_x^k\varphi(\varepsilon_x) =0 \in H^{k+1}(\hY; \Omega^{n-k}_{\hY}).$$
In particular, if a strong first order smoothing of $Y$ exists, then the classes $\varphi(\varepsilon_x)$ are not linearly independent.
\end{theorem}
 In some sense, the proof of Theorem~\ref{intromain} follows the main outlines of \cite{RollenskeThomas}. A key aspect of our arguments is that by restricting to weighted homogeneous singularities, we can work as if there exists a log resolution with a single (smooth) exceptional divisor $E$ as in loc.\ cit.\ More precisely, for $Y$ with such singularities, there is the weighted blowup, i.e.\ an orbifold resolution of singularities $Y^{\#} \to Y$ whose exceptional divisors $E$ are smooth divisors in the sense of orbifolds. There are  stacks  naturally associated to   $Y^{\#}$ and $E$, a picture which is worked out in detail in \cite[\S3]{FL} (whose methods we use systematically). Thus we can work as if $Y^{\#}$ and $E$ were smooth and use the familiar numerology of hypersurfaces in weighted projective space.   It would be interesting to generalize the proof of Theorem~\ref{intromain} to the case where the singularities are not necessarily weighted homogeneous. 
 
\medskip
The outline of this paper is as follows. In \S\ref{ss1.1}, we collect some necessary preliminaries about isolated singularities. $k$-liminal singularities are defined in \S\ref{ss1.2}, and the stack point of view is recalled in \S\ref{ss1.3}. Section \ref{s2} deals with the geometry of  $k$-liminal weighted homogeneous singularities and establishes the existence of a nonzero homogeneous pairing between two one-dimensional vector spaces. In \S\ref{ss3.1}, this construction is globalized to establish Theorem~\ref{intromain} (Theorem~\ref{mainthma} and Corollary~\ref{last}).  There is also a brief discussion in \S\ref{ss3.2} of the interplay between the Hodge theory of $Y$ or of $\hY$ and of a smoothing $Y_t$ of $Y$.
\subsection*{Acknowledgements} It is a pleasure to thank Johan de Jong and Richard Thomas for their comments and suggestions.
\section{Preliminaries}\label{s1}
\subsection{Some general Hodge theory}\label{ss1.1}  Let $X$ be a contractible Stein neighborhood of the isolated  singularity $x$ of dimension $n\ge 3$, and let $\pi\colon \hX \to X$ be a good resolution with exceptional divisor $E =\pi^{-1}(x)$, and $U=X-\{x\} = \hX -E$. In the global setting,  $Y$ will denote  a projective variety  of dimension $n$ with isolated singularities,  $Z = Y_{\text{\rm{sing}}}$ the singular locus of $Y$, $\pi\colon \hY \to Y$   a good (log) resolution,   $E$   the exceptional divisor, i.e.\ $E = \pi^{-1}(Z)$, and $V = \hY - E = Y- Z$.  Instead of assuming that $Y$ is projective,  it is more generally enough to assume that $Y$  has a resolution satisfying the $\partial\bar\partial$-lemma. 
\begin{lemma}\label{biratinv} With $Y$ and  $\pi\colon \hY \to Y$ as above, and for all $p,q$,  the groups $H^q(\hX; \Omega^p_{\hX}(\log E))$, $H^q(\hX; \Omega^p_{\hX}(\log E)(-E))$, $H^q(\hY; \Omega^p_{\hY}(\log E))$, and $H^q(\hY; \Omega^p_{\hY}(\log E)(-E))$ are all independent of the choice of resolution.
\end{lemma}
\begin{proof} The independence of $H^q(\hY; \Omega^p_{\hY}(\log E))$ is a result of Deligne \cite[3.2.5(ii)]{DeligneHodgeII}. The independence of $H^q(\hY; \Omega^p_{\hY}(\log E)(-E))$ then follows because $H^q(\hY; \Omega^p_{\hY}(\log E)(-E))$ is Serre dual to $H^{n-q}(\hY; \Omega^{n-p}_{\hY}(\log E))$. The local results for $\hX$ can then be reduced to this case (cf.\ \cite[Remark 3.15]{FL}). 
\end{proof}
\begin{remark}\label{duality} In case $Y$ is projective, we can understand the birational invariance as follows: Let $\uOb_{Y,Z}$ be the relative filtered de Rham complex as defined by Du Bois \cite{duBois}.  By \cite[Th\'eor\`eme 2.4]{duBois}, $\uOb_{Y,Z}$ is an invariant of $Y$ as an object in the filtered derived category,  and the corresponding Hodge spectral sequence degenerates at $E_1$ in case $Y$ is projective. By \cite[Example 7.25]{PS}, $\uOp_{Y,Z} \cong R\pi_*\Omega^p_{\hY}(\log E)(-E)$. Applying the Leray spectral sequence for hypercohomology gives
$$\Gr^p_FH^{p+q}(Y,Z) = \mathbb{H}^q(Y; \uOp_{Y,Z}) = H^q(\hY; \Omega^p_{\hY}(\log E)(-E)).$$
Hence  $H^q(\hY; \Omega^p_{\hY}(\log E)(-E)) = \Gr^p_FH^{p+q}(Y,Z)$ does not depend on the choice of a resolution.
Note that from the exact sequence
$$\cdots  \to H^{i-1}(Z) \to    H^i(Y,Z) \to H^i(Y) \to H^i(Z) \to \cdots,$$
$ H^i(Y,Z) \cong  H^i(Y)$ except for $i=0,1$. Moreover, the hypercohomology of the exact sequence
$$0 \to \Omega^\bullet_{\hY}(\log E)(-E) \to \Omega^\bullet_{\hY} \to \Omega^\bullet_E/\tau^\bullet_E\to 0$$
gives the Mayer-Vietoris sequence, an  exact sequence of mixed Hodge structures:
$$\cdots \to H^{i-1}(E)\to H^i(Y,Z) \to H^i(\hY) \to H^i(E) \to \cdots.$$
Finally, the duality between $\mathbb{H}^\bullet(\hY;  \Omega^\bullet_{\hY}(\log E)(-E))$ and $\mathbb{H}^\bullet(\hY;  \Omega^\bullet_{\hY}(\log E))$ corresponds to Poincar\'e duality (cf.\ \cite[\S5.5, B.21, B.24]{PS}) 
$$H^i(Y,Z) \cong H^i_c(Y-Z) \cong (H^{2n-i}(Y-Z))\spcheck(-n) = (H^{2n-i}(\hY-E))\spcheck(-n).$$
\end{remark} 
 \begin{lemma}\label{lemma3.3}  With $Y$ and  $\pi\colon \hY \to Y$ as above,  the map 
 $$\Gr^{n-k}_FH^{n+1}(Y) = H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY})$$ is injective for all $k\ge 0$.
 \end{lemma}
 \begin{proof}  We have the exact sequence
  $$H^k(\hY; \Omega^{n-k}_{\hY}) \to H^k(E; \Omega^{n-k}_E/\tau^{n-k}_E) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY}).$$
  By semipurity in the local setting \cite[(1.12)]{Steenbrink}, the map $H^n_E(\hX) \to H^n(E)$ is an isomorphism. Since it factors by excision as $H^n_E(\hX) \cong H^n_E(\hY) \to H^n(\hY) \to H^n(E)$, the map $H^n(\hY) \to H^n(E)$ is therefore surjective, and hence, by strictness of morphisms, so is the map 
  $$\Gr^{n-k}_FH^n (\hY) = H^k(\hY; \Omega^{n-k}_{\hY}) \to  \Gr^{n-k}_F H^n(E) = H^k(E; \Omega^{n-k}_E/\tau^{n-k}_E) .$$
  Thus the map $H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY})$ is injective.
 \end{proof} 
 
\subsection{$k$-Du Bois, $k$ rational, and $k$-liminal singularities}\label{ss1.2}The $k$-Du Bois and $k$-rational singularities, natural extensions of Du Bois and rational singularities respectively (case $k=0$), were recently introduced by \cite{MOPW}, \cite{JKSY-duBois}, \cite{KL2}, \cite{FL22c}, and \cite{MP-rat}. The relevance of these classes of singularities (especially for $k=1$) to the deformation theory of singular Calabi-Yau and Fano varieties is discussed in \cite{FL}, which additionally singles out the {\it $k$-liminal singularities} (for $k=1$) as particularly relevant to the deformation theory of such varieties. The $k$-liminal singularities should be understood as the frontier case between $(k-1)$-rational and $k$-rational. For the convenience of the reader, we summarize the relevant facts for these classes of singularities. 
\begin{definition}\label{def0.3}  Let $(X,x)$ be the germ of an isolated local complete intersection (lci) singularity of dimension $n\ge 3$ and let $\pi\colon \hX \to X$ be a good resolution with exceptional divisor $E$. Then $X$ is \textsl{$k$-Du Bois} if $R^i\pi_*\Omega^p_{\hX}(\log E)(-E) = 0$ for $i> 0$ and $p\le k$, and is \textsl{$k$-rational} if $R^i\pi_*\Omega^p_{\hX}(\log E) = 0$ for $i> 0$ and $p\le k$. By \cite{FL22c}, \cite{MP-rat}, if $(X,x)$ is $k$-rational, then it is $k$-Du Bois and by \cite{FL22d}, \cite{ChenDirksM},  if $(X,x)$ is $k$-Du Bois, then it is $(k-1)$-rational.
Finally, $(X,x)$ is \textsl{$k$-liminal} if it is $k$-Du Bois but not $k$-rational. In this case, if $X$ is a hypersurface singularity, then  $\dim \Gr^{n-k}_FH^n(L) =1$, by Theorem~\ref{introlink}. 
\end{definition}
The following collects some basic facts about $k$-liminal singularities:
\begin{lemma}\label{ex0.4}  \begin{enumerate} \item[\rm(i)] If  $\dim X =3$ and $X$ is a non-smooth isolated hypersurface singularity,  then $X$ is not $1$-rational, and $X$ is $1$-liminal $\iff$ $X$ is $1$-Du Bois  $\iff$ $X$ is an ordinary double point.  
\item[\rm(ii)] More generally, if $X$ is a $k$-Du Bois singularity and  $k > \frac12(n-1)$, then $X$ is smooth. If $X$ is a hypersurface singularity of dimension $n= 2k+1$ and $X$ is not smooth, then $X$ is $k$-Du Bois $\iff$  $X$ is $k$-liminal $\iff$ $X$ is an ordinary double point.
\item[\rm(iii)] Let $X$ be a weighted homogeneous isolated hypersurface singularity. Viewing $X$ as locally analytically isomorphic to the subvariety $\{f=0\}$ of $(\Cee^{n+1}, 0)$, where   $\Cee^*$ acts on $\Cee^{n+1}$ with weights $a_1, \dots, a_{n+1} \ge 1$, and $f$ is weighted homogeneous of degree $d$, define $w_i = a_i/d$. Then:
 \begin{enumerate}
 \item  $X$ is $k$-Du Bois $\iff$ $\sum_{i=1}^{n+1} w_i \ge k+1$.
 \item  $X$ is $k$-rational $\iff$ $\sum_{i=1}^{n+1} w_i > k+1$.
  \item  $X$ is $k$-liminal $\iff$ $\sum_{i=1}^{n+1} w_i = k+1$.
 \end{enumerate}
  \end{enumerate}
\end{lemma} 
\begin{proof} (i) This is  a result of Namikawa-Steenbrink \cite[Theorem 2.2]{NS} (cf.\ also \cite[Corollary 6.12]{FL22d}).  
\smallskip
\noindent (ii) This is  \cite[Corollary 6.3]{DM} (cf.\ also \cite[Corollary 4.4]{FL22d}). 
\smallskip
\noindent (iii) This is a result of Saito \cite[(2.5.1)]{SaitoV} (see also  \ \cite[Corollary 6.8]{FL22d}). 
   \end{proof} 
\begin{remark}\label{klimremark}  (i) By definition, a $0$-liminal singularity is $0$-Du Bois, i.e.\ Du Bois in the terminology of \cite{Steenbrink}, but not rational. Thus these singularities fall outside the scope of this paper. If $X$ is an isolated normal Gorenstein  surface singularity which is Du Bois but not rational, then by \cite[3.8]{Steenbrink} $X$ is either a simple elliptic or a cusp singularity. Such singularities are known to be deeply connected to degenerations of $K3$ surfaces, and it is natural to ask if there is a similar phenomenon for Calabi-Yau varieties in higher dimensions. 
\smallskip
\noindent (ii)  Assume that $X$ is a weighted homogeneous hypersurface singularity.  If $X$ is the cone over a smooth hypersurface $E$ of degree $d$ in $\Pee^n$, then, by Lemma~\ref{ex0.4}(iii),  the $k$-liminal condition is $n+1 = d(k+1)$, and in particular $n+1$ is divisible by $d$ and by $k+1$. Thus, these examples are somewhat sparse. By Theorem~\ref{introlink}, the Hodge structure on $H^{n-1}(E)$ is (up to a Tate twist) of Calabi-Yau type. Primarily for this reason, such hypersurfaces are exceptions to Donagi's proof for generic Torelli (\cite{Donagi}; cf.\ Voisin \cite{Voisin} for recent work along these lines). 
\end{remark} 
Despite Remark~\ref{klimremark}(ii) above, there are many examples of isolated  weighted homogeneous $k$-liminal singularities: 
\begin{lemma}\label{ex0.4.1}  For all $k$ with $\Dis 1\le k \le \left[\frac{n-1}{2}\right]$, there exists an isolated  weighted homogeneous $k$-liminal singularity   given by a diagonal hypersurface $f(z) = z_1^{e_1} + \cdots + z_{n+1}^{e_{n+1}}$.
\end{lemma} 
\begin{proof} Given $k$ such that $\Dis 1\le k \le \left[\frac{n-1}{2}\right]$,  let  $f(z) = z_1^{e_1} + \cdots + z_{n+1}^{e_{n+1}}$. First suppose that  $n=  2a+1$ is odd, so $\Dis\left[\frac{n-1}{2}\right] = a$. Then  choose $2\ell$ of the $e_i$ equal to $2$ and the remaining $n+1 - 2\ell=2(a+1-\ell)$ equal to  $\Dis \frac{n+1 - 2\ell}{2}= a+1-\ell$. Here $0\le \ell \le a-1$ because the value $\ell = a$ would give some $e_i = 1$. Then 
  $$\sum_i w_i = \sum_i \frac{1}{e_i} = \frac12(2\ell)  + (n+1 - 2\ell)\left(\frac{2}{n+1 - 2\ell}\right) = \ell + 2,$$
  and hence $k=\sum_i w_i -1=\ell +1$ can take on all possible values from $1$ to $a$.
  
  Similarly, if $n=2a$ is even, so that $\Dis\left[\frac{n-1}{2}\right] = a-1$, and $1\le \ell \le a-2$, choose $2\ell-1$ of the $e_i$ to be $2$, $2$ of the $e_i$ to be $4$, and the remaining $n+1 - (2\ell+1) = 2a -2\ell$ to be  $\Dis \frac{n+1 - (2\ell+1)}{2} = a-\ell$. Then
   $$\sum_i w_i = \sum_i \frac{1}{e_i} = \frac12(2\ell-1)  + \frac12 + (n+1 - (2\ell+1))\left(\frac{2}{n+1 - (2\ell+1)}\right) = \ell + 2,$$
  and hence $k=\sum_i w_i -1=\ell +1$ can take on all possible values from $2$ to $a-1$. For the remaining possibility $k=1$, take $n-1=2a-1$ of the $e_i$ equal to $a$ and the remaining two equal to $2a$ to get $\sum_iw_i = 2$ and hence $k=1$.
\end{proof} 
 
The following then generalizes \cite[2.6]{RollenskeThomas}:
\begin{lemma}\label{0.2.1} If the singularities of $X$ are isolated  $1$-Du Bois lci singularities, then  $H^0(X;T^1_X) \cong H^1(\hX; \Omega^{n-1}_{\hX}(\log E))$. In the global case,
$\mathbb{T}^1_Y \cong H^1(\hY; \Omega^{n-1}_{\hY}(\log E))$, compatibly with the map $\mathbb{T}^1_Y \to H^0(Y;T^1_Y)$ and restriction, i.e.\  the following diagram commutes:
$$\begin{CD}
  H^1(\hY; \Omega^{n-1}_{\hY}(\log E)) @>>> H^0(Y;R^1\pi_*\Omega^{n-1}_{\hY}(\log E)) \\
  @V{\cong}VV @VV{\cong}V \\
\mathbb{T}^1_Y @>>>  H^0(Y;T^1_Y) .
\end{CD}$$
\end{lemma}
\begin{proof} First, $H^0(X;T^1_X)\cong H^1(U; T^0_X|U) = H^1(U;\Omega^{n-1}_{\hX}(\log E)|U)$. The local cohomology sequence gives
$$H^1_E(\hX; \Omega^{n-1}_{\hX}(\log E)) \to H^1(\hX; \Omega^{n-1}_{\hX}(\log E)) \to H^0(X;T^1_X) \to H^2_E(\hX; \Omega^{n-1}_{\hX}(\log E)).$$ Since $1$-Du Bois lci singularities are rational,   $H^1_E(\hX; \Omega^{n-1}_{\hX}(\log E)) =0$  by \cite[1.8]{FL} and  the $1$-Du Bois assumption implies that $H^2_E(\hX; \Omega^{n-1}_{\hX}(\log E))=0$ (cf.\ \cite[2.8]{FL}). Hence $H^0(X;T^1_X) \cong H^1(\hX; \Omega^{n-1}_{\hX}(\log E))$. The global case is similar, using $\mathbb{T}^1_Y \cong H^1(V;\Omega^{n-1}_{\hY}(\log E)|V)$, and the compatibility is clear.
\end{proof}
There is a similar result for $1$-rational singularities:
\begin{lemma}\label{0.2} If the singularities of $X$ are isolated $1$-rational lci singularities, then $H^0(X;T^1_X) \cong H^1(\hX; \Omega^{n-1}_{\hX}(\log E)(-E))$. Globally,
$\mathbb{T}^1_Y \cong H^1(\hY; \Omega^{n-1}_{\hY}(\log E)(-E))$, and there is a commutative diagram
$$\begin{CD}
  H^1(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)) @>>> H^0(Y;R^1\pi_*\Omega^{n-1}_{\hY}(\log E)(-E)) \\
  @V{\cong}VV @VV{\cong}V \\
\mathbb{T}^1_Y @>>>  H^0(Y;T^1_Y) .
\end{CD}$$
\end{lemma}
\begin{proof} Since isolated $1$-rational singularities are $1$-Du Bois, it suffices by Lemma~\ref{0.2.1} to show that the map $H^1(\hX; \Omega^{n-1}_{\hX}(\log E)(-E)) \to H^1(\hX; \Omega^{n-1}_{\hX}(\log E))$ is an isomorphism. We have the long exact sequence
\begin{gather*}
 H^0(\hX; \Omega^{n-1}_{\hX}(\log E)|E) \to H^1(\hX; \Omega^{n-1}_{\hX}(\log E)(-E)) \to H^1(\hX; \Omega^{n-1}_{\hX}(\log E)) \\
 \to H^1(\hX; \Omega^{n-1}_{\hX}(\log E)|E).
 \end{gather*}
Moreover, $H^1(\hX; \Omega^{n-1}_{\hX}(\log E)|E)= \Gr_F^{n-1}H^n(L)$, of dimension $\ell^{n-1, 1}  = \ell^{1, n-2} =0$ by the $1$-rational condition \cite[Theorem 5.3(iv)]{FL22d}. Likewise $\dim H^0(\hX; \Omega^{n-1}_{\hX}(\log E)|E) =  \ell^{n-1, 0}  = 0$ since the singularity is rational and by a result of Steenbrink \cite[Lemma 2]{SteenbrinkDB}. Hence 
$$H^1(\hX; \Omega^{n-1}_{\hX}(\log E)(-E)) \cong H^1(\hX; \Omega^{n-1}_{\hX}(\log E)).$$ The global case and the compatibility are again clear.
\end{proof}
\begin{remark}\label{remark1.3}  In the global case, where we do not make the assumption that $\omega_Y \cong \scrO_Y$, the above lemmas remain true provided that we replace $H^1(\hY; \Omega^{n-1}_{\hY}(\log E))$ resp.\  $ H^1(\hY; \Omega^{n-1}_{\hY}(\log E)(-E))$ by $H^1(\hY; \Omega^{n-1}_{\hY}(\log E)\otimes\pi^*\omega_Y^{-1})$ resp.\  $ H^1(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)\otimes \pi^*\omega_Y^{-1})$.
\end{remark}
To illustrate how these results may be used in practice, we give a quick proof of a slight variant of  \cite[Corollary 5.8]{FL}:
 \begin{theorem}\label{1limthm} Suppose that $Y$ is a canonical Calabi-Yau variety of dimension $n\ge 3$ with isolated $1$-liminal hypersurface singularities. Then a strong first order smoothing of $Y$ exists $\iff$ for every $x\in Z$, there  exists $a_x\in \Cee$, $a_x\neq 0$, such that $\sum_xa_x\varphi(\varepsilon_x)=0$ in $H^2(\hY; \Omega^{n-1}_{\hY}(\log E))$, where  $\varepsilon _x \in \Gr^{n-1}_FH^n (L_x)$ is a generator and $\varphi$ is the composition
  $$H^1(\hY; \Omega^{n-1}_{\hY}(\log E)|E) \xrightarrow{\partial}  H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)) \to H^2(\hY; \Omega^{n-1}_{\hY}).$$
 In particular, if $Y$ satisfies the above condition,  it is smoothable. 
  \end{theorem}
  \begin{proof}  There is a commutative diagram
  $$\begin{CD}
  \mathbb{T}^1_Y @>>> H^0(Y; T^1_Y) @. \\
  @V{\cong}VV @VVV @.\\
   H^1(\hY; \Omega^{n-1}_{\hY}(\log E)) @>>> H^1(\hY; \Omega^{n-1}_{\hY}(\log E)|E) @>{\partial}>>  H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)).
  \end{CD}$$
  Here, if as usual $E_x = \pi^{-1}(x)$,  $H^1(\hY; \Omega^{n-1}_{\hY}(\log E)|E_x)$ has dimension one for every $x\in Z$  by the $1$-liminal assumption.  Let $\varepsilon_x$ be a basis vector. By \cite[Lemma 2.6, Theorem 2.1(v)]{FL}, the map $T^1_{Y,x}   \to H^1(\hY; \Omega^{n-1}_{\hY}(\log E)|E_x)$ is surjective and its kernel is $\mathfrak{m}_xT^1_{Y,x}$. Thus, $Y$ has a strong first order smoothing $\iff$ for every $x\in Z$, there exists $a_x\in \Cee$, $a_x\neq 0$, such that $\sum_{x\in Z}a_x \partial(\varepsilon_x)=0$ in $H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E))$. 
 By Lemma~\ref{lemma3.3}, the map  $H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)) \to H^2(\hY; \Omega^{n-1}_{\hY})$ is injective. It follows that  $\sum_xa_x\partial(\varepsilon_x)=0$ in $H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E))$ $\iff$ $\sum_xa_x\varphi(\varepsilon_x)=0$ in $H^2(\hY; \Omega^{n-1}_{\hY})$.  Thus a strong first order smoothing exists $\iff$ $\sum_xa_x\varphi(\varepsilon_x)=0$. The final statement then follows from \cite[Corollary 1.5]{FL22c}.
 \end{proof}
 
 \begin{remark}\label{1limFano} There is a similar result in the $1$-liminal Fano case:   Assume that  $Y$ has only  isolated $1$-liminal hypersurface singularities  and that $\omega_Y^{-1}$ is ample.  In this case, the above construction produces an obstruction to a strong first order smoothing, namely  $\sum_{x\in Z} c_x\lambda_x^k \partial(\varepsilon_x) \in H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)\otimes \pi^*\omega_Y^{-1})$. The group $H^2(\hY; \Omega^{n-1}_{\hY}(\log E)(-E)\otimes \pi^*\omega_Y^{-1})$ is Serre dual to $H^{n-2}(\hY; \Omega^1_{\hY}(\log E) \otimes \pi^*\omega_Y)$.   In many  cases,  $H^{n-2}(\hY; \Omega^1_{\hY}(\log E) )\otimes \pi^*\omega_Y)=0$. For example, if there exists a smooth Cartier divisor $H$ on $Y$, thus not passing through the singular points of $Y$, such that $\omega_Y =\scrO_Y(-H)$, and in addition $H^{n-3}(H; \Omega^1_H) = 0$, then an argument with the Goresky-MacPherson-Lefschetz theorem in intersection cohomology \cite{GoreskyMacPherson} shows that 
  $$H^{n-2}(\hY; \Omega^1_{\hY}(\log E)\otimes \pi^*\omega_Y)=H^{n-2}(\hY; \Omega^1_{\hY}(\log E)\otimes \scrO_{\hY}(-H))= 0,$$
  where we identify the divisor $H$ on $Y$ with its preimage $\pi^*H$ on $\hY$.
The proof of Theorem~\ref{1limthm} then shows  that, under these assumptions,  a strong first order smoothing of $Y$ always exists, and hence $Y$ is smoothable by \cite[Theorem  4.5]{FL}. A somewhat stronger statement is proved in \cite[Corollary 4.10]{FL}. 
 \end{remark}
  
  
 
   
\subsection{Weighted homogeneous singularities and quotient stacks}\label{ss1.3} For the remainder of this section, we are concerned with generalizing the above picture, and in particular Lemma~\ref{0.2},  in the  context of stacks: Assume that the isolated singularity $X$ is  locally  analytically isomorphic to a weighted cone in $\Cee^{n+1}$  over a weighted hypersurface $E\subseteq WP^n$. Thus we may as well assume that $X$ is the weighted cone as in Lemma~\ref{ex0.4}(iii), with an isolated singularity at $0$. 
\begin{definition}\label{def1.8}   Let $X$ be the weighted cone  in $\Cee^{n+1}$  over a weighted hypersurface $E\subseteq WP^n$, where $WP^n$ is a weighted projective space, and  $X^{\#}$  the weighted blowup of $X$ as in \cite[\S3]{FL}.  Let  $\uE$, $\uhX$, $\uP^n$ be the corresponding  quotient stacks. If $X$ has an isolated singularity at $0$, then  $\uhX$ and $\uE$ are  quotient stacks for an action of $\Cee^*$  on smooth schemes with finite stabilizers,  and hence $\uhX$ is a smooth stack and $\uE$ is a smooth divisor in $\uhX$. 
Globally, let $Y$ be a projective variety  of dimension $n$ with isolated   weighted homogeneous hypersurface singularities. Let   $\pi\colon  Y^{\#} \to Y$   denote the weighted blowup of $Y$ at the singularities,  and let $E$ be the exceptional divisor, i.e.\ $E = \pi^{-1}(Z)$ where $Z = Y_{\text{\rm{sing}}}$. We can also construct a stacky version of $Y^{\#}$ as follows: For each $x\in Z$, we have the corresponding exceptional divisor $E_x$.  Let $X$ denote the corresponding weighted cone in $\Cee^{n+1}$.
There is a (Zariski) open neighborhood $U\subseteq Y$ of $x$ and an \'etale morphism  $U\to X$.  We can then pull back the stack $\uhX$  to a stack $ \underline{U}^{\#}$ and glue $\underline{U}^{\#}$ and $Y-\{x\}$ along the Zariski open subset $U-\{x\}$. Doing this for each singular point defines the stack $\uhY$. A similar construction works in the analytic category. 
\end{definition}
 
As in Definition~\ref{def1.8}, let $X^{\#}$ be the weighted blowup of $X$, with $\uhX$ the associated stack, and let $\hX$ be an arbitrary  log resolution. Given a projective $Y$ with isolated  weighted homogeneous hypersurface singularities, we define $\uhY$ as before and let $\pi\colon \hY \to Y$ be a log resolution. To avoid confusion, we denote the exceptional divisor of  $\pi\colon \hX \to X$ or $\pi\colon \hY \to Y$  by $\widehat{E}$. We claim that, in the statement of  Lemmas~\ref{0.2.1} and \ref{0.2}, we can replace ordinary cohomology with stack cohomology. The main point is an extension of Lemma~\ref{biratinv}:
\begin{lemma}\label{stackisoms} In the above notation, for all $p,q$, there are isomorphisms
\begin{gather*}
H^q(\uhX; \Omega^p_{\uhX}(\log \uE)) \cong H^q(X^{\#}; \Omega^p_{X^{\#}}(\log E)) \cong H^q(\hX; \Omega^p_{\hX}(\log \widehat{E}));\\
H^q(\uhY; \Omega^p_{\uhY}(\log \uE)) \cong H^q(Y^{\#}; \Omega^p_{Y^{\#}}(\log E)) \cong H^q(\hY; \Omega^p_{\hY}(\log \widehat{E})),
\end{gather*}
where $\Omega^p_{X^{\#}}(\log E)$ and $\Omega^p_{Y^{\#}}(\log E)$ are the sheaves defined by Steenbrink on the spaces $X^{\#}$ and $Y^{\#}$. Likewise, with similar definitions of $\Omega^p_{X^{\#}}(\log E)(-E)$ and $\Omega^p_{Y^{\#}}(\log E)(-E)$, 
\begin{gather*}
H^q(\uhX; \Omega^p_{\uhX}(\log \uE)(-E)) \cong H^q(X^{\#}; \Omega^p_{X^{\#}}(\log E)(-E)) \cong H^q(\hX; \Omega^p_{\hX}(\log \widehat{E})(-\widehat{E}));\\
H^q(\uhY; \Omega^p_{\uhY}(\log \uE)(-E)) \cong H^q(Y^{\#}; \Omega^p_{Y^{\#}}(\log E)(-E)) \cong H^q(\hY; \Omega^p_{\hY}(\log \widehat{E})(-\widehat{E})).
\end{gather*}
\end{lemma}
\begin{proof} These statements follow from  the arguments of \cite[Lemma 3.13, Lemma 3.14]{FL} and Lemma~\ref{biratinv}.
\end{proof} 
Thus for example in the situation of Lemma~\ref{0.2}, we have the following:
\begin{corollary}\label{0.2.2} If all of the singularities of $Y$ are weighted homogeneous isolated $1$-rational singularities, then there is a commutative diagram 
$$\begin{CD}
H^1(\uhY; \Omega^{n-1}_{\uhY}(\log \uE)(-\uE)) @>>> H^0(Y;R^1\pi_*\Omega^{n-1}_{\uhY}(\log \uE)(-\uE))\\
  @V{\cong}VV @VV{\cong}V \\
  H^1(\hY; \Omega^{n-1}_{\hY}(\log \widehat{E})(-\widehat{E})) @>>> H^0(Y;R^1\pi_*\Omega^{n-1}_{\hY}(\log \widehat{E})(-\widehat{E})) \\
  @V{\cong}VV @VV{\cong}V \\
\mathbb{T}^1_Y @>>>  H^0(Y;T^1_Y) .
\end{CD}$$
Here $H^0(Y;R^1\pi_*\Omega^{n-1}_{\uhY}(\log \uE)(-\uE))$ is a direct sum of terms isomorphic to the corresponding local terms $H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-\uE))$. \qed
\end{corollary}
In the local setting, we note  the following for future reference:
\begin{lemma}\label{PRseq} There is an exact sequence
$$0 \to \Omega^k_{\uE} \to \Omega^k_{\uhX}(\log \uE)|\uE \to \Omega^{k-1}_{\uE} \to 0.$$
\end{lemma} 
\begin{proof} Poincar\'e residue induces a surjection $\Omega^1_{\hX}(\log \uE)|\uE \to \scrO_{\uE}$ whose kernel is easily checked to be $\Omega^1_{\uE}$  as $\uE$ is smooth. Taking the $k^{\text{\rm{th}}}$ exterior power gives the exact sequence. 
\end{proof}
 
\section{Local calculations}\label{s2}
\subsection{Numerology}   In this section, we consider the local case.   We keep the notation of the previous section: $X$ is the affine weighted cone over a hypersurface $E$ in a weighted projective space $WP^n$, with an isolated singularity at $0$, and $X^{\#}$ is the weighted blowup, with $\uhX$, $\uE$, and $\uP^n$ the corresponding stacks.      Let $a_1, \dots, a_{n+1}$ be the $\Cee^*$ weights, $d$ the degree of $E$, and set $w_i = a_i/d$. Setting $N = \sum_ia_i -d$,  as a line bundle on the stack $\uE$, 
  $$K_{\uE}  =\scrO_{\uE}(-N) = \scrO_{\uE}(d -\sum_ia_i).$$
Since $\sum_iw_i = N/d + 1$, the $k$-liminal condition is equivalent to:
  $$k = \sum_iw_i -1 = N/d \iff N = dk.$$
  Thus $K_{\uE}  =\scrO_{\uE}(-dk)$. As for $K_{\uhX}$, we have $K_{\uhX} = \scrO_{\uhX}(r\uE)$ for some $r\in \Zee$. By adjunction,
  $$K_{\uE}  =\scrO_{\uE}(-dk) = K_{\uhX} \otimes \scrO_{\uhX}(\uE)|\uE = \scrO_{\hX}((r+1)\uE)|\uE = \scrO_{\uE}(-(r+1)\uE).$$
  Thus $r+1 = dk$, $r = dk -1$, and 
  $$K_{\uhX} = \scrO_{\uhX}((dk -1)\uE)  = \scrO_{\uhX}((N-1)\uE).$$
  
  To simplify the  notation,  set
  $$a = d(k-1) = N-d = \sum_ia_i -2d.$$
  Thus $a=0$ $\iff$ $k=1$, i.e.\ $X$ is $1$-liminal. Moreover, 
  $$K_{\uE}(a) = K_{\uE}  \otimes \scrO_{\uE}(a)=\scrO_{\uE}(-d).$$
  
  \subsection{Some cohomology calculations} 
  
\begin{assumption}\label{ass2.1} From now on, we assume that $X$ is a $k$-liminal weighted homogeneous isolated hypersurface singularity with $k \ge 2$.  In particular, $X$ is $1$-rational, so that Lemma~\ref{0.2} and Corollary~\ref{0.2.2} apply.  
\end{assumption}
 
 \begin{lemma}\label{0.4} With notation as above, if $j\le 2$ and $1\le i \le a-1$, then 
 $$H^j(\uE; \Omega^{n-1}_{\uE}(i))  =  H^j(\uE; \Omega^{n-2}_{\uE}(i))  = 0.$$
 For $i=a$, we have  $H^j(\uE; \Omega^{n-1}_{\uE}(a))=0$ for $j\le 2$ and $H^j(\uE; \Omega^{n-2}_{\uE}(a)) =0$ for $j=0,2$, but $\dim H^1(\uE; \Omega^{n-2}_{\uE}(a))  = 1$.
 \end{lemma}
 \begin{proof}  First, 
 $H^j(\uE; \Omega^{n-1}_{\uE}(i)) =  H^j(\uE; K_{\uE}(i)) = H^j(\uE; \scrO_{\uE}(-kd+i))$. 
 We have the exact sequence
 $$0\to \scrO_{\uP^n}(r) \to \scrO_{\uP^n}(r+d) \to \scrO_{\uE}(r+d) \to 0.$$
 Since $H^i(\uP^n; \scrO_{\uP^n}(r))=0$ for $i=1,2,3$ and all $r$,  $H^j(\uE; K_{\uE}(i)) =0$ for $j=1,2$ and all $i$. For $j=0$,  since $0\le i \le a-1 = kd-d-1$,  $-kd+i\le -d-1 < 0$, and hence $H^0(\uE; K_{\uE}(i)) = H^0(\uE; \scrO_{\uE}(-kd+i)) =0$ in this range as well.
 
 For $H^j(\uE; \Omega^{n-2}_{\uE}(i))$, note first that, as $E$ has dimension $n-1$, 
 $$\Omega^{n-2}_{\uE}(i) \cong T_{\uE} \otimes K_{\uE}(i) = T_{\uE}(-kd+i).$$
 From the  normal bundle sequence
 $$0 \to T_{\uE} \to T_{\uP^n}|\uE \to \scrO_{\uE}(d) \to 0,$$
 we therefore obtain
 $$0\to T_{\uE}(-kd+i) \to T_{\uP^n}(-kd+i)|\uE \to \scrO_{\uE}(-kd+d+i)\to 0.$$
 For $i \le a-1$, $-kd+d+i \le -1$. Then an argument as before shows that, for $j\le 2$, 
 $$H^j(\uE; \Omega^{n-2}_{\uE}(i)) \cong H^j(\uE; T_{\uP^n}(-kd+i)|\uE).$$
 We have  the Euler exact sequence
 $$0 \to \scrO_{\uE} \to \bigoplus_{i=1}^{n+1}\scrO_{\uE}(a_i)  \to T_{\uP^n}|\uE \to 0.$$
 Still assuming that $j\le 2$ and $i\le a-1$, it suffices to show that
 $$H^{j+1}(\uE; \scrO_{\uE}(-kd+i)) = H^j(\uE; \scrO_{\uE}(-kd+i+a_i))=0$$
 for $j\le 2$.  This is certainly true if $n\ge 5$, again using   $-kd+i+a_i \le a_i-d  \le -1$, since $X$ is not smooth and hence $a_i < d$. For $n=4$, 
  $H^3(\uE; \scrO_{\uE}(-kd+i))= H^3(\uE;K_{\uE}(i))$
  which is Serre dual to $H^0(\uE; \scrO_{\uE}(-i))$ so we are done as before since $i\ge 1$.
  
  To prove the second statement, note that $\Omega^{n-1}_{\uE}(a) = K_{\uE}(a) = \scrO_{\uE}(-d)$ and   $H^j(\uE; \scrO_{\uE}(-d)) =0$ for $j \le 2$ by the same reasons as before. Likewise, $\Omega^{n-2}_{\uE}(a) \cong T_{\uE} \otimes K_{\uE}(a) = T_{\uE}(-d)$. Via the Euler exact sequence
   $$0 \to \scrO_{\uE}(-d) \to \bigoplus_{i=1}^{n+1}\scrO_{\uE}(a_i-d)  \to T_{\uP^n}(-d)|\uE \to 0,$$
   we see that $H^j(\uE; T_{\uP^n}(-d)|\uE)=0$ for $j\le 2$. Moreover the normal bundle sequence gives
   $$0 \to T_{\uE}(-d) \to T_{\uP^n}(-d)|\uE \to \scrO_{\uE} \to 0.$$
   Thus  $H^0(\uE; T_{\uE}(-d)) = H^2(\uE; T_{\uE}(-d))=0$ but the coboundary map $H^0(\scrO_{\uE}) \to H^1(\uE; T_{\uE}(-d))$ is an isomorphism. 
 \end{proof}
 
  \begin{corollary}\label{0.4.1} Under Assumption~\ref{ass2.1}, 
  \begin{enumerate}
  \item[\rm(i)]   $H^0(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-i\uE)|\uE)=0$ for $1 \le i\le a$;
   \item[\rm(ii)] $H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-i\uE)|\uE)=0$ for $1\le i< a$;
    \item[\rm(iii)]  $\dim H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE) (-a\uE)|\uE) = 1$. 
    \end{enumerate} 
 \end{corollary}
 \begin{proof} 
   By Lemma~\ref{PRseq},  there is an exact sequence
   $$0 \to \Omega^{n-1}_{\uE}(i)  \to \Omega^{n-1}_{\uhX}(\log \uE)(-i\uE)|\uE  \to \Omega^{n-2}_{\uE}(i) \to 0.$$
  By   Lemma~\ref{0.4}, if $1\le i\le a$, then $H^0(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-i\uE)|\uE)=0$,   and  
  $$H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-i\uE)|\uE )   \cong  H^1(\uE;\Omega^{n-2}_{\uE}(i) ),$$ which is $0$ for $1\le i< a$ and has dimension $1$ for $i=a$. 
 \end{proof} 
 \begin{theorem}\label{0.7.1} Under Assumption~\ref{ass2.1},  
 $$  H^0(X;T^1_X) \cong  H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -a\uE) ).$$
  Moreover, the natural map 
 $H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -a\uE))  \to H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE)  ( -a\uE) |\uE)$ induces an isomorphism 
 $$H^0(X;T^1_X)/\mathfrak{m}_xH^0(X;T^1_X) \cong H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE) ( -a\uE) |\uE) .$$
 \end{theorem}
 \begin{proof} For the first part, we have an exact sequence
 $$0 \to \Omega^{n-1}_{\uhX}(\log \uE) ( -(i+1)\uE) \to \Omega^{n-1}_{\uhX}(\log \uE) ( -i\uE) \to \Omega^{n-1}_{\uhX}(\log \uE) ( -i\uE) |\uE   \to 0.$$
 Thus, by Corollary~\ref{0.4.1}, for $1\le i< a$ we have an isomorphism 
 $$H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -(i+1)\uE) \to H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -i\uE))$$
 and by induction, starting with the isomorphism $H^0(X;T^1_X)\cong H^1(\uhX; \Omega^{n-1}_{\uhY}(\log \uE)(-\uE))$ of Lemma~\ref{0.2} and Corollary~\ref{0.2.2}, we see that $H^0(X;T^1_X) \cong  H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -a\uE) )$.  
 
 To see the final statement, we have an exact sequence
 $$H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)( -(a+1)\uE)) \to H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)( -a\uE))\to H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE)  ( -a\uE) |\uE),$$
 and hence an injection 
 $$H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -a\uE))/\im H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -(a+1)\uE)\to H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE)  ( -a\uE) |\uE).$$
 By Corollary~\ref{0.4.1}(iii),  $\dim H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE) ( -a\uE) |\uE) = 1$. Thus, if the map 
 $$H^0(X;T^1_X)/\mathfrak{m}_xH^0(X;T^1_X) \to H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE)  ( -a\uE) |\uE)$$ is nonzero, it is an isomorphism. However, to prove that this map is nonzero, it is necessary to consider the $\Cee^*$ picture as in \cite[\S3]{FL}: The vector bundle $\Omega^{n-1}_{\uhX}(\log \uE) $ on  $\uhX$ is of the form $\rho^*W$ for some vector bundle $W$ on $\uE$, where $\rho\colon \uhX \to \uE$ is the natural morphism, and $\scrO_{\uhX}(-\uE) = \rho^*\scrO_{\uE}(1)$.  Then
 $$ H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE)) = \bigoplus_{r\ge 0}H^1(\uE; W(r)) = \bigoplus_{r\ge -N}H^0(X;T^1_X)(r) = \bigoplus_{r\ge -d}H^0(X;T^1_X)(r).$$
Here, the final equality holds because $-d$ is the smallest weight occurring in $H^0(X;T^1_X)$ and $-N = -dk \le -d$.  Note also that $\bigoplus_{r\ge -d+1}H^0(X;T^1_X)(r) = \mathfrak{m}_xH^0(X;T^1_X)$. 
 Taking the tensor product with $\scrO_{\uhX}(-i\uE)$ has the effect of shifting the weight spaces by $i$, since 
 $$\rho^*W \otimes \scrO_{\uhX}(-i\uE) \cong \rho^*W \otimes \rho^*\scrO_{\uE}(i) = \rho^*(W\otimes \scrO_{\uE}(i)).$$
  Thus 
 $$ H^1(\uhX;\Omega^{n-1}_{\uhX}(\log \uE)( -i\uE)) = \bigoplus_{r\ge 0}H^1(\uE; W(r+i)) = \bigoplus_{r\ge i}H^1(\uE; W(r))= \bigoplus_{r \ge -N+i}H^0(X;T^1_X)(r).$$
Here $-d \ge -N + i$ $\iff$ $i \le N -d = a$. This recovers the fact  that $H^1(\uhX; \Omega^{n-1}_{\hX}(\log \uE)( -i\uE)) \cong H^0(X;T^1_X)$ for $i \le a$, whereas 
$$H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE) ( -(a+1)\uE)) = \bigoplus_{r \ge -d+1}H^0(X;T^1_X)(r) = \mathfrak{m}_xH^0(X;T^1_X)$$
as claimed. 
  \end{proof} 
  
\subsection{Definition of the nonlinear map}  We now consider the analogue of \cite[ Lemma 4.10]{RollenskeThomas}. First, we have the subsheaf $T_{\uhX}(-\log \uE) \subseteq T_{\uhX}$ (on the stack $\uhX$), the kernel of the map $T_{\uhX}\to N_{\uE/\uhX}$, which is   dual to  the inclusion $\Omega^1_{\uhX}\subseteq \Omega^1_{\uhX}(\log \uE)$.    There is thus a commutative diagram
 $$\begin{CD}
 T_{\uhX}(-\log \uE) @>>> T_{\uhX}\\
 @V{\cong}VV @VV{\cong}V \\
  \Omega^{n-1}_{\uhX}(\log \uE)(-\uE)  \otimes K_{\uhX}^{-1} @>>>  \Omega^{n-1}_{\uhX}  \otimes K_{\uhX}^{-1}
 \end{CD}$$
  There are compatible isomorphisms 
 \begin{align*}
 T_{\uhX}(d\uE) &\cong \Omega^{n-1}_{\uhX}\otimes K_{\uhX}^{-1} \otimes \scrO_{\uhX}(  d\uE)  =  \Omega^{n-1}_{\uhX}(d-dk +1)\uE)=    \Omega^{n-1}_{\uhX}(-a\uE + \uE);\\
 T_{\uhX}(-\log \uE)(d\uE)  &\cong \Omega^{n-1}_{\uhX}(\log\uE)(-\uE) \otimes K_{\uhX}^{-1} \otimes \scrO_{\uhX}(  d\uE)  \\
 &=   \Omega^{n-1}_{\uhX}(\log \uE)(d-dk )\uE)=    \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE).
 \end{align*}
 Taking $k^{\text{\rm{th}}}$ exterior powers,    $\bigwedge ^kT_{\uhX}$ is dual to $\Omega^k_{\uhX}$ and hence is isomorphic to $\Omega^{n-k}_{\uhX}\otimes K_{\uhX}^{-1} $ and  $\bigwedge ^kT_{\uhX}(-\log \uE)$ is dual to $\Omega^k_{\uhX}(\log \uE)$ and hence is isomorphic to $\Omega^{n-k}_{\uhX}(\log \uE)(-\uE)\otimes K_{\uhX}^{-1} $.  There are compatible isomorphisms
  \begin{gather*} 
   \bigwedge ^k(T_{\uhX}(d\uE)) \cong \Omega^{n-k}_{\uhX}\otimes K_{\uhX}^{-1}\otimes\scrO_{\uhX}(dk\uE)= \Omega^{n-k}_{\uhX}\otimes\scrO_{\uhX}(\uE);   \\
  \bigwedge ^k(T_{\uhX}(-\log \uE)(d\uE)) \cong \Omega^{n-k}_{\uhX}(\log \uE)(-\uE)\otimes K_{\uhX}^{-1}\otimes\scrO_{\uhX}(dk\uE)= \Omega^{n-k}_{\uhX} (\log \uE).
    \end{gather*}
  
  
 So we have a commutative diagram
 $$\begin{CD}
 \bigwedge ^k(T_{\uhX}(-\log \uE)(d\uE)) \cong  \bigwedge ^k\left(  \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)\right) @>>>  \bigwedge ^k\left(  \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE\right)\\
 @V{\cong}VV   @VV{\cong}V \\
 \Omega^{n-k}_{\uhX} (\log \uE) @>>> \Omega^{n-k}_{\uhX} (\log \uE)|\uE.
 \end{CD}$$
  There is also the induced map $ T_{\uhX}(-\log \uE) \to T_{\uE}$, and the following commutes:
  $$\begin{CD}
  T_{\uhX}(-\log \uE) @>>> T_{\uE} \\ 
  @V{\cong}VV @VV{\cong}V\\
 \Omega^{n-1}_{\uhX}(\log \uE)(-\uE) \otimes  K_{\uhX}^{-1} @>{\operatorname{Res}}>> \Omega^{n-2}_{\uE} \otimes  K_{\uE}^{-1},
\end{CD}$$
using the adjunction isomorphism $\scrO_{\uhX}(-\uE)\otimes  K_{\uhX}^{-1}|\uE = ( K_{\uhX} \otimes \scrO_{\uhX}(\uE))^{-1}|\uE \cong K_{\uE}^{-1}$. 
  
  
  
  The  exact sequence of Lemma~\ref{PRseq} yields an exact sequence
$$0 \to \Omega^{n-1}_{\uE}(a) \to \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE \to \Omega^{n-2}_{\uE} (a)\to 0.$$
By Lemma~\ref{0.4}, there is an induced  isomorphism $H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE) \to H^1(\uE; \Omega^{n-2}_{\uE} (a))$. Moreover, 
$$\Omega^{n-2}_{\uE} (a) \cong T_{\uE} \otimes K_{\uE}(a) = T_{\uE}(-kd+a) = T_{\uE}(-d).$$
Taking $k^{\text{th}}$ exterior powers,
$$\bigwedge^k(T_{\uE}(-d)) = \left(\bigwedge^kT_{\uE}\right)(-kd) = \left(\bigwedge^kT_{\uE}\right)\otimes K_{\uE} \cong \Omega^{n-k-1}_{\uE}.$$
A combination of wedge product and cup product induces  symmetric homogeneous  degree $k$ maps 
\begin{gather*}
\nu_{\uhX}\colon H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)) \to H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE));\\
\mu_{\uhX}= \nu_{\uhX} |\uE\colon H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE) \to H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)|\uE),
\end{gather*}
 and a commutative diagram  (with nonlinear vertical maps)
 
$$\begin{CD}
 H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)) @>>> H^1(\uE; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE) \\
@V{\nu_{\uhX}}VV @VV{\mu_{\uhX}}V\\
 H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)) @>>> H^k(\uE;\Omega^{n-k}_{\uhX} (\log \uE)|\uE) .
\end{CD}$$
There are similarly  compatible  symmetric homogeneous  degree $k$ maps 
\begin{gather*}
\nu_{\uhX}' \colon H^1(\uhX; T_{\uhX}(-\log \uE)(dE) ) \to H^k(\uhX; \bigwedge ^k(T_{\uhX}(-\log \uE)(dE)) )\cong H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE));\\
\mu_{\uhX}'\colon H^1(\uE; T_{\uE}(-d)) \cong  H^1(\uE; \Omega^{n-2}_{\uE} (a)) \to H^k(\uE; \bigwedge^k(T_{\uE}(-d))) \cong H^k(\uE; \Omega^{n-k-1}_{\uE}).
\end{gather*}
The following  diagram with nonlinear vertical maps commutes:
$$\begin{CD}
 H^1(E; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE)  @>{\cong}>> H^1(\uE; T_{\uE}(-d)) \cong  H^1(\uE; \Omega^{n-2}_{\uE} (a))\\
 @V{\mu_{\uhX}}VV @VV{\mu_{\uhX}'}V\\
H^k(E;\Omega^{n-k}_{\uhX} (\log \uE)|\uE) @>{\operatorname{Res}}>> H^k(\uE; \bigwedge^k(T_{\uE}(-d))) \cong H^k(\uE; \Omega^{n-k-1}_{\uE}).
 \end{CD}$$
By Corollary~\ref{0.4.1}(iii), $H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE) \cong H^1(\uE; T_{\uE}(-d))$ has dimension one, as does $H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)|\uE) = \Gr^{n-k}_FH^n(L)$. 
The map $H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)) \to H^k(\uE; \Omega^{n-k-1}_{\uE})$ factors through the (surjective) map $H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)) \to H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)|\uE) = \Gr^{n-k}_FH^n(L)$,   and the map 
$$H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)|\uE) = \Gr^{n-k}_FH^n(L) \to H^k(\uE;\Omega^{n-k-1}_{\uE}) = \Gr^{n-k-1}_FH^{n-1}(E) = \Gr^{n-k}_FH^{n-1}(E)(-1)$$
is an isomorphism in almost all cases. More precisely, let $H_0^{n-1}(E)$ be the primitive cohomology of $E$ in dimension $n-1$ and let  $ H_0^{n-1-k}(\uE;\Omega^k_{\uE}) = \Gr^k_FH_0^{n-1}(E)$ be the corresponding groups. 
\begin{lemma}\label{lemma2.4}  If $X$ is not an ordinary double point or if $n$ is even, then the map $\Gr^{n-k}_FH^n(L )\to H^k(\uE;\Omega^{n-k-1}_{\uE})$ is an isomorphism, and hence 
$$\dim H^k(\uE;\Omega^{n-k-1}_{\uE}) = 1.$$ 
If   $X$ is an ordinary double point and $n=2k+1$ is odd, then 
$$\Gr^{n-k}_FH^n(L )\to H^k(\uE;\Omega^{n-k-1}_{\uE})=\Gr^{n-k-1}_FH^{n-1}(E)$$ is injective with image     $ H_0^k(\uE;\Omega^k_{\uE}) =\Cee([A]-[B])$, and hence $\dim H_0^k(\uE;\Omega^k_{\uE}) = 1$ . 
\end{lemma}
\begin{proof} As noted in Definition~\ref{def0.3}, if $L$ is the link of the singularity, then $\dim \Gr^{n-k}_FH^n(L) =1$.  Then Lemma~\ref{PRseq} gives the exact sequence
$$\Gr^{n-k}_FH^n(\uE) \to \Gr^{n-k}_FH^n(L) \to \Gr^{n-k-1}_FH^{n-1}(\uE) \to \Gr^{n-k}_FH^{n+1}(\uE).$$
Since $E$ is an orbifold  weighted hypersurface in $WP^n$, $\Gr^i_FH^j(\uE)=0$ except for the cases $j=2i$ or $i+j = n-1$. Thus  $\Gr^{n-k}_FH^n(\uE) = \Gr^{n-k-1}_FH^{n-1}(\uE) =0$ unless $n = 2(n-k)$, i.e.\ $k=\frac12n$, or $n+1 = 2(n-k)$, i.e.\ $k=\frac12(n-1)$. The first case is excluded since we assumed that $X$ is a singular point and the second case only arises if $n=2k+1$ and $X$ is an ordinary double point (Lemma~\ref{ex0.4}). This proves the first statement, and the second statement is the well-known computation of the primitive cohomology of an even-dimensional quadric. 
\end{proof}
 \begin{proposition}\label{0.9} The map $\mu_{\uhX}$ is not $0$. Hence there exist bases  $v\in H^1(\uhX; \Omega^{n-1}_{\uhX}(\log \uE)(-a\uE)|\uE)$ and $\varepsilon \in H^k(\uhX;\Omega^{n-k}_{\uhX} (\log \uE)|\uE)$ of the two one-dimensional vector spaces and a nonzero $c\in \Cee$ such that, for all $\lambda \in \Cee$,
 $$\mu_{\uhX}(\lambda v) = c\lambda^k\varepsilon.$$
 \end{proposition}
 \begin{proof} It suffices to prove that the map $\mu_{\uhX}'$ is nonzero. Taking the $(i+1)^{\text{\rm{st}}}$ exterior power  of the normal bundle sequence 
 $$0 \to T_{\uE}(-d) \to T_{\uP^n}|\uE(-d) \to \scrO_{\uE}  \to 0$$ gives   exact sequences
 \begin{equation*}
 0 \to  \bigwedge^{i+1}T_{\uE} (-(i+1)d)\to  \bigwedge^{i+1}T_{\uP^n}|\uE (-(i+1)d) \to  \bigwedge^iT_{\uE} (-id)\to 0,\tag{$*$}
 \end{equation*}
 and thus a sequence of connecting homomorphisms
 $$\partial_i \colon H^i(\uE;  \bigwedge^iT_{\uE} (-id)) \to H^{i+1}(\uE;  \bigwedge^{i+1}T_{\uE} (-(i+1)d).$$
 We claim the following: 
 
 \begin{claim}\label{0.10}  There exists a nonzero element $\eta \in H^1(\uE;T_{\uE}(-d))$, necessarily a generator,  such that $\mu'_{\uhX}(\eta)  = \pm\partial_{k-1} \circ \cdots \circ   \partial_1(\eta)$.
 \end{claim}
 
 \begin{claim}\label{0.11} The connecting homomorphism $\partial_i$ is an isomorphism for $1\leq i \leq k-2$ and injective for $i=k-1$.
 \end{claim}
 Clearly the two claims imply Proposition~\ref{0.9}.
 \renewcommand{\qedsymbol}{}
 \end{proof}
 
 \begin{proof}[Proof of Claim~\ref{0.10}] The element $\eta =\partial_0(1)\in H^1(\uE;T_{\uE}(-d))$ is the extension class for the extension $0 \to T_{\uE}(-d) \to T_{\uP^n}|\uE(-d) \to \scrO_{\uE} \to 0$. Then a calculation shows that, up to sign,  
 $$\wedge \eta \in H^1(\uE; Hom( \bigwedge^iT_{\uE} (-id),  \bigwedge^{i+1}T_{\uE} (-(i+1)d))$$
 is the corresponding extension class for the extension $(*)$. Since the connecting homomorphism is given by cup product with the extension class, we see that 
 $$\mu'_{\uhX}(\eta) = \eta^k =  \pm\partial_{k-1} \circ \cdots \circ   \partial_1(\eta) \in H^k(\uE;  \bigwedge^kT_{\uE} (-kd)).\qed$$
 \renewcommand{\qedsymbol}{}
 \end{proof}
 
 \begin{proof}[Proof of Claim~\ref{0.11}] It suffices to show that $H^i(\uE;  \bigwedge^{i+1}T_{\uP^n}|\uE (-(i+1)d)) =0$ for $1\le i \leq k-1$. First note that
 $$ \bigwedge^{i+1}T_{\uP^n}|\uE (-(i+1)d)) = \Omega^{n-i-1}_{\uP^n} (\sum_ka_k -(i+1)d).$$
 We have the exact sequence
 $$0 \to \Omega^\ell_{\uP^n}(r-d) \to \Omega^\ell_{\uP^n}(r)\to  \Omega^\ell_{\uP^n}|\uE (r) \to 0.$$
 By Bott vanishing, $H^j(\uE;  \Omega^\ell_{\uP^n}|\uE (r)) = 0$ as long as $j\leq n-2$ and $j\neq \ell$ or $\ell+1$. In our situation, $i\leq k-1 < k \le \frac12(n-1)$, and thus $i < n-i-1$. In particular, $i \neq n-i-1$ or $n-i$. Thus $H^i(\uE;  \bigwedge^{i+1}T_{\uP^n}|\uE (-(i+1)d)) =0$. 
 \end{proof}
 
 \begin{remark} The above calculations are connected with the computation of the Hodge filtration on $E$. For example, in case $X$ is a cone over the smooth degree $d$ hypersurface $E$ in $\Pee^n$, then  $H^0(\Pee^n; K_{\Pee^n}\otimes (n-k)d)  =H^0(\Pee^n;\scrO_{\Pee^n}(-n-1 + d(k+1)) = H^0(\Pee^n;\scrO_{\Pee^n})$ is identified via residues with $F^{n-k}H^n(L) = \Gr_F^{n-k}H^n(L)$.
 \end{remark}
 
 \section{The global setting}\label{s3}
 
\subsection{Deformation theory}\label{ss3.1} We assume the following for the rest of this subsection:
\begin{assumption}\label{ass3.1} $Y$ is a canonical Calabi-Yau variety of dimension $n\ge 4$, all of whose singularities are $k$-liminal isolated weighted homogeneous hypersurface singularities, with $k \ge 2$, as the case $k=1$ has already been considered in Theorem~\ref{1limthm}.  We freely use  the notation of the previous sections, in particular that of Definition~\ref{def1.8}.
\end{assumption}  
 
  The argument of Theorem~\ref{0.7.1} shows:
 
\begin{lemma} There is a commutative diagram
 $$\begin{CD}
 \mathbb{T}^1_Y  @>{\cong}>>  H^1(\uhY; \Omega^{n-1}_{\uhY}(\log \uE) ( -a\uE)) \\
 @VVV @VVV\\
 H^0(Y; T^1_Y)@>{\cong}>> H^1(Y; R^1\pi_*\Omega^{n-1}_{\uhY}(\log \uE) ( -a\uE)).\qed
 \end{CD}$$ 
\end{lemma}
We also have the subsheaf $T_{\uhY}(-\log \uE) \subseteq T_{\uhY}$. As in \S2, globally there is an isomorphism
 $$ \bigwedge ^k\left(  \Omega^{n-1}_{\uhY}(\log \uE)(-a\uE)\right) \cong \Omega^{n-k}_{\uhY} (\log \uE).$$
 
 Then the global form  of the discussion in \S\ref{s2}  yields:
 
 
 \begin{theorem}\label{mainCD} There is  a commutative diagram 
 $$\begin{CD}
H^1(\uhY; \Omega^{n-1}_{\uhY}(\log \uE) ( -a\uE)) @>>> H^1(\uE;\Omega^{n-1}_{\uhY}(\log \uE)( -a\uE)|\uE )\\
 @V{\nu_{\uhY}}VV @VV{\mu_{\uhY}}V  \\
 H^k(\uhY; \Omega^{n-k}_{\uhY}(\log \uE)) @>>>  H^k(\uE; \Omega^{n-k}_{\uhY}(\log\uE)|\uE) .\qed
 \end{CD}$$
 \end{theorem}
 
 
 Here $\mu_{\uhY}$ is the sum of the local maps $\mu_{\uhX}$ at each component of $E$ and $\nu_{\uhY}$ is also a  homogeneous map of degree $k$. Note that,   after we localize at a singular point $x$ of $Y$,   
 $$\dim H^k(\uE_x; \Omega^{n-k}_{\uhX}(\log \uE_x)|\uE_x) = \dim \Gr^{n-k}_FH^n(L_x) = 1.$$
 By Corollary~\ref{0.4.1}(iii), $\dim H^1(\uhY;\Omega^{n-1}_{\uhY}(\log \uE ) ( -a\uE)|\uE_x ) =1$ as well, and so the $\mu_{\uhY}$ in the diagram, at each singular point $x$ of $Y$, is a homogeneous degree $k$ map between two one-dimensional vector spaces.  Fixing once and for all an isomorphism  $H^1(\uhY;\Omega^{n-1}_{\uhY}(\log \uE ) ( -a\uE)|\uE_x ) \cong \Cee$, i.e.\ a basis vector $v_x \in  H^1(\uhY;\Omega^{n-1}_{\uhY}(\log \uE ) ( -a\uE)|\uE_x ) $, and a basis vector $\varepsilon_x  \in \Gr^{n-k}_FH^n(L_x)$, it follows by  Proposition~\ref{0.9} that there exists nonzero $c_x\in \Cee$ such that, for every $\lambda = (\lambda_x) \in \Cee^Z\cong H^1(\uE;\Omega^{n-1}_{\uhY}(\log \uE)( -a\uE)|\uE )$,
 $$\mu_{\uhY}(\lambda) = \sum_{x\in Z} c_x\lambda_x^k \varepsilon_x.$$ 
 
 Consider the following diagram with exact rows:
 $$\begin{CD}
H^1(\Omega^{n-1}_{\uhY}(\log \uE)( -a\uE)) @>>> H^1(\Omega^{n-1}_{\uhX}(\log \uE)( -a\uE)|\uE )@>>> H^2(\Omega^{n-1}_{\uhY}(\log \uE)( -(a+1)\uE))\\
 @V{\nu_{\uhY}}VV @VV{\mu_{\uhY}}V @. \\
 H^k( \Omega^{n-k}_{\uhY}(\log \uE)) @>>>  H^k(\Omega^{n-k}_{\uhY}(\log \uE)|\uE) @>{\partial}>>  H^{k+1}(\Omega^{n-k}_{\uhY}(\log \uE)(-\uE)).
 \end{CD}$$
By referring to the above diagram, we see the following: if a class $\alpha = (\alpha_x) \in H^1(\uE; \Omega^{n-1}_{\uhX}(\log \uE) \otimes \scrO_{\uhX}( -a\uE)|\uE )$ is   the image of $\beta \in H^1(\uhY; \Omega^{n-1}_{\uhY}(\log \uE)( -a\uE))$, then $\mu_{\uhY}(\alpha)$ is   the image of $\nu_{\uhY}(\beta) \in H^k(\uhY;\Omega^{n-k}_{\uhY}(\log \uE)) $ and  hence $\partial(\mu_{\uhY}(\alpha)) = 0$ in $H^{k+1}(\Omega^{n-k}_{\uhY}(\log \uE)(-\uE))$.  
 
We now consider the non-stacky situation of a log resolution $\pi \colon \hY \to Y$ with exceptional divisor which we continue to denote by $E$. Then via the isomorphism $H^{k+1}(\uhY; \Omega^{n-k}_{\uhY}(\log \uE)(-\uE)) \cong H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$ of Lemma~\ref{stackisoms}, we can also view  the coboundary $\partial(\mu_{\uhY}(\alpha))$ as an element of $H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$. Moreover, if $\partial(\mu_{\uhX}(\alpha))$ is of the form  $\sum_{x\in Z} c_x\lambda_x^k \partial(\varepsilon_x)$, then it has the same form when viewed as an element of $H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$, by the commutativity of the diagram 
$$\begin{CD}
H^k(\uhY; \Omega^{n-k}_{\uhY}(\log \uE)|\uE) @>{\partial}>>  H^{k+1}(\uhY; \Omega^{n-k}_{\uhY}(\log \uE)(-\uE))\\
@V{=}VV  @VV{\cong}V\\
H^k(\hY; \Omega^{n-k}_{\hY}(\log E)|E) @>{\partial}>>  H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E).
\end{CD}$$
 
 
 
So finally we obtain:
\begin{theorem}\label{mainthma} Suppose that the class $(\bar\theta_x) \in \bigoplus_{x\in Y_{\text{\rm{sing}}}}H^0(T^1_{Y,x})/\mathfrak{m}_xH^0(T^1_{Y,x}) $ is in the image of $\theta \in \mathbb{T}^1_Y$, and let $\lambda_x \in \Cee $ be the complex number  corresponding  to $\bar\theta_x$ for some choice of isomorphisms
$$H^0(T^1_{Y,x})/\mathfrak{m}_xH^0(T^1_{Y,x}) \cong  H^1(E_x;\Omega^{n-1}_{\hX}(\log E_x)( -aE_x)|E _x) \cong \Cee.$$
 Finally, write $\Gr^{n-k}_FH^n(L_x) = H^k(E_x; \Omega^{n-k}_{\hY}(\log E_x)|E_x) =\Cee\cdot \varepsilon_x$ for some choice of a generator $\varepsilon_x$. Then, for all $x\in Z$ there exist   $c_x\in \Cee^*$ depending only on the above identifications, such that, in $H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$,
 $$\sum_{x\in Z} c_x\lambda_x^k \partial(\varepsilon_x) =0,$$
 where $\partial \colon H^k(E; \Omega^{n-k}_{\hY}(\log E)|E)\to H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$ is the coboundary map. \qed
\end{theorem}   
 We can follow  the coboundary map 
 $$\partial \colon \Gr^{n-k}_FH^n(L) = H^k(E; \Omega^{n-k}_{\hY}(\log E)|E) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$$
 with the natural map 
 $$H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY}).$$
 Let $\varphi\colon \Gr^{n-k}_FH^n(L) = H^k(E; \Omega^{n-k}_{\hY}(\log E)|E) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY})$ be the above composition. This is the same as the induced map on $\Gr^{n-k}_F$ of the natural map $H^n(L) \to H^{n+1}(\hY)$ which is the Poincar\'e dual of the map $H_{n-1}(L) \to H_{n-1}(\hY)$. 
\begin{corollary}\label{last}  With  the notation and hypotheses of Theorem~\ref{mainthma},  and with $\varphi\colon \Gr^{n-k}_FH^n(L) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY})$ the natural map as above,  the following holds in $H^{k+1}(\hY; \Omega^{n-k}_{\hY})$:
$$\sum_{x\in Z} c_x\lambda_x^k\varphi(\varepsilon_x) =0.$$ 
In particular, if a strong first order smoothing of $Y$ exists, then for all $x\in Z$ there exists $\lambda_x\in \Cee^*$  with $\sum_{x\in Z} c_x\lambda_x^k\varphi(\varepsilon_x) =0$. \qed
\end{corollary}
\begin{remark} (i) By Poincar\'e duality, the map $H^n(L) \to H^{n+1}(\hY)$ is the same as the map $H_{n-1}(L) \to H_{n-1}(\hY)$, which factors as $H_{n-1}(L) \to H_{n-1}(Y) \to H_{n-1}(\hY)$.  By Remark~\ref{duality}, we can identify $\partial \colon \Gr^{n-k}_FH^n(L)  \to H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E))$ with the corresponding map
$$\Gr^{n-k}_FH_{n-1}(L)(-n) \to \Gr^{n-k}_FH_{n-1}(Y)(-n).$$
This gives an equivalent statement to Theorem~\ref{mainthma} which only involves $Y$, not the choice of a resolution. 
\smallskip
\noindent (ii) By Lemma~\ref{lemma3.3}, the map $H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)) \to H^{k+1}(\hY; \Omega^{n-k}_{\hY})$ is injective.
 Thus $\sum_{x\in Z} c_x\lambda_x^k\varphi(\varepsilon_x) =0$ $\iff$ $\sum_{x\in Z} c_x\lambda_x^k \partial(\varepsilon_x) =0$, so that Theorem~\ref{mainthma} and Corollary~\ref{last} contain the same information.
 \end{remark}
   
  \begin{remark} As in Remark~\ref{1limFano}, we can also consider the Fano case, where $Y$ has isolated $k$-liminal weighted homogeneous hypersurface singularities and $\omega_Y^{-1}$ is ample.  In this case, the construction produces an obstruction to a strong first order smoothing, namely  $\sum_{x\in Z} c_x\lambda_x^k \partial(\varepsilon_x) \in H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)\otimes \pi^*\omega_Y^{-1})$. By Serre duality, $H^{k+1}(\hY; \Omega^{n-k}_{\hY}(\log E)(-E)\otimes \pi^*\omega_Y^{-1})$ is dual to $H^{n-k-1}(\hY; \Omega^k_{\hY}(\log E) \otimes \pi^*\omega_Y)$. 
  
  In many reasonable cases, however, $H^{n-k-1}(\hY; \Omega^k_{\hY}(\log E)\otimes \pi^*\omega_Y)=0$. For example, if there exists a smooth Cartier divisor $H$ on $Y$, thus not passing through the singular points, such that $\omega_Y =\scrO_Y(-H)$, and in addition $H^{n-k-2}(H; \Omega^k_H) = 0$, then the argument of  Remark~\ref{1limFano} shows that 
  $$H^{n-k-1}(\hY; \Omega^k_{\hY}(\log E)\otimes \pi^*\omega_Y)=H^{n-k-1}(\hY; \Omega^k_{\hY}(\log E)\otimes \scrO_{\hY}(-H))= 0,$$
  where we identify the divisor $H$ on $Y$ with its preimage $\pi^*H$ on $\hY$.  For example, these hypotheses are satisfied if $Y$ is a hypersurface in $\Pee^{n+1}$ of degree $d\leq n+1$. However, as soon as $n=2k+1$ is odd and $n\ge 5$, there exist such hypersurfaces with only nodes as singularities (the $k$-liminal case with $k = \frac12(n-1)$) such that the map $\mathbb{T}^1_Y \to H^0(Y; T^1_Y)$ is not surjective (cf.\ for example \cite[Remark 4.11(iv)]{FL}). Thus, the obstructions to the surjectivity of the map $\mathbb{T}^1_Y \to H^0(Y; T^1_Y)$ are not detected by the nonlinear obstruction $\sum_{x\in Z} c_x\lambda_x^k \partial(\varepsilon_x)$.  Of course, a nodal hypersurface in $\Pee^{n+1}$ is smoothable, but the above examples show that, even in  the  Fano case,  the nodes cannot always be smoothed independently.
  \end{remark}
  
  \subsection{Geometry of a smoothing}\label{ss3.2}  We make the following assumption throughout this subsection:
  
  \begin{assumption}  $Y$ denotes a \emph{projective} variety, not necessarily satisfying $\omega^{-1}_Y$ ample or $\omega_Y \cong \scrO_Y$,  with only isolated lci singular points (not necessarily weighted homogeneous). Denote by $Z$ the singular locus of $Y$.  Let $f\colon \mathcal{Y} \to \Delta$  be a \emph{projective} smoothing of $Y$, i.e.\ $Y_0 = Y \cong f^{-1}(0)$ and the remaining fibers $Y_t = f^{-1}(t)$, $t\neq 0$, are smooth. For $x\in Z$,  let $L_x$ denote the link at $x $ and let $M_x$ denote the Milnor fiber  at $x$. Finally, let  $M =\bigcup_{x\in Z}M_x$ and $L= \bigcup_{x\in Z}L_x$.
  \end{assumption}   
  
  
We have the Mayer-Vietoris sequence of mixed Hodge structures (where $H^i(Y_t)$ is given the limiting mixed Hodge structure): 
\begin{equation}\label{MVMilnor}
 \cdots \to H^{i-1}(M) \to  H^i(Y,Z) \to H^i(Y_t) \to H^i(M) \to \cdots .
 \end{equation}
In particular, just under the assumption that  $Y$ has isolated lci singularities, $H^i(Y,Z) \to H^i(Y_t)$ is an isomorphism except for the cases $i =n, n+1$.  There is a more precise result if we assume that  the singularities are $k$-Du Bois:
\begin{lemma}\label{first}  Suppose that all singular points of $Y$ are isolated lci $k$-Du Bois singularities. Then    
\begin{enumerate} 
\item[\rm(i)] $\Gr^p_FH^n(M_x)=0$ for $p\le k$ and   $\Gr^{n-p}_FH^n(M_x)=0$ for $p\le k-1$.
\item[\rm(ii)]  For all $i$, if  $p\le k$, then $\Gr^p_FH^i(Y_t) = \Gr^p_FH^i(Y)$ and 
 if  $p\le k-1$, then $\Gr^{n-p}_FH^i(Y_t) = \Gr^{n-p}_FH^i(Y)$.
\end{enumerate}   
\end{lemma} 
\begin{proof} The first statement follows from  \cite[\S6]{FL22d} and the second  from (i), (\ref{MVMilnor}),  and strictness. 
\end{proof} 
\begin{remark} Under the assumption of  isolated lci $k$-Du Bois singularities as above (or more generally  isolated lci $(k-1)$-rational singularities), the above implies that $\Gr^W_{2n-a}H^n(Y_t) = 0$   for all $a\le 2k-1$, and hence that, for all $a\le 2k-1$, $\Gr^W_aH^n(Y_t) = 0$  as well. Thus, if $T$ is the monodromy operator acting on $H^n(Y_t)$ and $N = \log T^m$ for a sufficiently divisible power of $T$, then $N^{n-2k+1} =0$. 
\end{remark}
 Under the assumption of  $k$-liminal singularities, the proof of  Lemma~\ref{first} and \cite[Corollary 6.14]{FL22d}  give the following:
 
\begin{lemma}\label{second} In the above notation, if all singular points of $Y$ are isolated $k$-liminal hypersurface singularities, then 
$$\Gr^{n-k}_FH^n(M_x) \cong \Gr^{n-k}_FH^n(L_x) =\Cee \cdot \varepsilon_x$$
for some nonzero $\varepsilon_x \in \Gr^{n-k}_FH^n(L_x)$.
 Moreover,  there is an exact sequence
$$0 \to \Gr^{n-k}_FH^n(Y) \to \Gr^{n-k}_FH^n(Y_t)\to \bigoplus_{x\in Z}\Cee \cdot \varepsilon_x \xrightarrow{\psi}   \Gr^{n-k}_FH^{n+1}(Y) \to \Gr^{n-k}_FH^{n+1}(Y_t)\to 0.\qed$$
\end{lemma}
 
We also have the natural map $\varphi\colon \Gr^{n-k}_FH^n(L) =\bigoplus_{x\in Z}\Cee \cdot \varepsilon_x\to \Gr^{n-k}_FH^{n+1}(\hY)$, and 
there is a commutative diagram
$$\begin{CD}
\Gr^{n-k}_FH^n(M) @>{\cong}>> \Gr^{n-k}_FH^n(L) \\
@V{\psi}VV @VV{\varphi}V \\
\Gr^{n-k}_FH^{n+1}(Y) @>>> \Gr^{n-k}_FH^{n+1}(\hY).
\end{CD}$$
By   Lemma~\ref{lemma3.3},   $\Gr^{n-k}_FH^{n+1}(Y) \to \Gr^{n-k}_FH^{n+1}(\hY)$ is injective. 
Thus the dimension of the kernel and image of the map $\psi \colon  \bigoplus_{x\in Z}\Cee \cdot \varepsilon_x \to  \Gr^{n-k}_FH^{n+1}(Y)$ are the same as the dimensions of the kernel and image of the  map $\varphi \colon  \bigoplus_{x\in Z}\Cee \cdot \varepsilon_x \to  \Gr^{n-k}_FH^{n+1}(\hY)$. Then we have the following generalization of  \cite[Lemma 8.1(2)]{FriedmanSurvey}:
\begin{corollary}\label{third} Still assuming that  all singular points of $Y$ are isolated $k$-liminal hypersurface singularities, in the above notation, let $s' = \dim \Ker \{\varphi  \colon \bigoplus_{x\in Z}\Cee \cdot \varepsilon_x \to  \Gr^{n-k}_FH^{n+1}(\hY)\}$ and let $s'' =\#(Z) - s'= \dim \im \varphi$. Then:
\begin{enumerate}
\item[\rm(i)]  $h^{n-k, k}(Y_t) = h^{k, n-k}(Y_t)= \dim \Gr^{n-k}_FH^n(Y_t) = \dim \Gr^{n-k}_FH^n(Y) +s'$.
\item[\rm(ii)] $\dim \Gr^k_FH^n(Y)  = \dim \Gr^{n-k}_FH^n(Y) +s'$.
\item[\rm(iii)] $h^{n-k,k+1}(Y_t)= \dim \Gr^{n-k}_FH^{n+1}(Y_t) = \dim \Gr^{n-k}_FH^{n+1}(Y) -s''$.  
\end{enumerate}
\end{corollary}
\begin{proof}  (i) and (iii) follow from the exact sequence in Lemma~\ref{second}. As for (ii), by Lemma~\ref{first}(ii), 
$$\dim \Gr^k_FH^n(Y)  =  \dim \Gr^k_FH^n(Y_t)  =\dim \Gr^{n-k}_FH^n(Y_t) = \dim \Gr^{n-k}_FH^n(Y) +s',$$
using (i). 
\end{proof}
\begin{remark}
In case $\dim Y =3$ and all singular points are $1$-liminal, hence  ordinary double points,  assume in addition   that $h^1(\scrO_Y) = h^2(\scrO_Y)=0$. If $\omega_Y\cong \scrO_Y$, this is a natural assumption to make: If  $h^1(\scrO_Y) \neq 0$, $Y$ is smooth by a result of Kawamata \cite[Theorem 8.3]{KawamataFiber}, and  $h^1(\scrO_Y) = 0$ $\iff$ $h^2(\scrO_Y) =h^2(\omega_Y) =0$ by Serre duality. Let $Y'$ be   a small resolution of $Y$, and let $[C_x]\in H^2(Y'; \Omega^2_{Y'})=H^4(Y')$ be the fundamental class of the exceptional curve over the point $x\in Z$. Setting $\psi \colon \Cee^Z \to H^2(Y'; \Omega^2_{Y'})$ to be the natural map $(a_x) \mapsto \sum_{x\in Z}a_x[C_x]$, let $s'=\dim \Ker \psi$ and $s''=\dim \im \psi$. Then  arguments similar to those above show:
\begin{enumerate}
\item[\rm(i)] $b_2(Y_t) = b_2(Y) = b_2(Y') - s''$. 
\item[\rm(ii)] $b_3(Y_t) = b_3(Y') + 2s'$.
\item[\rm(iii)]  $b_4(Y) = b_4(Y') = b_2(Y) + s''$. 
\end{enumerate}
In particular, if $Y$ is a canonical, $1$-liminal Calabi-Yau threefold and   $\psi=0$, or equivalently $s'' =0$ in the above notation, i.e.\ $Y$ is $\Q$-factorial,  then $Y$ is smoothable. 
\end{remark}
 
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
\providecommand{\MRhref}[2]{
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
"
25,"\importpackages{}
\graphicspath{ {./images/} }


\maketitle
\thispagestyle{empty}
\pagestyle{empty}
\begin{abstract}
On construction sites, progress must be monitored continuously to ensure that the current state corresponds to the planned state in order to increase efficiency, safety and detect construction defects at an early stage.
Autonomous mobile robots can document the state of construction with high data quality and consistency.
However, finding a path that fully covers the construction site is a challenging task as it can be large, slowly changing over time, and contain dynamic objects.
Existing approaches are either exploration approaches that require a long time to explore the entire building, object scanning approaches that are not suitable for large and complex buildings, or planning approaches that only consider 2D coverage.
In this paper, we present a novel approach for planning an efficient 3D path for progress monitoring on large construction sites with multiple levels. By making use of an existing 3D model we ensure that all surfaces of the building are covered by the sensor payload such as a 360-degree camera or a lidar. This enables the consistent and reliable monitoring of construction site progress with an autonomous ground robot. We demonstrate the effectiveness of the proposed planner on an artificial and a real building model, showing that much shorter paths and better coverage are achieved than with a traditional exploration planner.
\end{abstract}
\section{INTRODUCTION}
In modern construction, each stage of construction is planned as a digital model in the process of \gls{bim}. Continuously monitoring the progress of the construction ensures that the current state matches the original plan. In this way, construction defects can be detected at an early stage, improving safety and productivity during construction. The state of the construction site is documented in a ``digital twin"" that accumulates data over time such as 360-camera images or lidar point clouds.
In order to improve data quality, it is necessary to have a monitoring plan showing exactly where to record the required data. This leads to a more complete model and more consistent and comparable data.
Data for such a model could be manually captured by humans following the plan and taking photos and scans of the environment. However, humans tend to make errors when working on long repetitive tasks, leading to incomplete data.
Therefore, this task can be automated using autonomous mobile robots.
They can precisely follow the plan and capture data with high consistency. Ground robots are well suited for construction sites because they can safely navigate inside buildings and carry heavy sensor payloads.
Creating a monitoring plan that covers all surfaces of a large building is a challenging task. Existing object scanning approaches \cite{cunningham, daudelin}  are often limited to small objects and scenes, and do not allow for the robot to be inside the object or scene. Exploration approaches \cite{exp_transf, tare} on the other hand do not use prior information about the environment and take a long time to explore the complete building. There are approaches to efficiently scan existing buildings \cite{chen}, but they use a floor plan for planning and thus can only ensure 2D coverage.
We propose a novel approach to compute a time-optimal 3D path for monitoring large construction sites with multiple levels (Fig. \ref{fig:precom_res_large_target_multilevel}). By utilizing the 3D building model as prior information, we create an efficient monitoring plan for an autonomous ground robot for covering all surfaces of the construction site with a sensor payload. The approach supports various 3D sensors such as 360-degree cameras and lidar. We demonstrate the effectiveness of the proposed planner on an artificial and a real building model. In simulation, we compare our approach to an exploration planner \cite{exp_transf} showing that we achieve much shorter paths and a better coverage with a lidar scanner.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure*}[thpb]
      \centering
      [width=\textwidth]{graphics/Overview_precomputations_smaller.jpg}
      \caption{Planning pipeline}
      \label{fig:precom_overview}
\end{figure*}
\section{Related Work} \label{sec:rel_work}
Planning a path that covers all surfaces of an environment combines aspects from various research on different tasks, among others, object scanning, exploration, and the scanning of existing buildings which is often not automated with robots, but done with e.g. terrestrial laser scanners. All the mentioned tasks are mostly solved using \gls{nbv} planning. A common approach for \gls{nbv} planning is to first generate a set of candidate viewpoints based on different criteria. Afterwards, these candidates are ranked by their reward. Finally, either the best candidate is chosen as \gls{nbv} or a set of candidates is selected as viewpoints.
In the approach proposed by Cunningham-Nelson et al. \cite{cunningham}, a fixed object is scanned with a mobile manipulator. It starts with no information about the scene except the object position and first estimates a prior model by navigating to waypoints in a cone shape. For each generated candidate viewpoint, it is checked how many clusters from the prior model are in the sensor's field of view. A subset of candidate viewpoints is selected by using a greedy approach which always selects the candidate with the most seen clusters. Afterwards, one or more viewing angles are generated from which all associated clusters are visible.
Daudelin and Campbell \cite{daudelin} focus on the scanning of small object scenes of unknown size. The \gls{nbv} is computed online based on frontier cells, which are unknown cells that border empty and occupied cells and which have the highest probability of belonging to the object. The candidate positions are generated dynamically and evaluated for as many orientations as required to cover all directions of view.
Cao et al. \cite{tare} propose an exploration algorithm for large environments. The goal is to find the shortest path that covers all uncovered surfaces in the local search space. The uniformly sampled candidate viewpoints are rewarded with the area of surfaces they can cover. According to the reward of the viewpoints, a minimum set of viewpoints is chosen probabilistically. Afterwards, a path based on the viewpoints is computed by solving a \acrfull{tsp}. These two steps, the viewpoint selection and the path planning, are repeated a given number of times and the overall best solution, i.e. the one with the lowest path costs, is used. Therefore, the method optimizes the entire exploration path, instead of only maximizing the instant reward.
Based on a given 2D floor plan, Chen et al. \cite{chen} plan where to place a terrestrial laser scanner to create a 3D model of an existing building. Since here the lines that form the walls in the floor plan are used, only 2D coverage can be ensured. In order to rate the generated candidates, a sweep-ray algorithm is used to determine the visible line segments with respect to a minimum and maximum viewing distance. For selecting the minimum set of viewpoints, a greedy best-first search, a greedy algorithm with a backtracking process, and a simulated annealing algorithm are used.
\label{rel_work:current_exploration}
An exploration algorithm is used for the comparison with the proposed approach (cf. Section \ref{eval:comp_exp}). It is based on the work proposed by Wirth and Pellenz \cite{exp_transf} and uses a 2D occupancy grid. Here, the next exploration target is selected based on the distance from frontier cells, in this case, free cells adjacent to unknown cells, and the cost of the path to it based on the distance and safety of the route.
\section{Method}
We propose a \acrfull{nbv} planner to generate inspection routes for monitoring construction progress. In contrast to most of the work presented in Section \ref{sec:rel_work}, the planning process is based on a prior 3D model of the construction site and is performed offline in advance.
Fig. \ref{fig:precom_overview} shows an overview of our planning pipeline. First, we convert the given 3D model of the construction site to various representations used for planning. Using these models, candidate viewpoints for data recording are generated. For each candidate, a reward is computed that reflects the covered surface area. Based on the reward, a subset of viewpoints is selected that optimally covers all surfaces of the building. After converting the sensor viewpoints to robot waypoints, the shortest path that connects these waypoints is computed. The modular design of the planning pipeline allows to easily exchange steps if a better approach is found.
\subsection{Process prior information} \label{method:precomputations}
\begin{figure*}[thpb]
\centering
\begin{subfigure}[t]{.24\textwidth}
  \centering
  [width=\linewidth]{graphics/small_model.png}
  \caption{Artificial building model.}
  \label{fig:artificial_model}
\end{subfigure}\hfill
\begin{subfigure}[t]{.24\textwidth}
  \centering
  [width=\linewidth]{graphics/octomap.png}
  \caption{OctoMap}
  \label{fig:octomap}
\end{subfigure}\hfill
\begin{subfigure}[t]{.24\textwidth}
  \centering
  [width=\linewidth]{graphics/sdf.png}
  \caption{TSDF}
  \label{fig:sdf}
\end{subfigure}\hfill
\begin{subfigure}[t]{.24\textwidth}
  \centering
  [width=\linewidth]{graphics/mesh_map.png}
  \caption{Mesh Map (violet: traversable space, red: non-traversable space)}
  \label{fig:mesh_map}
\end{subfigure}
\caption{Different model representations.}
\label{fig:models}
\end{figure*}
The prior information, i. e. the building mesh, needs to be present in two ways. First, as a complete model of the environment (Fig. \ref{fig:artificial_model}) used for path planning, and second, as a subset of the complete model, hereafter called the target model, which contains only the parts needed in the recorded data, e.g., only single rooms or also the complete model.
The complete model is converted into an OctoMap \cite{occupancy_grid, octomap} (Fig. \ref{fig:octomap}) and a \acrfull{sdf} \cite{esdf_tsdf} (Fig. \ref{fig:sdf}). The candidate generation and the waypoint order computation are performed using a mesh representation provided by the \textit{Mesh Navigation} \cite{mesh_navigation} (Fig. \ref{fig:mesh_map}). This mesh representation contains different layers indicating terrain traversability. We use a cost layer for local height differences and an inflation layer for lethal vertices in the first layer.
The target model is converted to a point cloud to form the target set, a set of points called target points, that is used to quantify the surface coverage.
\subsection{Generate candidate viewpoints}
The viewpoint candidate generation is split into the position and the orientation generation.
The candidate positions need to be generated equally distributed over the traversable space in order to handle multiple levels and height differences. Therefore, the mesh representation is iterated from a given start point and all reachable vertices, whose costs are below a given threshold and thus are seen as traversable, are used as candidates. In order to reduce this number, a uniformly distributed random sample is used.
Depending on the used sensor, multiple orientations are sampled for each candidate position in order to cover the complete environment. The number of equally distributed orientations $N$ per candidate position is given by
\begin{equation}
    N = \frac{n * 360\si{\degree}}{\mathit{fov}_h}
\end{equation}
where $n$ controls the overlap between the samples and $\mathit{fov}_h$ is the horizontal field of view of the sensor.
\subsection{Compute reward of candidates}
The reward of each candidate is given as the number of target points expected to be seen from its position according to the prior information.
In order to compute this information, the visibility is checked for each combination of candidate and target point. The general assumption is that a target point is visible if it is within the sensor's range and field of view and a clear line of sight exists between the candidate and target point.
In order to check if such a clear line of sight exists, ray casting can be used. However, since this is an expensive operation, other checks are performed beforehand to exclude invisible targets with as little computational effort as possible.
The first visibility check ensures that the target point is within the sensor's range and field of view.
The next check rejects all the target points that cannot be seen since the view is blocked by the robot itself. For this, the \lstinline{robot_body_filter} \footnote{\url{http://wiki.ros.org/robot_body_filter}} is used. Since applying this filter for each candidate is costly, a mask (Fig. \ref{fig:self-filter_mask}) is precomputed and during the check for each target point, this point is mapped to the closest point on the mask. For the mask, a sampled unit sphere is used. The points on the mask are generated as spherical Fibonacci point sets \cite{spherical_fibonacci_mapping, fibonacci_lattice} and the mapping is performed as proposed by Keinert et al. \cite{spherical_fibonacci_mapping}.
In the third check, all target points located on the non-visible side of walls are excluded. To achieve this, we use the \acrshort{sdf} representation. If both, the direction from the target point to the candidate and the gradient at the target point have the same sign in each component, the target point is on the opposite side of a wall and cannot be seen. Finally, for the remaining points, it is checked if a clear line of sight exists using ray casting on the 3D occupancy grid map.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\subsection{Select set of viewpoints} \label{method:set_coverage_problem}
Next, the minimum number of candidate viewpoints that cover the target set as completely as possible must be selected. This corresponds to the ``set cover problem''. The solution to this problem is NP-complete, but there are heuristic solvers, three of which have been tested and compared here. The first one is a greedy approach which always selects the candidate with the highest reward. Each time a candidate is selected, the reward of the remaining ones is updated. But, since redundancies can occur with this approach if the covered set of one candidate is later completely covered by other candidates, the second approach introduces a backtracking step. The last approach is probabilistic. Instead of always selecting the best candidate, worse candidates can be selected based on an exponential distribution.
However, instead of selecting new candidates until the reward reaches 0, we introduce a minimum reward for a candidate to have in order to be selected. This minimum reward describes a trade-off between the number of viewpoints, on which the length of the path depends, and the coverage rate. The selection process is finished, when there are either no candidates or uncovered target points left, or when there is no candidate left whose reward is greater than the minimum reward.
\subsection{Compute waypoint order}
After the viewpoints have been selected, they are converted into waypoints by transforming them from the sensor frame to the robot's base link frame. To minimize the time for the execution of the plan, we find the shortest path connecting all waypoints.
Therefore, we use the length of the paths between every two waypoints as the cost and calculate the cost-optimal order of the waypoints. The paths are computed on the complete model using the \textit{Mesh Navigation} \cite{mesh_navigation}.
Choosing the best waypoint order with the lowest costs is a \acrfull{tsp} type problem. The problem here can be assumed as symmetric and metric. Solving the \acrshort{tsp} is NP-complete, but there exist heuristic algorithms. We use a simulated annealing algorithm \cite{sim_annealing} initialized with the minimum spanning tree approach \cite{cormen_introduction} to solve this problem.
\subsection{Execution of path}
For executing the path, the previously computed paths from the \acrshort{tsp} solution are used to approach the waypoints.
During execution, a 2D occupancy map is created. To react to dynamic obstacles, the precomputed path is continuously monitored by ensuring that the robot polygon for each pose of the planned path only contains free cells on the created map.
If an obstacle is found, only the blocked parts of the path are replanned, while all valid parts are preserved.
When a waypoint is reached, the data is recorded with the sensor payload, i.e. a 360-degree image or a lidar scan.
\section{Evaluation}
We evaluate our approach on two different models, a small artificial building (Fig. \ref{fig:artificial_model}) with an area of about \SI{183}{\square\meter} and a large model with a floor area of about \SI{1365}{\square\meter} which is part of a real building. The artificial building is constructed to contain difficulties that may also occur in real buildings, e.g. rooms with interior walls, multiple doors, and an unreachable room as well as a ramp in order to evaluate the planning on traversable height differences. The real building has two floors and multiple stairs.
We demonstrate the generality of the approach by performing the evaluations on two different robots, \textit{Asterix} (Fig. \ref{fig:asterix}) and the Boston Dynamics Spot (Fig. \ref{fig:spot}). Both are highly mobile ground robots, each equipped with a sensor payload including a Velodyne VLP-16 lidar.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\subsection{Set cover problem}
When selecting the final viewpoints, a ``set cover problem"" has to be solved (see Section \ref{method:set_coverage_problem}). We found the simple greedy approach to perform best in terms of computation time and number of selected viewpoints.
The variant without redundancies and the probabilistic approach could only marginally improve the result while having much longer computation times. Therefore, in the following experiments, the greedy approach has been used.
As described in Section \ref{method:set_coverage_problem}, we use a minimum reward to stop the selection of viewpoints early. In Fig. \ref{fig:set_cov_cutoff}, the reward of each newly selected viewpoint is shown as well as the number of covered target points. As can be seen, the reward of the newly selected viewpoints decreases very fast. Thus, without a specified minimum reward, many viewpoints would be selected without a considerable increase in coverage. For the following experiments, we set this value to 100, since there is a good balance between coverage and the number of viewpoints.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\subsection{Comparison to exploration planner}
\label{eval:comp_exp}
We compare our approach to an exploration planner that generates a map of an unknown environment. We use the algorithm proposed by Wirth and Pellenz \cite{exp_transf} for comparison. The driving distance has a high influence on the total time it takes to cover all surfaces of the building. Therefore, we compare the approaches in terms of total path length and coverage by comparing the acquired lidar point clouds.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
The path of the exploration on the artificial model using \textit{Asterix} can be seen in Fig. \ref{fig:exploration_path_gridmap}. The driven path has a length of \SI{172}{\meter}. The ramp on the bottom side is missing because the planner cannot handle height differences. Due to the greedy approach, many small uncovered areas are remaining after the first pass of the building as can be seen in Fig. \ref{fig:exploration_process}. Returning to these uncovered areas leads to path redundancies.
In contrast, Fig. \ref{fig:precom_res_small_complete} contains the path planned with the presented approach. Here, the path is much shorter with only \SI{155.9}{\meter} and contains 62 waypoints. As can be seen, all reachable areas including the ramp contain viewpoints and the path has no unnecessary redundancies.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure*}[tpb]
\centering
\begin{subfigure}[t]{0.475\linewidth}
  \centering
  [width=\linewidth]{graphics/execution_small_model_asterix_complete__point_cloud_accumulated.png}
  \caption{Point cloud recorded with our proposed approach.}
\end{subfigure}\hfill
\begin{subfigure}[t]{0.475\linewidth}
  \centering
  [width=\linewidth]{graphics/exploration_acc_pc_missing_areas.png}
  \caption{Point cloud recorded during exploration. The highlighted areas are (partially) uncovered.}
\end{subfigure}
\caption{Accumulated point clouds. The evaluations were performed on the artificial model.}
\label{fig:accumulated_point_clouds}
\end{figure*}
On the first floor of the large real building model, driving distances are much greater, so path redundancies have a much greater impact on the total driving distance. This can be seen in the map of the partial exploration using the Spot in Fig. \ref{fig:exploration_path_gridmap_spot_large}. Again, many areas have been left uncovered during the initial traversal of the building. Due to simulation time limitations, the exploration planner was unable to fully map the building. As can be seen in Fig. \ref{fig:precom_res_large_complete} our proposed planner covers all rooms of the building floor without path redundancies. The total path length is \SI{456.3}{\meter} with 111 viewpoints. Additionally, we demonstrate that our approach can handle multiple levels (see Fig. \ref{fig:precom_res_large_target_multilevel}).
Next, we compare the data quality to assess the surface coverage.
During exploration, a point cloud is recorded and accumulated continuously using a voxel filter with a leaf size of \SI{5}{\centi\meter}. In order to make the result of the developed planner comparable, a point cloud was also recorded continuously instead of only at the viewpoints. The results are shown in Fig. \ref{fig:accumulated_point_clouds}. It can be seen that the point cloud of the exploration does have several uncovered areas whereas the accumulated point cloud of our approach is nearly complete.
This comparison shows that our proposed approach can capture the data more effectively by reducing path redundancies while also improving data coverage.
\subsection{Computation time}
Since the proposed approach is an offline planner, the computation time is not a major concern. Additionally, the computation time required for the planning is highly dependent on the used models as well as on chosen parameters. For example, the voxel size of the model representations, the number of target points that are sampled, and the used selector and \acrshort{tsp} solver greatly impact the computation time. However, to give an estimation, some computation times for the planning on the artificial and the real model with the complete model as the target model are shown here. In the evaluation on the artificial model, 327 candidates have been generated, and a \acrshort{sdf} voxel size of \SI{0.0375}{\meter} has been used. The voxel size of the OctoMap was the same for both models, \SI{0.075}{\meter}. For the real building model, a \acrshort{sdf} voxel size of \SI{0.05}{\meter} was chosen and 594 candidates have been generated. The experiments were performed on an \textit{Intel Core i7-8565U CPU @ 1.80GHz}.
In Fig. \ref{fig:computation_time_steps} the computation time for each step can be seen. The steps have not been parallelized yet, but especially in the rewarding process, an acceleration is to be expected. Also the time for processing the prior information can be reduced significantly by loading previously converted models. However, this only works for multiple planning cycles on the same model with no model parameters changing.
\pgfplotstableread{
Label        prior_info candidates reward select_vp wp_cost waypoint_order misc
Artificial    71.7 0.2 43.8 1.8 107.0 19.7 5.2
Real         396.1 0.3 75.3 7.7 355.9 25.4 16.5
}\testdata
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\section{CONCLUSIONS}
We presented a modular time-optimal 3D path planning pipeline for efficient monitoring of construction sites with multiple levels. The use of the 3D building model as prior information enabled the offline planning of paths from which all surfaces of the given 3D model could be covered with a sensor payload. We showed on different building models that paths planned with our approach are much shorter than the ones from existing approaches and provide a higher coverage of the environment.
{IEEEtran}
,bibliography}
"
77,"\importpackages{}
\graphicspath{ {./images/} }


\maketitle
\begin{abstract}
  
We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan~\cite{rubinfeld2022testing}.  In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution --- e.g., the Gaussian --- must pass the test.  This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold.
We consider the setting where the target distribution is Gaussian (or more generally any strongly log-concave distribution) in $d$ dimensions and the noise model is either Massart or adversarial (agnostic).  For Massart noise our tester-learner runs in polynomial time and outputs a hypothesis with error $\opt + \epsilon$, which is information-theoretically optimal.  For adversarial noise our tester-learner has error $\wt{O}(\opt) + \epsilon$ and runs in quasipolynomial time. 
Prior work on testable learning ignores the labels in the training set and checks that the empirical moments of the covariates are close to the moments of the base distribution.  Here we develop new tests of independent interest that make critical use of the labels and combine them with the moment-matching approach of \cite{gollakota2022moment}.  This enables us to  simulate a variant of the algorithm of \cite{diakonikolas2020learning, diakonikolas2020non} for learning noisy halfspaces using nonconvex SGD but in the testable learning setting.
\end{abstract}
\newpage
\section{Introduction}
Learning halfspaces in the presence of noise is one of the most basic and well-studied problems in computational learning theory. A large body of work has obtained results for this problem under a variety of different noise models and distributional assumptions (see e.g.\ \cite{balcan2021noise} for a survey). A major issue with common distributional assumptions such as Gaussianity, however, is that they can be hard or impossible to verify in the absence of any prior information. 
The recently defined model of testable learning \cite{rubinfeld2022testing} addresses this issue by replacing such assumptions with efficiently testable ones. In this model, the learner is required to work with an arbitrary input distribution $\Djoint$ and verify any assumptions it needs to succeed.
It may choose to reject a given training set, but if it accepts, it is required to output a hypothesis with error close to $\opt(\C, \Djoint)$, the optimal error achievable over $\Djoint$ by any function in a concept class $\C$. Further, whenever the training set is drawn from a distribution $\Djoint$ whose marginal is truly a well-behaved target distribution $\Dtgt$ (such as the standard Gaussian), the algorithm is required to accept with high probability. Such an algorithm, or tester-learner, is then said to testably learn $\C$ with respect to target marginal $\Dtgt$. (See Definition \ref{definition:testable_learning} for a formal definition.)  Note that unlike ordinary distribution-specific agnostic learners, a tester-learner must take some nontrivial action {\em regardless} of the input distribution.
The work of \cite{rubinfeld2022testing,gollakota2022moment} established foundational algorithmic and statistical results for this model and showed that testable learning is in general provably harder than ordinary distribution-specific agnostic learning. As one of their main algorithmic results, they showed tester-learners for the class of halfspaces over $\R^d$ that succeed whenever the target marginal is Gaussian (or one of a more general class of distributions), achieving error $\opt + \eps$ in time and sample complexity $d^{\wt{O}(1/\eps^2)}$. This matches the running time of ordinary distribution-specific agnostic learning of halfspaces over the Gaussian using the standard approach of \cite{kalai2008agnostically}. Their testers are simple and label-oblivious, and are based on checking whether the low-degree empirical moments of the unknown marginal match those of the target $\Dtgt$.
These works essentially resolve the question of designing tester-learners achieving error $\opt + \eps$ for halfspaces, matching known hardness results for (ordinary) agnostic learning \cite{goel2020statistical,diakonikolas2020near,diakonikolas2021optimality}. Their running time, however, 
necessarily scales exponentially in $1/\eps$.
A long line of research has sought to obtain more efficient algorithms at the cost of relaxing the optimality guarantee \cite{awasthi2017power,diakonikolas2018learning,diakonikolas2020learning,diakonikolas2020non}.  These works give polynomial-time algorithms achieving bounds of the form $\opt + \epsilon$ and $O(\opt) + \epsilon$ for the Massart and agnostic setting respectively under structured distributions (see \cref{subsec:related} for more discussion).
The main question we consider here is whether such guarantees can be obtained in the testable learning 
framework.
 
\paragraph{Our contributions}
In this work we design the first tester-learners for halfspaces that run in fully polynomial or quasipolynomial time in all parameters. We match the optimality guarantees of fully polynomial-time distribution-specific learning algorithms for the Massart noise model (where the labels arise from a halfspace but are flipped by an adversary with probability at most $\eta$), and nearly match them for the agnostic model (where the labels can be completely arbitrary).  They succeed with respect to any chosen target marginal $\Dtgt$ that is isotropic and strongly log-concave (a canonical example being the standard Gaussian). 
\begin{theorem}[Formally stated as \cref{theorem:massart}]
    Let $\C$ be the class of origin-centered halfspaces over $\R^d$, and let $\Dtgt$ be any isotropic strongly log-concave distribution. In the setting where the labels are corrupted with Massart noise at rate at most $\eta < \frac{1}{2}$, $\C$ can be testably learned w.r.t.\ $\Dtgt$ up to error $\opt + \eps$  using $\poly(d, \frac{1}{\epsilon}, \frac{1}{1 - 2\eta})$ time and sample complexity.
\end{theorem}
\begin{theorem}[Formally stated as \cref{theorem:agnostic}]
    Let $\C$ and $\Dtgt$ be as above. In the adversarial noise or agnostic setting where the labels are completely arbitrary, for any $k \in \N$, $\C$ can be testably learned w.r.t.\ $\Dtgt$ up to error $O(\sqrt{k} \cdot \opt^{1 - 1/k}) + \eps$  using $\poly(d^{\wt{O}(k)}, \left(\frac{1}{\epsilon}\right)^{\wt{O}(k)})$ time and sample complexity. 
    In particular, by picking an appropriate $k = \polylog d$, we obtain error $\wt{O}(\opt) + \eps$ in quasipolynomial time and sample complexity. 
\end{theorem}
Our results for the Massart noise model are information-theoretically optimal.  For the agnostic setting, we leave open the possibility of obtaining $O(\opt) + \epsilon$ in fully polynomial time (rather than our quasipolynomial-time $\wt{O}(\opt)$ guarantee). 
\paragraph{Our techniques}
The tester-learners we develop are significantly more involved than prior work on testable learning. 
We build on the nonconvex optimization approach to learning noisy halfspaces due to \cite{diakonikolas2020learning, diakonikolas2020non} as well as the structural results on fooling functions of halfspaces using moment matching due to \cite{gollakota2022moment}. Unlike the label-oblivious, global moment tests of \cite{rubinfeld2022testing,gollakota2022moment}, our tests make crucial use of the labels and check \emph{local} properties of the distribution in regions described by certain candidate vectors. These candidates are approximate stationary points of a natural nonconvex surrogate of the 0-1 loss, obtained by running gradient descent. When the distribution is known to be well-behaved,  \cite{diakonikolas2020learning, diakonikolas2020non} showed that any such stationary point is in fact a good solution (for technical reasons we must use a slightly different surrogate loss). Their proof relies crucially on structural geometric properties that hold for these well-behaved distributions, an important one being that the probability mass of any region close to the origin is proportional to its geometric measure.
In the testable learning setting, we must efficiently check this property for candidate solutions.  Since these regions may be described as intersections of halfspaces, we may hope to apply the moment-matching framework of \cite{gollakota2022moment}. Na{\""i}vely, however, they only allow us to check in polynomial time that the probability masses of such regions are within an additive constant of what they should be under the target marginal. But we can view these regions as sub-regions of a known band described by our candidate vector. By running moment tests on the distribution \emph{conditioned} on this band and exploiting the full strength of the moment-matching framework, we are able to effectively convert our weak additive approximations to good multiplicative ones. This allows us to argue that our stationary points are indeed good solutions.
\subsection{Related work}\label{subsec:related}
We provide a partial summary of some of the most relevant prior and related work on efficient algorithms for learning halfspaces in the presence of adversarial label or Massart noise, and refer the reader to \cite{balcan2021noise} for a survey.
In the distribution-specific agnostic setting where the marginal is assumed to be isotropic and log-concave, \cite{klivans2009learning} showed an algorithm achieving error $O(\opt^{1/3}) + \eps$ for the class of origin-centered halfspaces. \cite{awasthi2017power} later obtained $O(\opt) + \eps$ using an approach that introduced the principle of iterative \emph{localization}, where the learner focuses attention on a band around a candidate halfspace in order to produce an improved candidate. \cite{daniely2015ptas} used this principle to obtain a PTAS for agnostically learning halfspaces under the uniform distribution on the sphere, and \cite{balcan2017sample} extended it to more general $s$-concave distributions. Further works in this line include \cite{yan2017revisiting,zhang2018efficient,zhang2020efficient,zhang2021improved}. \cite{diakonikolas2020non} introduced the simplest approach yet, based entirely on nonconvex SGD, and showed that it achieves $O(\opt) + \eps$ for origin-centered halfspaces over a wide class of structured distributions. Other related works include \cite{diakonikolas2018learning,diakonikolas2022learning_online}.
In the Massart noise setting with noise rate bounded by $\eta$, work of \cite{diakonikolas2019distribution} gave the first efficient distribution-free algorithm achieving error $\eta + \eps$; further improvements and followups include \cite{diakonikolas2021forster,diakonikolas2022strongly}. However, the optimal error $\opt$ achievable by a halfspace may be much smaller than $\eta$, and it has been shown that there are distributions where achieving error competitive with $\opt$ as opposed to $\eta$ is computationally hard \cite{diakonikolas2022near,diakonikolas2022cryptographic}. As a result, the distribution-specific setting remains well-motivated for Massart noise. Early distribution-specific algorithms were given by \cite{awasthi2015efficient,awasthi2016learning}, but a key breakthrough was the nonconvex SGD approach introduced by \cite{diakonikolas2020learning}, which achieved error $\opt + \eps$ for origin-centered halfspaces efficiently over a wide range of distributions. This was later generalized by \cite{diakonikolas2022learning_general}.
\subsection{Technical overview}
Our starting point is the nonconvex optimization approach to learning noisy halfspaces due to \cite{diakonikolas2020learning, diakonikolas2020non}. The algorithms in these works consist of running SGD on a natural non-convex surrogate $\L_\sigma$ for the 0-1 loss, namely a smooth version of the ramp loss. The key structural property shown is that if the marginal distribution is structured (e.g.\ log-concave) and the slope of the ramp is picked appropriately, then any $\w$ that has large angle with an optimal $\wopt$ cannot be an approximate stationary point of the surrogate loss $\L_\sigma$, i.e.\ that $\|\nabla \L_\sigma(\w)\|$ must be large. This is proven by carefully analyzing the contributions to the gradient norm from certain critical regions of $\spn(\w, \wopt)$, and crucially using the distributional assumption that the probability masses of these regions are proportional to their geometric measures. (See \cref{fig:regions}.)
In the testable learning setting, the main challenge we face in adapting this approach is checking such a property for the unknown distribution we have access to.
A preliminary observation is that the critical regions of $\spn(\w, \wopt)$ that we need to analyze are rectangles, and are hence functions of a small number of halfspaces. Encouragingly, one of the key structural results of the prior work of \cite{gollakota2022moment} pertains to ``fooling'' such functions. Concretely, they show that whenever the true marginal $\Dtrueemp$ matches moments of degree at most $\wt{O}(1/\tau^2)$ with a target $\Dtgt$ that satisfies suitable concentration and anticoncentration properties, then $|\ex_{\Dtrueemp}[f] - \ex_{\Dtgt}[f]| \leq \tau$ for any $f$ that is a function of a small number of halfspaces. If we could run such a test and ensure that the probabilities of the critical regions over our empirical marginal are also related to their areas, then we would have a similar stationary point property.
However, the difficulty is that since we wish to run in fully polynomial time, we can only hope to fool such functions up to $\tau$ that is a constant. Unfortunately, this is not sufficient to analyze the probability masses of the critical regions we care about as they may be very small.
The chief insight that lets us get around this issue is that each critical region $R$ is in fact of a very specific form, namely a rectangle that is axis-aligned with $\w$: $R = \{ \x : \inn{\w, \x} \in [-\sigma, \sigma] \text{ and } \inn{\vv, \x} \in [\alpha, \beta] \}$ for some values $\alpha, \beta, \sigma$ and some $\vv$ orthogonal to $\w$. Moreover, we \emph{know} $\w$, meaning we can efficiently estimate the probability $\pr_{\Dtrueemp}[\inn{\w, \x} \in [-\sigma, \sigma]]$ up to constant multiplicative factors without needing moment tests. Denoting the band $\{ \x : \inn{\w, \x} \in [-\sigma, \sigma]\}$ by $T$ and writing $\pr_{\Dtrueemp}[R] = \pr_{\Dtrueemp}[\inn{\vv, \x} \in [\alpha, \beta] \mid \x \in T]\pr_{\Dtrueemp}[T]$, it turns out that we should expect $\pr_{\Dtrueemp}[\inn{\vv, \x} \in [\alpha, \beta] \mid \x \in T] = \Theta(1)$, as this is what would occur under the structured target distribution $D^*$. (Such a ``localization'' property is also at the heart of the algorithms for approximately learning halfspaces of, e.g., \cite{awasthi2017power,daniely2015ptas}.) To check this, it suffices to run tests that ensure that $\pr_{\Dtrueemp}[\inn{\vv, \x} \in [\alpha, \beta] \mid \x \in T]$ is within an additive constant of this probability under $D^*$.
We can now describe the core of our algorithm (omitting some details such as the selection of the slope of the ramp). First, we run SGD on the surrogate loss $\L$ to arrive at an approximate stationary point and candidate vector $\w$ (technically a list of such candidates). Then, we define the band $T$ based on $\w$, and run tests on the empirical distribution conditioned on $T$. Specifically, we check that the low-degree empirical moments conditioned on $T$ match those of $\Dtgt$ conditioned on $T$, and then apply the structural result of \cite{gollakota2022moment} to estimate conditional probabilities of the form $\pr_{\Dtrueemp}[\inn{\vv, \x} \in [\alpha, \beta] \mid \x \in T]$ up to a suitable additive constant. This suffices to ensure that even over our empirical marginal, the particular stationary point $\w$ we have is indeed close in angular distance to an optimal $\wopt$.
A final hurdle that remains, often taken for granted under structured distributions, is that closeness in angular distance $\measuredangle(\w, \wopt)$ does not immediately translate to closeness in terms of agreement, $\pr[\sign(\inn{\w, \x}) \neq \sign(\inn{\wopt, \x})]$, over our unknown marginal. Nevertheless, we show an approximate relationship saying that for any $k \in \N$ we can run tests requiring time $d^{\wt{O}(k)}$ that ensure that an angle of $\theta=\measuredangle(\w, \wopt)$ translates to disagreement of at most $O(\sqrt{k}\cdot \theta^{1 - 1/k})$. In the Massart noise setting, we can make $\measuredangle(\w, \wopt)$ arbitrarily small, and so obtain our $\opt + \eps$ guarantee. In the adversarial noise setting, we face a more delicate tradeoff and can only make $\measuredangle(\w, \wopt)$ as small as $\Theta(\opt)$, leading to our $O(\sqrt{k}\cdot \opt^{1 - 1/k}) + \eps$ guarantee.
\section{Preliminaries}
\paragraph{Notation and setup}
Throughout, the domain will be $\X = \R^d$, and labels will lie in $\Y = \cube{}$. The unknown joint distribution over $\X \times \Y$ that we have access to will be denoted by $\Djoint$, and its marginal on $\X$ will be denoted by $\Dtrue$. The target marginal on $\X$ will be denoted by $\Dtgt$. We use the following convention for monomials: for a multi-index $\alpha = (\alpha_1, \dots, \alpha_d) \in \Z^d_{\geq 0}$, $\x^\alpha$ denotes $\prod_i x_i^{\alpha_i}$, and $|\alpha| = \sum_i \alpha_i$ denotes its total degree. 
We use $\C$ to denote a concept class mapping $\R^d$ to $\cube{}$, which throughout this paper will be the class of halfspaces or functions of halfspaces over $\R^d$. We use $\opt(\C, \Djoint)$ to denote the optimal error $\inf_{f \in \C} \pr_{(\x, y) \sim \Djoint}[f(\x) \neq y]$, or just $\opt$ when $\C$ and $\Djoint$ are clear from context.
We recall the definitions of the noise models we consider. In the Massart noise model, the labels satisfy $\pr_{y \sim \Djoint | \x}[y \neq \sign(\inn{\wopt, \x}) \mid \x] = \eta(\x)$, where $\eta(\x) \leq \eta < \frac{1}{2}$ for all $\x$. In the adversarial label noise or agnostic model, the labels may be completely arbitrary. In both cases, the learner's goal is to produce a hypothesis with error competitive with $\opt$.
We now formally define testable learning. The following definition is an equivalent reframing of the original definition \cite[Def 4]{rubinfeld2022testing}, folding the (label-aware) tester and learner into a single tester-learner. 
\begin{definition}[Testable learning, \cite{rubinfeld2022testing}]\label{definition:testable_learning}
Let $\C$ be a concept class mapping $\R^d$ to $\cube{}$. Let $\Dtgt$ be a certain target marginal on $\R^d$. Let $\eps, \delta > 0$ be parameters, and let $\psi : [0,1] \to [0,1]$ be some function. We say $\C$ can be testably learned w.r.t.\ $\Dtgt$ up to error $\psi(\opt) + \eps$ with failure probability $\delta$ if there exists a tester-learner $A$ meeting the following specification. For any distribution $\Djoint$ on $\R^d \times \cube{}$, $A$ takes in a large sample $S$ drawn from $\Djoint$, and either rejects $S$ or accepts and produces a hypothesis $h : \R^d \to \cube{}$. Further, the following conditions must be met: \begin{enumerate}[label=(\alph*)]
    \item (Soundness.) Whenever $A$ accepts and produces a hypothesis $h$, with probability at least $1 - \delta$ (over the randomness of $S$ and $A$), $h$ must satisfy $\pr_{(\x, y) \sim \Djoint}[h(\x) \neq y] \leq \psi(\opt(\C, \Djoint)) + \eps$.
    \item (Completeness.) Whenever $\Djoint$ truly has marginal $\Dtgt$, $A$ must accept with probability at least $1 - \delta$ (over the randomness of $S$ and $A$).
\end{enumerate}
\end{definition}
We also formally define the class of \emph{strongly log-concave} distributions, which is the class that our target marginal $\Dtgt$ is allowed to belong to, and collect some useful properties of such distributions. We will state the definition for isotropic $\Dtgt$ (i.e.\ with mean $0$ and covariance $I$) for simplicity.
\begin{definition}[{Strongly log-concave distribution, see e.g.\ \cite[Def 2.8]{saumard2014log}}]\label{definition:strongly_log_concave}
We say an isotropic distribution $\Dtgt$ on $\R^d$ is strongly log-concave if the logarithm of its density $q$ is a strongly concave function. Equivalently, $q$ can be written as \begin{equation}\label{eq:slc-def}
q(\x) = r(\x) \gamma_{\kappa^2 I}(\x) \end{equation} for some log-concave function $r$ and some constant $\kappa > 0$, where $\gamma_{\kappa^2 I}$ denotes the density of the spherical Gaussian $\mathcal{N}(0, \kappa^2 I)$.
\end{definition}
\begin{proposition}[{see e.g.\ \cite{saumard2014log}}]\label{prop:slc}
Let $\Dtgt$ be an isotropic strongly log-concave distribution on $\R^d$ with density $q$. \begin{enumerate}[label=(\alph*)]
    \item Any orthogonal projection of $\Dtgt$ onto a subspace is also strongly log-concave.
    \item There exist constants $U, R$ such that $q(\x) \leq U$ for all $\x$, and $q(x) \geq 1/U$ for all $\|\x\| \leq R$.
    \item There exist constants $U'$ and $\kappa$ such that $q(\x) \leq U' \gamma_{\kappa^2 I}(\x)$ for all $\x$.
    \item There exist constants $K_1, K_2$ such that for any $\sigma \in [0, 1]$ and any $\vv \in \S^{d-1}$, $\pr[|\inn{\vv, \x}| \leq \sigma] \in (K_1\sigma, K_2\sigma)$.
    \item There exists a constant $K_3$ such that for any $k \in \N$, $\ex[|\inn{\vv, \x}|^k] \leq (K_3 k)^{k/2}$.
    \item Let $\alpha = (\alpha_1, \dots, \alpha_d) \in \Z^d_{\geq 0}$ be a multi-index with total degree $|\alpha| = \sum_i \alpha_i = k$, and let $\x^\alpha = \prod_i x_i^{\alpha_i}$. There exists a constant $K_4$ such that for any such $\alpha$, $\ex[|\x^{\alpha}|]  \leq (K_4 k)^{k/2}$.
\end{enumerate}
\end{proposition}
For (a), see e.g.\ \cite[Thm 3.7]{saumard2014log}. The other properties follow readily from \cref{eq:slc-def}, which allows us to treat the density as subgaussian.
A key structural fact that we will need about strongly log-concave distributions is that approximately matching moments of degree at most $\wt{O}(1/\tau^2)$ with such a $\Dtgt$ is sufficient to fool any function of a constant number of halfspaces up to an additive $\tau$.
\begin{proposition}[{Variant of \cite[Thm 5.6]{gollakota2022moment}}]\label{prop:fool-hs}
Let $p$ be a fixed constant, and let $\F$ be the class of all functions of $p$ halfspaces mapping $\R^d$ to $\cube{}$ of the form
\begin{equation}\label{eq:fn-of-hs}
    f(\x) = g\left( \sign(\inn{\vv^1, \x} + \theta_1), \dots, \sign(\inn{\vv^p, \x} + \theta_p) \right)\,
\end{equation}
for some $g: \cube{p} \to \cube{}$ and weights $\vv^i \in \S^{d-1}$. Let $\Dtgt$ be any target marginal such that for every $i$, the projection $\inn{\vv^i, \x}$ has subgaussian tails and is anticoncentrated: (a) $\pr[|\inn{\vv^i, \x}| > t] \leq \exp(-\Theta(t^2))$, and (b) for any interval $[a, b]$, $\pr[\inn{\vv^i, \x} \in [a, b]] \leq \Theta(|b-a|)$. Let $D$ be any distribution such that for all monomials $\x^{\alpha} = \prod_i x^{\alpha_i}$ of total degree $|\alpha| = \sum_i \alpha_i \leq k$, \[ \left| \ex_{\Dtgt}[\x^\alpha] - \ex_{D}[\x^{\alpha}] \right| \leq \left(\frac{c|\alpha|}{d\sqrt{k}} \right)^{|\alpha|} \] for some sufficiently small constant $c$ (in particular, it suffices to have $d^{-\wt{O}(k)}$ moment closeness for every $\alpha$). Then \[ \max_{f \in \F} \left| \ex_{\Dtgt}[f] - \ex_{D}[f] \right| \leq \wt{O}\left(\frac{1}{\sqrt{k}}\right). \]
\end{proposition}
Note that this is a variant of the original statement of \cite[Thm 5.6]{gollakota2022moment}, which requires that the 1D projection of $\Dtgt$ along \emph{any} direction satisfy suitable concentration and anticoncentration. Indeed, an inspection of their proof reveals that it suffices to verify these properties for projections only along the directions $\{\vv^i\}_{i \in [p]}$ as opposed to all directions. This is because to fool a function $f$ of the form above, their proof only analyzes the projected distribution $(\inn{\vv^1, \x}, \dots, \inn{\vv^p, \x})$ on $\R^p$, and requires only concentration and anticoncentration for each individual projection $\inn{\vv^i, \x}$.
\section{Testing properties of strongly log-concave distributions}\label{sec:testing}
In this section we define the testers that we will need for our algorithm. We begin with a structural lemma that strengthens the key structural result of \cite{gollakota2022moment}, stated here as Proposition~\ref{prop:fool-hs}. It states that even when we restrict an isotropic strongly log-concave $\Dtgt$ to a band around the origin, moment matching suffices to fool functions of halfspaces whose weights are orthogonal to the normal of the band.
\begin{proposition}\label{prop: moments in band close means we good}
Let $\Dtgt$ be an isotropic strongly log-concave distribution. Let $\w \in \S^{d-1}$ be any fixed direction. Let $p$ be a constant. Let $f : \R^d \to \R$ be a function of $p$ halfspaces of the form in \cref{eq:fn-of-hs}, with the additional restriction that its weights $\vv^i \in \S^{d-1}$ satisfy $\inn{\vv^i, \w} = 0$ for all $i$. For some $\sigma \in [0, 1]$, let $T$ denote the band $\{ \x : |\inn{\w, \x}| \leq \sigma \}$. Let $D$ be any distribution such that $D_{|T}$ matches moments of degree at most $k = \wt{O}(1/\tau^2)$ with $\Dtgt_{|T}$ up to an additive slack of $d^{-\wt{O}(k)}$. Then $ \left| \ex_{\Dtgt}[f \mid T] - \ex_{D}[f \mid T] \right| \leq \tau. $
\end{proposition}
\begin{proof}
Our plan is to apply Proposition~\ref{prop:fool-hs}. To do so, we must verify that $\Dtgt_{|T}$ satisfies the assumptions required. In particular, it suffices to verify that the 1D projection along any direction orthogonal to $\w$ has subgaussian tails and is anticoncentrated. Let $\vv \in \S^{d-1}$ be any direction that is orthogonal to $\w$. By Proposition~\ref{prop:slc}(d), we may assume that $\pr_{\Dtgt}[T] \geq \Omega(\sigma)$.
To verify subgaussian tails, we must show that for any $t$, $\pr_{\Dtgt_{|T}}[|\inn{\vv, \x}| > t] \leq \exp(-Ct^2)$ for some constant $C$. The main fact we use is  Proposition~\ref{prop:slc}(c), i.e.\ that any strongly log-concave density is pointwise upper bounded by a Gaussian density times a constant. Write \[ \pr_{\Dtgt_{|T}}[|\inn{\vv, \x}| > t] = \frac{\pr_{\Dtgt}[\inn{\vv, \x} > t \text{ and } \inn{\w, \x} \in [-\sigma, \sigma]]}{\pr_{\Dtgt}[\inn{\w, \x} \in [-\sigma, \sigma]]}. \] The claim now follows from the fact that the numerator is upper bounded by a constant times the corresponding probability under a Gaussian density, which is at most $O(\exp(-C't^2)\sigma)$ for some constant $C'$, and that the denominator is $\Omega(\sigma)$.
To check anticoncentration, for any interval $[a, b]$, write \[ \pr_{\Dtgt_{|T}}[\inn{\vv, \x} \in [a, b]] = \frac{\pr_{\Dtgt}[\inn{\vv, \x} \in [a, b] \text{ and } \inn{\w, \x} \in [-\sigma, \sigma]]}{\pr_{\Dtgt}[\inn{\w, \x} \in [-\sigma, \sigma]]}. \] After projecting onto $\spn(\vv, \w)$ (an operation that preserves logconcavity), the numerator is the probability mass under a rectangle with side lengths $|b-a|$ and $2\sigma$, which is at most $O(\sigma|b-a|)$ as by Proposition~\ref{prop:slc}(b) the density is pointwise upper bounded by a constant. The claim follows since the denominator is $\Omega(\sigma)$.
Now we are ready to apply Proposition~\ref{prop:fool-hs}. We see that if $D_{|T}$ matches moments of degree at  most $k$ with $\Dtgt_{|T}$ up to an additive slack of $d^{-O(k)}$, then $|\ex_{\Dtgt}[f \mid T] - \ex_{D}[f \mid T]| \leq \wt{O}(1/\sqrt{k})$. Rewriting in terms of $\tau$ gives the theorem.
\end{proof}
We now describe the testers that we use. The first simply checks moments of the unconditioned distribution, and the second checks the probability within a band. The third checks moments of the conditioned distribution and uses Proposition~\ref{prop: moments in band close means we good}. Proofs are deferred to \cref{appendix:testing-proofs}.
\begin{proposition}\label{tester1_polynomials}
For any isotropic strongly log-concave $\Dtgt$, there exists some constants $C_1$ and a tester $\testerone$ that takes a set $S\subseteq\R^d\times \cube{}$, an even $k \in \N$, a parameter $\delta\in (0,1)$ and runs and in time $\poly\left(d^k, |S|, \log\frac{1}{\delta}\right)$. 
Let $\Dgeneric$ denote the uniform distribution over $S$.  If ${\testerone}$ accepts, then for any $\vv \in \S^{d-1}$ 
\begin{equation}\label{equation:property1_fool_polynomials} \ex_{(\x,y) \sim \Dgeneric}[(\inn{\vv, \x})^k] \leq (C_1k)^{k/2}. \end{equation}
Moreover, if $S$ is obtained by taking at least $\left( d^k, \left(\log \frac{1}{\delta}\right)^k \right)^{C_1}$ i.i.d. samples from a distribution whose $\R^d$-marginal is $D^*$, the test $\testerone$ passes with probability at least $1-\delta$. 
\end{proposition}
\begin{proposition}\label{tester2_band}
For any isotropic strongly log-concave $\Dtgt$, there exist some constants $C_2,C_3$ and a tester ${\testertwo}$ that takes a set $S\subseteq\R^d\times \cube{}$ a vector $\w\in \S^{d-1}$, parameters $\sigma,\delta\in (0,1)$ and runs in time $\poly\left(d,|S|, \log\frac{1}{\delta}\right)$.
Let $\Dgeneric$ denote the uniform distribution over $S$.  If ${\testertwo}$ accepts, then  
\begin{equation}\label{equation:property2_fool_band_indicators} \pr_{(\x,y) \sim \Dgeneric}[|\inn{\w, \x}| \leq \sigma] \in (C_2\sigma, C_3\sigma). \end{equation}
Moreover, if $S$ is obtained by taking at least $\frac{100}{K_1 \sigma^2} \log \left(\frac{1}{\delta}\right)$ i.i.d. samples from a distribution whose $\R^d$-marginal is $D^*$, 
 the test ${\testertwo}$ passes with probability at least $1-\delta$. 
\end{proposition}
\begin{proposition}\label{tester3_truncated}
For any isotropic strongly log-concave $\Dtgt$ and a constant $C_4$, there exists a constant $C_5$ and a tester ${\testerthree}$ that takes a set $S\subseteq\R^d\times \cube{}$ a vector $\w\in \S^{d-1}$, parameters $\sigma,\tau\,\delta \in (0,1)$ and runs in time $\poly\left(d^{\tilde{O}\left(\frac{1}{\tau^2}\right)},\frac{1}{\sigma},|S|, \log\frac{1}{\delta}\right)$.
Let $\Dgeneric$ denote the uniform distribution over $S$, let $T$ denote the band  $\{ \x : |\inn{\w, \x}| \leq \sigma \}$ and let $\mathcal{F}_\w$ denote the set $\{\pm 1\}$-valued functions of $C_4$ halfspaces whose weight vectors are orthogonal to $\w$.  If ${\testerthree}$ accepts, then  
\begin{equation}\label{equation:property3_fool_orthogonal_halfspaces_truncated} 
\max_{f \in \mathcal{F}_\w}
\left| \ex_{\x \sim \Dtgt}[f(\x) \mid \x \in T] - \ex_{(\x,y) \sim \Dgeneric}[f(\x) \mid \x\in T] \right| \leq \tau, \end{equation}
\begin{equation}\label{equation:property4_fool_variance_truncated}  
\max_{\vv \in \S^{d-1}:~ \inn{\vv, \w} =0} 
\left| \ex_{\x \sim \Dtgt}[(\inn{\vv, \x})^2\mid \x \in T] - \ex_{(\x,y) \sim \Dgeneric}[(\inn{\vv, \x})^2 \mid \x\in T] \right| \leq \tau. \end{equation}
Moreover, if $S$ is obtained by taking at least $\left( \frac{1}{\tau} \cdot \frac{1}{\sigma } \cdot d^{\frac{1}{\tau^2} \log^{C_5} \left( \frac{1}{\tau} \right)} \cdot 
\left(\log \frac{1}{\delta}\right)^{\frac{1}{\tau^2} \log^{C_5} \left( \frac{1}{\tau} \right)}  \right)^{C_5}$
i.i.d. samples from a distribution whose $\R^d$-marginal is $D^*$, 
 the test ${\testerthree}$ passes with probability at least $1-\delta$. 
\end{proposition}
\section{Testably learning halfspaces with Massart noise}\label{section:massart}
In this section we prove that we can testably learn halfspaces with Massart noise with respect to isotropic strongly log-concave distributions (see Definition \ref{definition:strongly_log_concave}).
\begin{theorem}[Tester-Learner for Halfspaces with Massart Noise]\label{theorem:massart}
    Let $\Djoint$ be a distribution over $\R^d\times \{\pm 1\}$ and let $\Dtgt$ be a strongly log-concave distribution over $\R^d$. Let $\C$ be the class of origin centered halfspaces in $\R^d$. Then, for any $\eta<1/2$, $\eps>0$ and $\delta\in(0,1)$, there exists an algorithm (Algorithm \ref{alg:main}) that testably learns $\C$ w.r.t. $\Dtgt$ up to excess error $\epsilon$ and error probability at most $\delta$ in the Massart noise model with rate at most $\eta$, using time and a number of samples from $\Djoint$ that are polynomial in $d,1/\eps,\frac{1}{1-2\eta}$ and $\log(1/\delta)$.
\end{theorem}
\begin{algorithm}
\caption{Tester-learner for halfspaces}\label{alg:main}
\KwIn{Training set $S$, parameters $\sigma$, $\delta$}
\KwOut{A near-optimal weight vector $\w$, or rejection}
Run $\testerone(S, k=2, \delta)$ to verify that the empirical marginal is approximately isotropic. Reject if $\testerone$ rejects. \\
Run PSGD on $\L_\sigma$ over $S$ to get a list $L$ of candidate vectors. \\
\For{each candidate $\w$ in $L$}{
Let $B_\w(\sigma)$ denote the band $\{ \x : |\inn{\w, \x}| \leq \sigma \}$. Let $\F_\w$ denote the class of functions of at most two halfspaces with weights orthogonal to $\w$. \\
Let $\delta' = \Theta(\delta/|L|)$. \\
Run $\testertwo(S, \w, \sigma, \delta')$ to verify that $\pr_{S}[B_\w] = \Theta(\sigma)$. Reject if $\testertwo$ rejects. \\
Run $\testerthree(S, \w, \sigma = \sigma/6, \tau, \delta')$ and $\testerthree(S, \w, \sigma = \sigma/2, \tau, \delta')$ for a suitable constant $\tau$ to verify that the empirical distribution conditioned on $B_\w(\sigma/6)$ and $\B_\w(\sigma/2)$ fools $\F_\w$ up to $\tau$. Reject if $\testerthree$ rejects. \\
Estimate the empirical error of $\w$ on $S$.
}
Output the $\w \in L$ with the best empirical error.
\end{algorithm}
To show our result, we revisit the approach of \cite{diakonikolas2020learning} for learning halfspaces with Massart noise under well-behaved distributions. Their result is based on the idea of minimizing a surrogate loss that is non convex, but whose stationary points correspond to halfspaces with low error. They also require that their surrogate loss is sufficiently smooth, so that one can find a stationary point efficiently. While the distributional assumptions that are used to demonstrate that stationary points of the surrogate loss can be discovered efficiently are mild, the main technical lemma, which demostrates that any stationary point suffices, requires assumptions that are not necessarily testable. We establish a label-dependent approach for testing, making use of tests that are applied during the course of our algorithm.
We consider a slightly different surrogate loss than the one used in \cite{diakonikolas2020learning}. In particular, for $\sigma>0$, we let
\begin{equation}\label{equation:surrogate_loss}
    \L_\sigma(\w) = \ex_{(\x,y)\sim \Djointemp} \biggr[ \loss_\sigma\biggr(-y \frac{\inn{\w, \x}}{\norm{\w}_2} \biggr) \biggr],
\end{equation}
where $\loss_\sigma:\R \to [0,1]$ is a smooth approximation to the ramp function with the properties described in Proposition \ref{proposition:activation}, obtained using a piecewise polynomial of degree $3$. Unlike the standard logistic function, our loss function has derivative exactly $0$ away from the origin (for $|t| > \sigma/2$).  This makes the analysis of the gradient of $\L_\sigma$ easier, since the contribution from points lying outside a certain band is exactly $0$.
\begin{proposition}\label{proposition:activation}
    There are constants $c,c'>0$, such that for any $\sigma>0$, there exists a continuously differentiable function $\ell_\sigma:\R \to [0,1]$ with the following properties.
    \begin{enumerate}
        \item For any $t\in[-\sigma/6,\sigma/6]$, $\ell_\sigma(t) = \frac{1}{2}+\frac{t}{\sigma}$.
        \item For any $t>\sigma/2$, $\ell_\sigma(t)=1$ and for any $t<-\sigma/2$, $\ell_\sigma(t) = 0$.
        \item For any $t\in\R$, $\ell_\sigma'(t) \in [0,c/\sigma]$, $\ell_\sigma'(t)=\ell_\sigma'(-t)$ and $|\ell_\sigma''(t)| \le c' / \sigma^2$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    We define $\ell_\sigma$ as follows.
\[
    \ell_\sigma(t) = 
    \begin{cases}
        \frac{t}{\sigma} + \frac{1}{2}, \text{ if }|t| \le \frac{\sigma}{6} \\
        1, \text{ if } t>\frac{\sigma}{2} \\
        0, \text{ if } t<\frac{-\sigma}{2} \\
        \ell^+(t), t\in(\frac{\sigma}{6}, \frac{\sigma}{2}] \\
        \ell^-(t), t\in[-\frac{\sigma}{2}, -\frac{\sigma}{6})
    \end{cases}
\]
for some appropriate functions $\ell^+,\ell^-$. It is sufficient that we pick $\ell^+$ satisfying the following conditions (then $\ell^-$ would be defined symmetrically, i.e., $\ell^-(t)=1-\ell^+(-t)$).
\begin{itemize}
    \item $\ell^+(\sigma/2) = 1$ and $\ell^{+\prime}(\sigma/2) = 0$.
    \item $\ell^+(\sigma/6) = 2/3$ and $\ell^{+\prime}(\sigma/6) = 1/\sigma$.
    \item $\ell^{+\prime \prime}$ is defined and bounded, except, possibly on $\sigma/6$ and/or $\sigma/2$.
\end{itemize}
We therefore need to satisfy four equations for $\ell^+$. So we set $\ell^+$ to be a degree $3$ polynomial: $\ell^+(t) = a_1t^3+a_2t^2+a_3t+a_4$. Whenever $\sigma>0$, the system has a unique solution that satisfies the desired inequalities. In particular, we may solve the equation to get $a_1=-9/\sigma^3, a_2 = 15/(2\sigma^2), a_3=-3/(4\sigma)$ and $a_4=5/8$. For the resulting function (see Figure \ref{fig:activation} below and Figure \ref{fig:activation-derivatives} in the appendix) we have that there are constants $c,c'>0$ such that $\ell^{+\prime}(t) \in [0,c/\sigma]$ and $|\ell^{+\prime\prime}(t)| \le c'/\sigma^2$ for any $t\in[\sigma/6,\sigma/2]$.
\end{proof}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
The smoothness allows us to run PSGD to obtain stationary points efficiently, and we now state the convergence lemma we need.
\begin{proposition}[PSGD Convergence, Lemmas 4.2 and B.2 in \cite{diakonikolas2020learning}]\label{corollary:sgd}
    Let $\L_\sigma$ be as in Equation \eqref{equation:surrogate_loss} with $\sigma\in(0,1]$, $\ell_\sigma$ as described in Proposition \ref{proposition:activation} and $\Djointemp$ such that the marginal $\Dtrueemp$ on $\R^d$ satisfies Property \eqref{equation:property1_fool_polynomials} for $k=2$. Then, for any $\epsilon>0$ and $\delta\in (0,1)$, there is an algorithm whose time and sample complexity is $O(\frac{d}{\sigma^4} +\frac{\log(1/\delta)}{\eps^4\sigma^4})$, which, having access to samples from $\Djointemp$, outputs a list $L$ of vectors $\w\in\S^{d-1}$ with $|L| = O(\frac{d}{\sigma^4} +\frac{\log(1/\delta)}{\eps^4\sigma^4})$ so that there exists $\w\in L$ with 
    \[
        \norm{\nabla_{\w}\L_\sigma(\w)}_2 \le \epsilon\,, \text{ with probability at least }1-\delta\,.
    \]
    In particular, the algorithm performs Stochastic Gradient Descent on $\L_\sigma$ Projected on $\S^{d-1}$ (PSGD).
\end{proposition}
It now suffices to show that, upon performing PSGD on $\L_\sigma$, for some appropriate choice of $\sigma$, we acquire a list of vectors that testably contain a vector which is approximately optimal. We first prove the following lemma, whose distributional assumptions are relaxed compared to the corresponding structural Lemma 3.2 of \cite{diakonikolas2020learning}. In particular, instead of requiring the marginal distribution to be ``well-behaved"", we assume that the quantities of interest (for the purposes of our proof) have expected values under the true marginal distribution that are close, up to multiplicative factors, to their expected values under some ``well-behaved"" (in fact, strongly log-concave) distribution. While some of the quantities of interest have values that are miniscule and estimating them up to multiplicative factors could be too costly, it turns out that the source of their vanishing scaling can be completely attributed to factors of the form $\pr[|\inn{\w, \x}|\le \sigma]$ (where $\sigma$ is small), which, due to standard concentration arguments, can be approximated up to multiplicative factors, given $\w\in\S^{d-1}$ and $\sigma>0$ (see Proposition \ref{tester2_band}). As a result, we may estimate the remaining factors up to sufficiently small additive constants (see Proposition \ref{tester3_truncated}) to get multiplicative overall closeness to the ``well behaved"" baseline.
\begin{lemma}\label{lemma:stationary_points_suffice_massart}
    Let $\L_\sigma$ be as in Equation \eqref{equation:surrogate_loss} with $\sigma\in(0,1]$, $\ell_\sigma$ as described in Proposition \ref{proposition:activation}, let $\w\in\S^{d-1}$ and consider $\Djointemp$ such that the marginal $\Dtrueemp$ on $\R^d$ satisfies Properties \eqref{equation:property2_fool_band_indicators} and \eqref{equation:property3_fool_orthogonal_halfspaces_truncated} for $C_4 = 2$ and accuracy $\tau$. 
    Let $\woptemp\in\S^{d-1}$ define an optimum halfspace and let $\eta<1/2$ be an upper bound on the rate of the Massart noise. Then, there are constants $c_1,c_2,c_3>0$ such that if $\norm{\nabla_{\w}\L_\sigma(\w)}_2 < c_1(1-2\eta)\sigma$ and $\tau\le c_2$, then
    \[
        \measuredangle(\w,\woptemp) \le \frac{c_3}{{1-2\eta}}\cdot  \sigma \;\;\text{ or }\;\; \measuredangle(-\w,\woptemp) \le \frac{c_3}{{1-2\eta}}\cdot  \sigma
    \]
\end{lemma}
\begin{proof}
    We will prove the contrapositive of the claim, namely, that there are constants $c_1,c_2,c_3>0$ such that if $\measuredangle(\w,\woptemp), \measuredangle(-\w,\woptemp) > \frac{c_3}{\sqrt{1-2\eta}}\cdot \sigma$, and $\tau\le c_2$, then $\norm{\nabla_{\w}\L_\sigma(\w)}_2 \ge c_1(1-2\eta)\sigma$.
    Consider the case where $\measuredangle(\w,\woptemp) < \pi/2$ (otherwise, perform the same argument for $-\w$). 
    Let $\vv$ be a unit vector orthogonal to $\w$ that can be expressed as a linear combination of $\w$ and $\woptemp$ and for which $\inn{\vv,\woptemp} = 0$. Then $\{\vv,\w\}$ is an orthonormal basis for $V=\spn(\w,\woptemp)$. For any vector $\x\in\R^d$, we will use the following notation: $\x_{\w} = \inn{\w,\x}$, $\x_{\vv} = \inn{\vv,\x}$. It follows that $\proj_V(\x) = \x_{\w}\w+\x_{\vv}\vv$, where $\proj_V$ is the operator that orthogonally projects vectors on $V$.
    Using the fact that $\nabla_{\w}({\inn{\w,\x}}/{\norm{\w}_2}) = \x-\inn{\w,\x}\w = \x-\x_{\w}\w$ for any $\w\in\S^{d-1}$, the interchangeability of the gradient and expectation operators and the fact that $\ell_\sigma'$ is an even function we get that
    \begin{align*}
        \nabla_{\w}\L_\sigma(\w) = \ex \Bigr[ - \ell_\sigma'( |{\inn{\w,\x}}| ) \cdot y\cdot (\x-\x_{\w} \w) \Bigr]
    \end{align*}
    Since the projection operator $\proj_V$ is a contraction, we have $\norm{\nabla_{\w}\L_\sigma(\w)}_2 \ge \norm{\proj_V\nabla_{\w}\L_\sigma(\w)}_2$,
    and we can therefore restrict our attention to a simpler, two dimensional problem. In particular, since $\proj_V(\x) = \x_{\w}\w+\x_{\vv}\vv$, we get
    \begin{align*}
        \norm{\proj_V\nabla_{\w}\L_\sigma(\w)}_2
        &= \Bigr\| \ex \Bigr[ - \ell_\sigma'( |\x_{\w}| ) \cdot y\cdot \x_{\vv} \vv \Bigr] \Bigr\|_2 \\
        &= \Bigr| \ex \Bigr[ - \ell_\sigma'( |\x_{\w}| ) \cdot y\cdot \x_{\vv} \Bigr] \Bigr| \\
        &= \Bigr| \ex \Bigr[ - \ell_\sigma'( |\x_{\w}| ) \cdot \sign(\inn{\woptemp,\x}) \cdot (1-2\ind\{y\neq \sign(\inn{\woptemp,\x})\})\cdot \x_{\vv} \Bigr] \Bigr|
    \end{align*}
    Let $F(y,\x)$ denote $ 1-2\ind\{y\neq \sign(\inn{\woptemp,\x})\}$. We may write $\x_{\vv}$ as $|\x_{\vv}|\cdot \sign(\x_{\vv})$ and let $\G\subseteq\R^2$ such that $\sign(\x_{\vv})\cdot \sign(\inn{\woptemp, \x}) = -1$ iff $\x\in\G$. Then, $\sign(\x_{\vv})\cdot \sign(\inn{\woptemp, \x}) = \ind\{\x\not\in \G\} - \ind\{\x\in \G \}$. We get
    \begin{align*}
        \norm{&\proj_V\nabla_{\w}\L_\sigma(\w)}_2 = \\
        & = \Bigr| \ex \Bigr[ \ell_\sigma'( |\x_{\w}| ) \cdot (\ind\{\x\in \G\} - \ind\{\x\not\in \G \}) \cdot F(y,\x) \cdot |\x_{\vv}| \cdot  \Bigr] \Bigr| \ge \\
        & \ge \ex \Bigr[ \ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\in \G\} \cdot F(y,\x) \cdot |\x_{\vv}|  \Bigr]  -  \ex \Bigr[ \ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\not\in \G \} \cdot F(y,\x) \cdot |\x_{\vv}|  \Bigr] 
    \end{align*}
    Let $A_1 = \ex [ \ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\in \G\} \cdot F(y,\x) \cdot |\x_{\vv}|  ]$ and $A_2 = \ex [ \ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\not\in \G\} \cdot F(y,\x) \cdot |\x_{\vv}|  ]$. (See Figure \ref{fig:regions}.) Note that $\ex_{y|\x}[F(y,\x)] = 1-2\eta(\x) \in [1-2\eta,1]$, where $1-2\eta>0$. Therefore, we have that $A_1 \ge (1-2\eta)\cdot \ex [ \ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\in \G\} \cdot |\x_{\vv}| ]$ and $A_2 \le \ex [ \ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\not\in \G\} \cdot |\x_{\vv}|  ]$.
    
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
    
    Note that due to Proposition \ref{proposition:activation}, $\ell_\sigma'(|\x_{\w}|) \le c/\sigma$ for some constant $c$ and $\ell_\sigma'(|\x_{\w}|) = 0$ whenever $|\x_{\w}|> \sigma/2$. Therefore, if $\U_2$ is the band $B_{\w}(\sigma/2) = \{\x:|\x_{\w}| \le \sigma/2\}$ we have
    \begin{equation}\label{equation:A1}
        A_2 \le \frac{c}{\sigma}\cdot \ex [ \ind\{\x\not\in \G\}\cdot \ind\{\x\in \U_2\} \cdot |\x_{\vv}|  ]
    \end{equation}
    
    Moreover, for each individual $\x$, we have $\ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\in \G\} \cdot |\x_{\vv}| \ge 0$, due to the properties of $\ell_\sigma'$ (Proposition \ref{proposition:activation}). Hence, for any set $\U_1\subseteq\R^d$ we have that 
    \[
        A_1 \ge (1-2\eta) \cdot \ex [\ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\in \G\} \cdot \ind\{\x\in\U_1\} \cdot |\x_{\vv}|]
    \]
    Setting $\U_1 = B_{\w}(\sigma/6) = \{\x:|\x_{\w}| \le \sigma/6\}$, by Proposition \ref{proposition:activation}, we get $\ell_\sigma'(|\x_{\w}|) \cdot \ind\{\x\in\U_1\} = \frac{1}{\sigma} \cdot \ind\{\x\in\U_1\}$.
    \begin{equation}\label{equation:A2}
        A_1 \ge \frac{1-2\eta}{\sigma}\cdot \ex [ \ind\{\x\in \G\} \cdot \ind\{\x\in\U_1\} \cdot |\x_{\vv}|]
    \end{equation}
    We now observe that by the definitions of $\G,\U_1,\U_2$,
    
    for any constant $R>0$, there exist some constants $c',c''>0$ such that if $\sigma/\tan\theta < c'R$ (the points in $\R^2$ where $\partial\overline{\G}$ intersects either $\partial \U_1$ or $\partial\U_2$ have projections on $\vv$ that are $\Theta(\sigma/\tan\theta)$) we have that
    \begin{align*}
        &\ind\{\x\in \G\} \cdot \ind\{\x\in\U_1\} \ge \ind\{|\x_{\vv}|\in [c'R, 2c'R]\} \cdot \ind\{\x\in \U_1\} \;\;\;\text{ and }\\
        &\ind\{\x\in \G\} \cdot \ind\{\x\in\U_2\} \le \ind\{|\x_{\vv}| \le c''\sigma/\tan\theta\} \cdot \ind\{\x\in \U_2\}
    \end{align*}
    By equations \eqref{equation:A1} and \eqref{equation:A2}, we get the following bounds whose graphical representations can be found in Figure \ref{fig:regions}.
    \begin{align}
        A_1& \ge \frac{c'R(1-2\eta)}{\sigma} \cdot \ex [ \ind\{|\x_{\vv}|\in [c'R, 2c'R]\} \cdot \ind\{\x\in \U_1\} ] \label{equation:A1-bound} \\
        A_2& \le \frac{c\cdot c''}{\tan\theta} \cdot \ex [ \ind\{|\x_{\vv}| \le c''\sigma/\tan\theta\} \cdot \ind\{\x\in \U_2\} \label{equation:A2-bound} ]
    \end{align}
    So far, we have used no distributional assumptions. Now, consider the corresponding expectations under the target marginal $\Dtgt$ (which we assumed to be strongly log-concave).
    \begin{align*}
        I_1& = \ex_{\Dtgt} [ \ind\{|\x_{\vv}|\in [c'R, 2c'R]\} \cdot \ind\{\x\in \U_1\} ] \\
        I_2& = \ex_{\Dtgt} [ \ind\{|\x_{\vv}| \le c''\sigma/\tan\theta\} \cdot \ind\{\x\in \U_2\} ]
    \end{align*}
    Any strongly log-concave distribution enjoys the ``well-behaved"" properties defined by \cite{diakonikolas2020learning}, and therefore, if $R$ is picked to be small enough, then $I_1$ and $I_2$ are of order $\Theta(\sigma)$ (due to upper and lower bounds on the two dimensional marginal density over $V$ within constant radius balls -- aka anti-anticoncentration and anticoncentration). Moreover, by Proposition \ref{prop:slc}, we have $\pr[\x\in\U_1]$ and $\pr[\x\in\U_2]$ are both of order $\Theta(\sigma)$. Hence we have that there exist constants $c_1',c_2'>0$ such that for the conditional expectations we have
    \begin{align*}
        \ex_{\Dtgt} \bigr[ \ind\{|\x_{\vv}|\in [c'R, 2c'R]\} \;\bigr|\; \ind\{\x\in \U_1\} \bigr] \ge c_1' \\
        \ex_{\Dtgt} \bigr[ \ind\{|\x_{\vv}| \le c''\sigma/\tan\theta\} \;\bigr|\; \ind\{\x\in \U_2\} \bigr] \le c_2'
    \end{align*}
    By assumption, Property \eqref{equation:property3_fool_orthogonal_halfspaces_truncated} holds and, therefore, if $\tau\le c_1'/2,c_2'/2=:c_2$, we get that
    \begin{align*}
        \ex_{\Dtrueemp} \bigr[ \ind\{|\x_{\vv}|\in [c'R, 2c'R]\} \;\bigr|\; \ind\{\x\in \U_1\} \bigr] \ge c_1'/2 \\
        \ex_{\Dtrueemp} \bigr[ \ind\{|\x_{\vv}| \le c''\sigma/\tan\theta\} \;\bigr|\; \ind\{\x\in \U_2\} \bigr] \le c_2'/2
    \end{align*}
    Moreover, by Property \eqref{equation:property2_fool_band_indicators}, we have that (under the true marginal) $\pr[\x\in\U_1]$ and $\pr[\x\in\U_2]$ are both $\Theta(\sigma)$. Hence, in total, we get that for some constants $\tilde{c}_1, \tilde{c}_2$, we have 
    \begin{align*}
        A_1&\ge \tilde{c}_1\cdot ({1-2\eta}) \\
        A_2&\le \tilde{c}_2\cdot \frac{\sigma}{\tan \theta}
    \end{align*}
    Hence, if we pick $\sigma = \Theta((1-2\eta) \tan\theta)$, we get the desired result.    
\end{proof}
Combining Proposition \ref{corollary:sgd} and Lemma \ref{lemma:stationary_points_suffice_massart}, we get that for any choice of the parameter $\sigma\in(0,1]$, by running PSGD on $\L_\sigma$, we can construct a list of vectors of polynomial size (in all relevant parameters) that testably contains a vector that is close to the optimum weight vector. In order to link the zero-one loss to the angular similarity between a weight vector and the optimum vector, we use the following Proposition. 
\begin{proposition}\label{proposition:angle_to_01}
    Let $\Djointemp$ be a distribution over $\R^d\times \{\pm1\}$, $\woptemp\in\arg\min_{\w\in\S^{d-1}}\pr_{\Djointemp}[y\neq \sign(\inn{\w,\x})]$ and $\w\in\S^{d-1}$. 
    
    
    
    
    Then, for any $\theta \ge \measuredangle(\w,\woptemp)$, $\theta\in[0,\pi/4]$, if the marginal $\Dtrueemp$ on $\R^d$ satisfies Property \eqref{equation:property1_fool_polynomials} for $C_1>0$ and some even $k\in\mathbb{N}$ and Property \eqref{equation:property2_fool_band_indicators} with $\sigma$ set to $(C_1k)^{\frac{k}{2(k+1)}} \cdot (\tan \theta)^{\frac{k}{k+1}}$, then, there exists a constant $c>0$ such that the following is true.
    \[
        \pr_{\Djointemp}[y\neq \sign(\inn{\w,\x})]
        
        \le \optemp + c\cdot k^{1/2}\cdot \theta^{1-\frac{1}{k+1}}\,.
    \]
    
\end{proposition}
\begin{proof}
    For the following all the probabilities and expectations are over $\Djointemp$.
First we observe that
\begin{align*}
    \pr[y\neq \sign(\inn{\w,\x})] &\le \pr[y\neq \sign(\inn{\w,\x}) \cap y= \sign(\inn{\woptemp,\x})] + \pr[y\neq \sign(\inn{\woptemp,\x})] \le \\
    &\le \pr[\sign(\inn{\w,\x}) \neq \sign(\inn{\woptemp,\x})] + \optemp\,.
\end{align*}
Then, we observe that by assumption that $\Djoint$ satisfies Property~\eqref{equation:property2_fool_band_indicators}, we have
\[
    \pr[|\inn{\w,\x}|\le \sigma] \le C_3\sigma
\]
and that
\[
    \pr[\sign(\inn{\w,\x}) \neq \sign(\inn{\woptemp,\x}) \cap |\inn{\w,\x}|> \sigma] \le \pr\Bigr[|\inn{\vv,\x}|\ge \frac{\sigma}{\tan \theta}\Bigr]\,,
\]
where $\vv$ is some vector perpendicular to $\w$. Using Markov's inequality, we get
\[
    \pr\Bigr[|\inn{\vv,\x}|\ge \frac{\sigma}{\tan \theta}\Bigr] \le \frac{(\tan\theta)^k}{\sigma^k}\cdot \ex[ |\inn{\vv,\x}|^k ]\,.
\]
But, by assumption that $\Djoint$ satisfies Property~\eqref{equation:property1_fool_polynomials}, there is some constant $C_1>0$ such that $\ex[ |\inn{\vv,\x}|^k ] \le (C_1k)^{k/2}$. Thus
\begin{align*}
    \pr[\sign(\inn{\w,\x}) \neq \sign(\inn{\woptemp,\x})] &\le \pr[|\inn{\w,\x}|\le \sigma] \\
     &\qquad+\pr[\sign(\inn{\w,\x}) \neq \sign(\inn{\woptemp,\x}) \cap |\inn{\w,\x}|> \sigma] \\
     &\leq C_3 \sigma + \frac{(C_1k)^{k/2} (\tan \theta)^k}{\sigma^k}.
\end{align*}
By picking $\sigma$ appropriately in order to balance the two terms (note that this is a different $\sigma$ than the one in \cref{lemma:stationary_points_suffice_massart}), we get the desired result.
\end{proof}
We are now ready to prove Theorem \ref{theorem:massart}.
\begin{proof}[Proof of Theorem \ref{theorem:massart}]
    Throughout the proof we consider $\delta'$ to be a sufficiently small polynomial in all the relevant parameters. Each of the failure events will have probability at least $\delta'$ and their number will be polynomial in all the relevant parameters, so by the union bound, we may pick $\delta'$ so that the probability of failure is at most $\delta$.
    In Proposition \ref{corollary:sgd}, Lemma \ref{lemma:stationary_points_suffice_massart} and Proposition \ref{proposition:angle_to_01} we have identified certain properties of the marginal distribution that are sufficient for our purposes. Our testers $\testerone,\testertwo,\testerthree$ verify that these properties hold for the empirical marginal over our sample $S$, and it will be convenient to analyze the optimality of our algorithm purely over $S$. In particular, we will need to require that $|S|$ is sufficiently large, so that when the true marginal is indeed the target $\Dtgt$, our testers succeed with high probability (for the corresponding sample complexity, see Propositions \ref{tester1_polynomials}, \ref{tester2_band} and \ref{tester3_truncated}). Moreover, by standard generalization theory, since the VC dimension of halfspaces is only $O(d)$ and for us $|S|$ is a large $\poly(d, 1/\eps)$, both the error of our final output and the optimal error over $S$ will be close to that over $\Djoint$. So in what follows, we will abuse notation and refer to the uniform distribution over $S$ as $\Djointemp$ and the optimal error over $S$ simply as $\optemp$.
    
    We begin with some basic tests. Throughout the algorithm, whenever a tester fails, we reject, otherwise we proceed. First, we run $\testerone$ (Proposition \ref{tester1_polynomials}) with $k=2$ to verify that the marginals are approximately isotropic. By Proposition \ref{tester1_polynomials} we get that $\Djointemp$ satisfies the distributional requirement of Proposition \ref{corollary:sgd}.
    
    Let $\sigma \in(0,1]$ to be defined. We then run PSGD on $\L_\sigma$ as described in Proposition \ref{corollary:sgd} with $\epsilon = c_1(1-2\eta)\sigma/2$, where $c_1$ is given by Lemma \ref{lemma:stationary_points_suffice_massart}. By Proposition \ref{corollary:sgd}, we get a list $L$ of vectors $\w\in\S^{d-1}$ with $|L| = \poly(d,1/\sigma)$ such that there exists $\w\in L$ with $\norm{\nabla_{\w}\L_\sigma(\w)}_2 < c_1(1-2\eta)\sigma$. 
    
    Having acquired the list $L$, for each $\w \in L$, we run testers $\testertwo$ with inputs $(\w,\sigma/2,\delta')$ and $(\w,\sigma/6,\delta')$ (Proposition \ref{tester2_band}) and $\testerthree$ with inputs $(\w,\sigma/2,c_2,\delta')$ and with $(\w,\sigma/6,c_2,\delta')$ (Proposition \ref{tester3_truncated}, $c_2$ as defined in Lemma \ref{lemma:stationary_points_suffice_massart}). This ensures that for each $\w$, the probability within the band $B_\w(\sigma/2) = \{ \x : |\inn{\w, \x}| \leq \sigma/2 \}$ is $\Theta(\sigma)$ (and similarly for $B_\w(\sigma/6)$) and moreover that our marginal conditioned on each of the bands fools (up to an additive constant) functions of halfspaces with weights orthogonal to $\w$.  As a result, we may apply Lemma \ref{lemma:stationary_points_suffice_massart} to each of the elements of $L$ and form a list of $2|L|$ vectors $\w\in\S^{d-1}$ which contains some $\w$ with $\measuredangle(\w,\woptemp) \le c_2\sigma/({1-2\eta})$ (where $c_3$ is as defined in Lemma \ref{lemma:stationary_points_suffice_massart}). 
    Since we have already passed the tester $\testerone$ with $k=2$ and we may use the tester $\testertwo$ once again, with appropriate parameters for each the elements of $L$ and their negations, we may also apply Proposition \ref{proposition:angle_to_01} to get that our list contains a vector $\w$ with
    \[
        \pr_{\Djointemp}[y\neq \sign(\inn{\w,\x})] \le \optemp + c\cdot \theta^{2/3},\,
    \]
    where $\measuredangle(\w,\woptemp) \le \theta := c_2\sigma/\sqrt{1-2\eta}$. By picking $\sigma = \Theta(\epsilon^{3/2}({1-2\eta}))$, we get
    \[
        \pr_{\Djointemp}[y\neq \sign(\inn{\w,\x})] \le \optemp + \epsilon\,.
    \]
    However, we do not know which of the weight vectors in our list is the one guaranteed to achieve small error. In order to discover this vector, we estimate the probability of error of each of the corresponding halfspaces (which can be done efficiently, due to Hoeffding's bound) and pick the one with the smallest error. This final step does not require any distributional assumptions and we do not need to perform any further tests.
\end{proof}
\section{Testably learning halfspaces in the agnostic setting}\label{section:agnostic}
In this section, we prove our result on efficiently and testably learning halfspaces in the agnostic setting. 
\begin{theorem}[Efficient Tester-Learner for Halfspaces in the Agnostic Setting]\label{theorem:agnostic}
        Let $\Djoint$ be a distribution over $\R^d\times \{\pm 1\}$ and let $\Dtgt$ be a strongly log-concave distribution over $\R^d$ (Definition \ref{definition:strongly_log_concave}). Let $\C$ be the class of origin centered halfspaces in $\R^d$. Then, for any even $k\in\mathbb{N}$, any $\eps>0$ and $\delta\in(0,1)$, there exists an algorithm that agnostically testably learns $\C$ w.r.t. $\Dtgt$ up to error $O(k^{1/2}\cdot \opt^{1-\frac{1}{k+1}})+\epsilon$, where $\opt = \min_{\w\in\S^{d-1}}\pr_{\Djoint}[y\neq \sign(\inn{\w,\x})]$, and error probability at most $\delta$, using time and a number of samples from $\Djoint$ that are polynomial in $d^{\tilde{O}(k)},(1/\eps)^{\tilde{O}(k)}$ and $(\log(1/\delta))^{O(k)}$.
        In particular, by picking some appropriate $k\le \log^2 d$, we obtain error $\tilde{O}(\opt) + \eps$ in quasipolynomial time and sample complexity, i.e.\ $\poly(2^{\polylog d}, (\frac{1}{\epsilon})^{\polylog d})$.
\end{theorem}
To prove Theorem \ref{theorem:agnostic}, we may follow a similar approach as the one we used for the case of Massart noise. However, in this case, the main structural lemma regarding the quality of the stationary points involves an additional requirement about the parameter $\sigma$. In particular, $\sigma$ cannot be arbitrarily small with respect to the error of the optimum halfspace, because, in this case, there is no upper bound on the amount of noise that any specific point $\x$ might be associated with. As a result, picking $\sigma$ to be arbitrarily small would imply that our algorithm only considers points that lie within a region that has arbitrarily small probability and can hence be completely corrupted with the adversarial $\optemp$ budget. 
On the other hand, the polynomial slackness that the testability requirement introduces (through Proposition \ref{proposition:angle_to_01}) between the error we achieve and the angular distance guarantee we can get via finding a stationary point of $\L_\sigma$ (which is now coupled with $\optemp$), appears to the exponent of the guarantee we achieve in Theorem \ref{theorem:agnostic}.
\begin{lemma}\label{lemma:stationary_points_suffice_agnostic}
    Let $\L_\sigma$ be as in Equation \eqref{equation:surrogate_loss} with $\sigma\in(0,1]$, $\ell_\sigma$ as described in Proposition \ref{proposition:activation}, let $\w\in\S^{d-1}$ and consider $\Djointemp$ such that the marginal $\Dtrueemp$ on $\R^d$ satisfies Properties \eqref{equation:property2_fool_band_indicators}, \eqref{equation:property3_fool_orthogonal_halfspaces_truncated} and \eqref{equation:property4_fool_variance_truncated} for $\w$ with $C_4 = 2$ and accuracy parameter $\tau$. Let $\optemp$ be the minimum error achieved by some origin centered halfspace and let $\woptemp\in\S^{d-1}$ be a corresponding vector. Then, there are constants $c_1,c_2,c_3,c_4>0$ such that if $\optemp \le c_1 \sigma$, $\norm{\nabla_{\w}\L_\sigma(\w)}_2 < c_2\sigma$, and $\tau\le c_3$ then
    \[
        \measuredangle(\w,\woptemp) \le c_4 \sigma \;\;\text{ or }\;\; \measuredangle(-\w,\woptemp) \le c_4 \sigma.
    \]
\end{lemma}
\begin{proof}
    In the agnostic case, the proof is analogous to the proof of Lemma \ref{lemma:stationary_points_suffice_massart}. However, in this case, the difference is that the random variable $F(y,\x) = 1-2\ind\{y\neq \sign(\inn{\woptemp,\x})\}$ does not have conditional expectation on $\x$ that is lower bounded by a constant. Instead, we need to consider an additional term $A_3$ correcponding to the part $2\ind\{y\neq \sign(\inn{\woptemp,\x})\}$ and the term $A_1$ will not be scaled by the factor $(1-2\eta)$ as in Lemma \ref{lemma:stationary_points_suffice_massart}. Hence, with similar arguments we have that
    \[
        \norm{\nabla_{\w} \L_\sigma(\w)}_2 \ge A_1 - A_2 - A_3\,,
    \]
    where $A_1 \ge \tilde{c}_1$, $A_2 \le \tilde{c}_2\cdot \frac{\sigma}{\tan\theta}$ and (using properties of $\ell_\sigma'$ as in Lemma \ref{lemma:stationary_points_suffice_massart} and the Cauchy-Schwarz inequality)
    \begin{align*}
        A_3 
        &= 2\ex [\ell_\sigma'( |\x_{\w}| ) \cdot \ind\{\x\in \G\} \cdot \ind\{y\neq\sign(\inn{\w,\x})\} \cdot |\x_{\vv}|] \le\\
        &\le \frac{2c}{\sigma}\cdot \ex [ \ind\{\x\in \U_2\} \cdot \ind\{y\neq\sign(\inn{\w,\x})\} \cdot |\x_{\vv}|] \le \\
        &\le \frac{2c}{\sigma}\cdot \sqrt{\ex [ \ind\{\x\in \U_2\} \cdot (\x_{\vv})^2]}\cdot \sqrt{\ex [\ind\{y\neq\sign(\inn{\w,\x})\}]} = \\
        &= \frac{2c\sqrt{\optemp}}{\sigma}\cdot \sqrt{\ex [ \inn{\w,\x}^2 \;|\; \x\in\U_2 ]\cdot \pr[\x\in\U_2]}\,.
    \end{align*}
    Similarly to our approach in the proof of Lemma \ref{lemma:stationary_points_suffice_massart}, we can use the assumed properties \eqref{equation:property2_fool_band_indicators} and \eqref{equation:property4_fool_variance_truncated} to get that
    \[
        A_3 \le \tilde{c}_3\frac{\sqrt{\optemp}}{\sqrt{\sigma}}\,,
    \]
    which gives that in order for the gradient loss to be small, we require $\optemp \le \Theta(\sigma)$.
\end{proof}
We can now prove \cref{theorem:agnostic}.
\begin{proof}[Proof of \cref{theorem:agnostic}]
    We will follow the same steps as for proving Theorem \ref{theorem:massart}. Once more, we draw a sufficiently large sample so that our testers are ensured to accept with high probability when the true marginal is indeed the target marginal $\Dtgt$ and so that we have generalization, i.e.\ the guarantee that any approximate minimizer of the empirical error (error on the uniform empirical distribution over the sample drawn) is also an approximate minimizer of the true error.
    The main difference between the Massart noise case and the agnostic case is that in the former we were able to pick $\sigma$ arbitrarily small, while in the latter we face a more delicate tradeoff. To balance competing contributions to the gradient norm, we must ensure that $\sigma$ is at least $\Theta(\optemp)$ while also ensuring that it is not too large. And since we do not know the value of $\opt$, we will need to search over a space of possible values for $\sigma$ that is only polynomially large in relevant parameters (similar to the approach of \cite{diakonikolas2020non}). In our case, we may sparsify the space $(0,1]$ of possible values for $\sigma$ up to accuracy $\Theta((\frac{\epsilon}{\sqrt{k}})^{1+1/k})$ and form a list of $\poly(k/\epsilon)$ possible values for $\sigma$, one of which will satisfy $c_1\sigma - \Theta((\frac{\epsilon}{\sqrt{k}})^{1+1/k}) \le \optemp \le c_1\sigma$. hence, we perform the same (testing-learning) process for each of the possible values of $\sigma$ and get a list of candidate vectors which is still of polynomial size.
    The final step is, again, to use Proposition \ref{proposition:angle_to_01}, after running tester $\testerone$ with parameter $k$ (Proposition \ref{tester1_polynomials}) and tester $\testertwo$ with appropriate parameters for each of the candidate weight vectors. We get that our list contains a vector $\w$ with 
    \[
        \pr_{\Djointemp}[y\neq \sign(\inn{\w,\x})] \le \optemp + c\cdot k^{1/2}\cdot  \theta^{1-1/(k+1)},\,
    \]
    where $\measuredangle(\w,\woptemp) \le \theta := c_2\sigma$ for $\sigma$ such that $c_1\sigma - \Theta((\frac{\epsilon}{\sqrt{k}})^{1+1/k}) \le \optemp \le c_1\sigma$. 
    \[
        \pr_{\Djointemp}[y\neq \sign(\inn{\w,\x})] \le \optemp + c \sqrt{k}\cdot \Bigr(\frac{c_2}{c_1}\opt + \Theta\Bigr(\Bigr(\frac{\epsilon}{\sqrt{k}}\Bigr)^{1+\frac{1}{k}}\Bigr)\Bigr)^{1-\frac{1}{k+1}} \le O(\sqrt{k}\cdot \opt^{1-\frac{1}{k+1}}) + \epsilon\,.
    \]
    However, we do not know which of the weight vectors in our list is the one guaranteed to achieve small error. In order to discover this vector, we estimate the probability of error of each of the corresponding halfspaces (which can be done efficiently, due to Hoeffding's bound) and pick the one with the smallest error. This final step does not require any distributional assumptions and we do not need to perform any further tests.
    Finally, to obtain our $\tilde{O}(\opt)$ quasipolynomial time guarantee, observe first that we may assume without loss of generality that $\opt \geq 1/d^C$ for some $C$; if instead $\opt = o(1/d^2)$, say, then a sample of $O(d)$ points will with high probability be noiseless, and so simple linear programming will recover a consistent halfspace that will generalize. Moreover, we may assume that $\opt \le 1/10$, since otherwise achieving $O(\opt)$ is trivial (we may output an arbitrary halfspace). Let us adapt our algorithm so that we run tester $\testerone$ (see Proposition~\ref{tester1_polynomials}) multiple times for all $k = 1,2,\dots, \lceil\log^2 d\rceil$ (this only changes our time and sample complexity by a $\polylog(d)$ factor). Then Proposition~\ref{proposition:angle_to_01} holds for some $k^*$ such that $k^*\in[\log(1/\opt), 2\log(1/\opt)]$, since the interval has length at least $1$ (and therefore it contains some integer) and $2\log(1/\opt) \le 2C \log d \leq \log^2 d$ (for large enough $d$).
    Therefore, by picking the best candidate we get a guarantee of order
    \begin{align*} 
        \sqrt{k^*}\cdot \opt^{1 - 1/{k^*}} &= \sqrt{k^*}\cdot \opt^{-1/k^*} \opt \\
        &= \sqrt{k^*} \cdot 2^{\frac{1}{k^*} \log \frac{1}{\opt}} \cdot \opt \\
        &\le \sqrt{2\log(1/\opt)} \cdot 2 \cdot \opt \tag*{(since $\log(1/\opt) \leq k^* \leq 2\log(1/\opt)$)} \\
        &= \wt{O}(\opt)\,.
    \end{align*}
    
\end{proof}
\newcommand{\etalchar}[1]{$^{#1}$}
{alpha}
\newcommand{\etalchar}[1]{$^{#1}$}
\appendix
\section{Proofs for Section \ref{sec:testing}}\label{appendix:testing-proofs}
\subsection{Proof of Proposition~\ref{tester1_polynomials}}
The tester $\testerone$ does the following:
\begin{enumerate}
    \item For all $\alpha \in \Z_{\geq 0}^d$ with $|\alpha| =k$:
    \begin{enumerate}
        \item Compute the corresponding moment $\ex_{(\x,y) \sim \Dgeneric} \x^{\alpha}:=\frac{1}{|S|}\sum_{\x \in S} \x^{\alpha}$.
        \item If $\left\lvert\ex_{(\x,y) \sim \Dgeneric} [\x^{\alpha}]
        -
        \ex_{\x \sim \Dtgt} \left[\x^{\alpha}\right]
       \right\rvert > \frac{1}{d^k}$ then reject.
    \end{enumerate}
    \item If all the checks above passed, accept.
\end{enumerate}
First, we claim that for some absolute constant $C_1$, if the tester above accepts, we have $\ex_{(\x,y) \sim \Dgeneric}[(\inn{\vv, \x})^k] \leq (C_1k)^{k/2}$ for any $\vv \in \S^{d-1}$. To show this, we first recall that by Proposition~\ref{prop:slc}(e) it is the case that $\ex_{(\x,y) \sim \Dtgt}[(\inn{\vv, \x})^k] \leq (K_3 k)^{k/2}$. But we have 
\begin{align*}
\left\lvert
\ex_{(\x,y) \sim \Dgeneric}[(\inn{\vv, \x})^k] -\ex_{(\x,y) \sim \Dtgt}[(\inn{\vv, \x})^k] 
\right\rvert 
&\leq
\sum_{\alpha : |\alpha| = k}
\left\lvert \ex_{(\x,y) \sim \Dgeneric} [\x^{\alpha}]
        -
        \ex_{\x \sim \Dtgt} [\x^{\alpha}]
       \right\rvert 
       \\
       &\leq
       d^k \cdot \max_{\alpha : |\alpha| = k}
\left\lvert\ex_{(\x,y) \sim \Dgeneric} [\x^{\alpha}]
        -
        \ex_{\x \sim \Dtgt} [\x^{\alpha}]
       \right\rvert 
       \leq 1
\end{align*}
Together with the bound $\ex_{(\x,y) \sim \Dtgt}[(\inn{\vv, \x})^k] \leq (K_3 k)^{k/2}$, the above implies that $\ex_{(\x,y) \sim \Dgeneric}[(\inn{\vv, \x})^k] \leq (C_1k)^{k/2}$ for some constant $C_1$.
Now, we need to show that if the elements of $S$ are chosen i.i.d. from $\Dtgt$, and $|S|\geq \left( d^k, \left(\log \frac{1}{\delta}\right)^k \right)^{C_1}$ then the tester above accepts with probability at least $1-\delta$. Consider any specific multi-index $\alpha \in \Z_{\geq 0}^d$ with $|\alpha|=k$. Now, by Proposition~\ref{prop:slc}(f) we have the following:
\begin{align*}
\ex_{\x \sim \Dtgt} \left[\left(\x^{\alpha}- \ex_{\z \sim \Dtgt} \left[\z^{\alpha}\right]\right)^{2\log(1/\delta)}\right]
&\leq
\sum_{\ell=0}^{2\log(1/\delta)}
\left(
\ex_{\x \sim \Dtgt} \left(\x^{\alpha}\right)^{\ell}
\right)
\cdot \left(\ex_{\z \sim \Dtgt} \left[\z^{\alpha} \right]\right)^{2 \log(1/\delta)-\ell}
\\
 &\leq \sum_{\ell=0}^{2\log(1/\delta)} (K_4 \ell k)^{\ell k/2} (K_4 k)^{k(2\log(1/\delta)-\ell)/2} \\
 &\leq 
 2\log(1/\delta) (2K_4\log(1/\delta)k)^{\log(1/\delta)k}
\end{align*}
This, together with Markov's inequality implies that 
\[
\pr
\left[
\left\lvert\frac{1}{|S|}\sum_{\x \in S} \x^{\alpha}
        -
        \ex_{\x \sim \Dtgt} \left[\x^{\alpha}\right]
       \right\rvert > \frac{1}{d^k}
\right]
\leq
\left( 
\frac{d^k (3K_4 k\log(1/\delta))^{k/2}}{|S|}
\right)^{2\log(1/\delta)}
\]
Since $S$ is obtained by taking at least $|S| \geq \left( d^k, \left(\log \frac{1}{\delta}\right)^k \right)^{C_1}$, for sufficiently large $C_1$ we see that the above is upper-bounded by $\frac{1}{d^k} \delta$. Taking a union bound over all $\alpha \in \Z_{\geq 0}^d$ with $| \alpha|=k$, we see that with probability at least $1-\delta$ the tester $\testerone$ accepts, finishing the proof.
\subsection{Proof of Proposition~\ref{tester2_band}}
Let $K_1$ be as in part (d) of Proposition \ref{prop:slc}.
    The tester ${\testertwo}$ computes the fraction of elements in $S$ that are in $T$. If this fraction is $K_1\sigma/2$-close to $ \pr_{\x \sim \Dtgt}[|\inn{\w, \x}| \leq \sigma] $, the algorithm accepts. The algorithm rejects otherwise. 
    Now, from (d) of Proposition \ref{prop:slc} we have that $\pr_{\x \sim \Dtgt}[|\inn{\w, \x}| \leq \sigma]\in [K_1 \sigma, K_2 \sigma]$. Therefore, if the fraction of elements in $S$ that belong in $T$ is $K_1\sigma/100$-close to $\pr_{\x \sim \Dtgt}[|\inn{\w, \x}| \leq \sigma]$, then this quantity is in $[K_1 \sigma/2, (K_2+K_1/2) \sigma]$ as required.
    Finally, if $|S|\geq \frac{100}{K_1 \sigma^2} \log \left(\frac{1}{\delta}\right)$ by standard Hoeffding bound, with probability at least $1-\delta$ we indeed have that the fraction of elements in $S$ that are in $T$ is $K_1\sigma/2$-close to $ \pr_{\x \sim \Dtgt}[|\inn{\w, \x}| \leq \sigma]$.
\subsection{Proof of Proposition~\ref{tester3_truncated}}
The tester ${\testerthree}$ does the following:
    \begin{enumerate}
     \item Runs the tester ${\testertwo}$ from Proposition \ref{tester2_band}. If ${\testertwo}$ rejects, ${\testerthree}$ rejects as well. 
     \item Let $S_{|T}$ be the set of elements in $S$ for which $\x \in T$.
     \item Let $k=\tilde{O}(1/\tau^2)$ be chosen as in Proposition \ref{prop: moments in band close means we good}.
     \item For all $\alpha \in \Z_{\geq 0}^d$ with $|\alpha|=k$:
    \begin{enumerate}
        \item Compute the corresponding moment $\ex_{(\x,y) \sim \Dgeneric}[\x^{\alpha} \mid \x\in T]:=\frac{1}{|S_{|T}|}\sum_{\x \in S_{|T}} \x^{\alpha}$.
        \item If $\left\lvert\ex_{(\x,y) \sim \Dgeneric}[\x^{\alpha} \mid \x\in T]
        -
        \ex_{\x \sim \Dtgt}[\x^{\alpha} \mid \x\in T]
       \right\rvert > \frac{\tau}{d^k} \cdot d^{-\tilde{O}(k)}$ then reject, where the polylogarithmic in $d^{-\tilde{O}(k)}$ is chosen to satisfy the additive slack condition in Proposition \ref{prop: moments in band close means we good}. 
    \end{enumerate}
    \item If all the checks above passed, accept.
    \end{enumerate}
First, we argue that if the checks above pass, then Equations \ref{equation:property3_fool_orthogonal_halfspaces_truncated} and \ref{equation:property4_fool_variance_truncated} will hold. If the tester passes, Equation \ref{equation:property3_fool_orthogonal_halfspaces_truncated} follows immediately from the guarantees in step (4b) of ${\testerthree}$ together with Proposition \ref{prop: moments in band close means we good}. 
Equation \ref{equation:property4_fool_variance_truncated}, in turn, is proven as follows:
\begin{align*}
\left\lvert
\ex_{(\x,y) \sim \Dgeneric}[(\inn{\vv, \x})^2] -\ex_{(\x,y) \sim \Dtgt}[(\inn{\vv, \x})^2] 
\right\rvert 
&\leq
\sum_{\alpha : |\alpha| = 2}
\left\lvert{\ex_{(\x,y) \sim \Dgeneric} [\x^{\alpha}]}
        -
        \ex_{\x \sim \Dtgt} \left[\x^{\alpha}\right]
       \right\rvert 
       \\
       &\leq
       d^2 \cdot \max_{\alpha : |\alpha| = 2}
\left\lvert{\ex_{(\x,y) \sim \Dgeneric} [\x^{\alpha}]}
        -
        \ex_{\x \sim \Dtgt} \left[\x^{\alpha}\right]
       \right\rvert 
       \leq \tau
\end{align*}
Now, we need to show that if the elements of $S$ are chosen i.i.d. from $\Dtgt$, and $|S|\geq ...$ then the tester above accepts with probability at least $1-\delta$. Consider any specific mult-index $\alpha\in \Z_{\geq 0}^d$ with $|\alpha|=k$. Now, by Proposition \ref{prop:slc}(f) we have for any positive integer $\ell$ the following:
\[
\ex_{\x \sim \Dtgt} \left[ \left\lvert
 \left(\x^{\alpha}\right)^{\ell}
\right\rvert\right]
\leq 
(K_4 \ell k)^{k/2}
\]
But by Proposition \ref{prop:slc}(d) we have that $\pr_{\x \sim \Dtgt}[\x \in T]=\pr_{\x \sim \Dtgt}[|\inn{\x, \w} | \leq \sigma] \geq K_1 \sigma$. Therefore, the density of the distribution $\Dtgt_{|T}$ (which is defined as the distribution one obtains by taking $\Dtgt$ and conditioning on $\x \in T$) is upper bounded by the product of the density of the distribution $\Dtgt$ and $\frac{1}{K_1 \sigma}$. This allows us to bound 
\[
\ex_{\x \sim \Dtgt} \left[ \left\lvert
 \left(\x^{\alpha}\right)^{\ell}
\right\rvert
\mid \x \in T
\right]
\leq
\frac{1}{K_1\sigma}
\ex_{\x \sim \Dtgt} \left[ \left\lvert
 \left(\x^{\alpha}\right)^{\ell}
\right\rvert\right]
\leq 
\frac{(K_4 \ell k)^{k/2}}
{K_1\sigma}
\]
This implies that
\begin{align*}
&\ex_{\x \sim \Dtgt} \left[\left(\x^{\alpha}- \ex_{\z \sim \Dtgt} \left[\z^{\alpha}
\mid \z \in T
\right]\right)^{2\log(1/\delta)}\mid \x \in T\right]
\\
&\leq
\sum_{\ell=0}^{2\log(1/\delta)}
\left(
\ex_{\x \sim \Dtgt} \left[\left(\x^{\alpha}\right)^{\ell} \mid \x \in T\right]
\right)
\cdot \left(\ex_{\x \sim \Dtgt} \left[\left(\x^{\alpha} \mid \x \in T\right] \right)\right)^{2 \log(1/\delta)-\ell}
\\
&\leq
 \frac{1}{(K_1 \sigma)^{2\log(1/\delta)}} \sum_{\ell=0}^{2\log(1/\delta)} (K_4 \ell k)^{\ell k/2} (K_4 k)^{k(2\log(1/\delta)-\ell)/2}
 \\  
 &\leq
 \frac{1}{(K_1 \sigma)^{2\log(1/\delta)}}
 2\log(1/\delta) (2K_4\log(1/\delta)k)^{\log(1/\delta)k}
\end{align*}
This, together with Markov's inequality implies that 
\[
\pr
\left[
\left\lvert\frac{1}{|S|}\sum_{\x \in S} \x^{\alpha}
        -
        \ex_{\x \sim \Dtgt} \left[\x^{\alpha}\right]
       \right\rvert > \frac{\tau}{d^k} \cdot d^{-\tilde{O}(k)}
\right]
\leq
\left( 
\frac{d^{\tilde{O}(k)} (3K_4 k\log(1/\delta))^{k/2}}{K_1 \sigma |S_{|T}| \tau }
\right)^{2\log(1/\delta)}
\]
Now, recall that the tester $\testertwo$ in step (1) accepted, we have $|S_{|T}| \geq \frac{1}{C_2 \sigma } |S|$.
Since $S$ is obtained by taking at least $|S| \geq \left( \frac{1}{\tau} \cdot \frac{1}{\sigma } \cdot d^{\frac{1}{\tau^2} \log^{C_5} \left( \frac{1}{\tau} \right)} \cdot 
\left(\log \frac{1}{\delta}\right)^{\frac{1}{\tau^2} \log^{C_5} \left( \frac{1}{\tau} \right)}  \right)^{C_5}$, for sufficiently large $C_5$ we see that the expression above is upper-bounded by $\frac{1}{d^k} \delta$. Taking a union bound over all $\alpha \in \Z_{\geq 0}^d$ with $| \alpha|=k$, we see that with probability at least $1-\delta$ the tester $\testerthree$ accepts, finishing the proof.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
[width=0.6\textwidth]{regions}
"
78,"\importpackages{}
\graphicspath{ {./images/} }


\title{Transformer-based Generative Adversarial Networks in Computer Vision: A Comprehensive Survey}
\author{
Shiv Ram Dubey, \IEEEmembership{Senior Member,~IEEE}, Satish Kumar Singh, \IEEEmembership{Senior Member,~IEEE}
\thanks{S.R. Dubey and S.K. Singh are with the Computer Vision and Biometrics Laboratory, Indian Institute of Information Technology, Allahabad, Prayagraj, Uttar Pradesh-211015, India (e-mail: srdubey@iiita.ac.in, sk.singh@iiita.ac.in). }
}
\markboth{Transformer-based GANs in Computer Vision: A Comprehensive Survey}
 {Dubey \MakeLowercase{\textit{}}: Bare Demo of IEEEtran.cls for Journals}
\maketitle
\begin{abstract}
Generative Adversarial Networks (GANs) have been very successful for synthesizing the images in a given dataset. The artificially generated images by GANs are very realistic. The GANs have shown potential usability in several computer vision applications, including image generation, image-to-image translation, video synthesis, and others. Conventionally, the generator network is the backbone of GANs, which generates the samples and the discriminator network is used to facilitate the training of the generator network. The discriminator network is usually a Convolutional Neural Network (CNN). Whereas, the generator network is usually either an Up-CNN for image generation or an Encoder-Decoder network for image-to-image translation. The convolution-based networks exploit the local relationship in a layer, which requires the deep networks to extract the abstract features. Hence, CNNs suffer to exploit the global relationship in the feature space. However, recently developed Transformer networks are able to exploit the global relationship at every layer. The Transformer networks have shown tremendous performance improvement for several problems in computer vision. Motivated from the success of Transformer networks and GANs, recent works have tried to exploit the Transformers in GAN framework for the image/video synthesis. This paper presents a comprehensive survey on the developments and advancements in GANs utilizing the Transformer networks for computer vision applications. The performance comparison for several applications on benchmark datasets is also performed and analyzed. The conducted survey will be very useful to deep learning and computer vision community to understand the research trends \& gaps related with Transformer-based GANs and to develop the advanced GAN architectures by exploiting the global and local relationships for different applications. 
\end{abstract}
\begin{IEEEkeywords}
Transformer Network; Generative Adversarial Networks, Deep Learning; Survey; Image and Video Synthesis.
\end{IEEEkeywords}
\section{Introduction}\label{Introduction}
\IEEEPARstart{G}{enerative} Adversarial Network (GAN) was introduced by Goodfellow et al. \cite{gan} in 2014 to synthesize images in a given distribution. GAN consists of two neural networks, namely Generator ($G$) and Discriminator ($D$) as shown in Fig. \ref{fig:gan_conceptual}. These networks are trained jointly in an adversarial manner. The generator network outputs a synthesized image from a random latent vector as input. Whereas, the discriminator network classifies the generated images in fake category and actual images in real category. Since its inception, several variants of GANs have been introduced to synthesize high quality images, such as Deep Convolutional GAN (DCGAN) \cite{dcgan}, Wasserstein GAN (WGAN) \cite{wgan}, Least Square GAN (LSGAN) \cite{lsgan}, ProgessiveGAN \cite{progressivegan}, StyleGAN \cite{staylegan}, \cite{karras2020analyzing}, DR-GAN \cite{tan2022dr}, DHI-GAN \cite{lin2022dhi}, ZeRGAN \cite{diao2022zergan}, Tensorizing GAN \cite{yu2021tensorizing}, etc.
Image-to-image translation is one of the major applications of GANs. The early work includes Conditional GAN \cite{conditional_gan} based Pix2pix model \cite{pix2pix} which takes images in a given domain as input and generates images in the target domain as output. In order to do so, the Pix2pix model modifies the generator network of GAN and uses the Encoder-Decoder framework, where the Encoder network is a CNN and the Decoder network is an Up-CNN. Pix2pix makes use of a pixel error to train the model along with the adversarial training, hence requires the paired dataset. To overcome this issue, CycleGAN \cite{cyclegan} uses a cyclic consistency loss that works on the unpaired datasets. Several variants of GAN have been proposed for image-to-image translation, such as PCSGAN \cite{pcsgan}, MUNIT \cite{munit}, CUT \cite{CUT}, CouncilGAN \cite{councilgan}, MobileAR-GAN \cite{mobileargan}, etc.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
A huge progress in GAN models, its objective functions and training strategies has been observed, such as an overview on the working of GAN model and its variants is presented in \cite{hong2019generative}, the challenges in GAN models with possible solutions are summarized in \cite{saxena2021generative}, a survey on GAN's variants, applications, and training is performed in \cite{jabbar2021survey}, and a review on GAN's algorithms, theory, and applications in conducted by Gui et al. \cite{gui2021review}.
The GAN-based models have been very successful for several applications as indicated by different surveys/reviews on GANs, such as computer vision \cite{wang2021generative}, image-to-image translation \cite{pang2021image}, face generation \cite{toshpulatov2021generative}, \cite{kammoun2022generative}, medical image analysis \cite{alamir2022role}, \cite{yi2019generative}, video generation \cite{aldausari2022video}, spatio-temporal data analysis \cite{gao2022generative}, and text-to-image synthesis \cite{zhou2021survey}.
In recent years, Transformer networks \cite{vaswani2017attention} have received lots of attention due to its outstanding performance. Transformer network is based on multi-head self-attention modules, which capture the global relationship in the data while transforming the input feature to output feature. Motivated from the success of Transformer networks, several variants have been introduced, such as Bidirectional Transformers (BERT) \cite{bert}, Generative Pre-Training Transformer (GPT) \cite{gpt3}, Swin Transformer \cite{swin_transformer}, Vision Transformer (ViT) \cite{vit}, Multitask ViT \cite{tian2023end}, Tokens-to-token ViT \cite{token_vit}, Video Transformer \cite{alfasly2022effective}, HSI Denoising Transformer (Hider) \cite{chen2022hider}, Word2Pix Transformer \cite{zhao2022word2pix}, and many more.
Transformer networks have been very successful for different problems of natural language processing and computer vision, as indicated by several survey papers. A survey of efficient transformer models is presented in \cite{tay2022efficient}. A review of BERT-based transformer models for text-based emotion detection is conducted by Acheampong et al.  \cite{acheampong2021transformer}. Kalyan et al. present a survey of transformer-based biomedical pretrained language models \cite{kalyan2021ammu}. The potential of transformers is also witnessed in vision \cite{khan2022transformers}, \cite{han2022survey}. Shin et al. highlights the transformer architectures for cross-modal vision and language tasks in terms of different perspectives and prospects \cite{shin2022perspectives}.
Motivated from the wider acceptance of GAN and Transformer models, recently, some researchers have proposed the Transformer-based GAN models for image generative tasks \cite{transgan}, \cite{transformer_attngan}, \cite{swingan}, \cite{vitgan}, \cite{uvcgan}. The recently developed Transformer-based GAN models have shown very promising performance for different applications of image and video synthesis. The contributions of this paper are as follows:
\begin{itemize}
    \item As per our best knowledge, no survey paper exists on GAN models utilizing Transformer networks. Hence, this paper provides a comprehensive survey on the developments in Transformer-based GANs for computer vision applications.
    \item The categorization of models is performed for different image and video applications, such as image generation, image translation, image inpainting, image restoration, image reconstruction, image enhancement, image super-resolution, image colorization, video generation, video inpainting, video translation, etc.
    \item The comparison of models is also performed in terms of the generator architecture, discriminator architecture, loss functions and datasets used for different image and video applications.
    \item The experimental results comparison and analysis is also conducted for different applications using standard metrics on benchmark datasets to provide the status of state-of-the-art Transformer-based GANs.
    \item The research trends and potential future directions are also narrated with a very concise view to benefit the researchers working on deep learning and computer vision problems.
\end{itemize}
Section \ref{background} presents a brief of Generative Adversarial Network and Transformer Network. Sections III-VI are devoted to transformer-based GAN models for image generation, image-to-image translation, video applications and miscellaneous applications, respectively. Section VII provides the conclusion and future directions. 
\section{Background} \label{background}
In this section, the background of GANs and Transformers is presented. 
\subsection{Generative Adversarial Network}
Generative Adversarial Network (GAN) was introduced by Goodfellow et al. \cite{gan} in 2014 for image generation. A conceptual representation of GAN is depicted in Fig. \ref{fig:gan_conceptual}. The generator network ($G$) in GAN synthesizes a new sample ($G(z)$) from a random latent vector/noise ($z$), i.e., $G: z \rightarrow G(z)$, where $z \in \mathbb{R}^d$ is sampled from a uniform probability distribution $p_v$ and $d$ is the dimensionality of $z$. If the probability distribution of the generated samples ($G(z)$) and real samples ($x_{data}$) are given by $p_{model}$ and $p_{data}$, respectively, then we want to learn $p_{model}$ that matches $p_{data}$. The training of the generator network is facilitated by the discriminator network ($D$). The purpose of the discriminator network is to distinguish the generated samples from the real samples. The output of the discriminator network is considered as the probability of real samples. Hence, the discriminator network tries to produce $D(x_{data}) \approx 1$, $\forall x_{data} \sim{p_{data}}$ and $D(G(z)) \approx 0$, $\forall z \sim{p_v}$, where $p_{data}$ represents the probability distribution of real samples. However, at the same time, the generator network tries to fool the discriminator network and to achieve $D(G(z)) \approx 1$, $\forall z \sim{p_v}$. It makes training of the GAN model as a min-max optimization. The objective function of GAN is given as,
\begin{equation}
\begin{aligned}
    {\mathcal{L}}_{GAN}(G, D) = 
    & Min_{G}Max_{D}(  \mathbb{E}_{x_{data}\sim{p_{data}}} [\log D(x_{data})] + \\
    & \mathbb{E}_{z\sim{p_v}}[\log(1-D(G(z)))] )
\end{aligned}
\end{equation}
where ${\mathcal{L}}_{GAN}(G, D)$ is the adversarial loss function.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
Considering unlimited training data and unlimited capacity for generator and discriminator:
\begin{itemize}
    \item The objective ${\mathcal{L}}_{GAN}(G, D)$ is equivalent to the Jensen-Shannon divergence between $p_{data}$ and $p_{model}$ and global optimum (Nash equilibrium) is given by $p_{data}$ and $p_{model}$.
    \item If at each step, $D$ is allowed to reach its optimum given $G$, and $G$ is updated to decrease ${\mathcal{L}}_{GAN}(G, D)$, then $p_{model}$ will eventually converge to $p_{data}$.
\end{itemize}
The discriminator network is usually a Convolutional Neural Network (CNN) and works as a binary classifier. The last layer of discriminator network is a Sigmoid function which produces the output as the probability for real class. The discriminator network is only required for training purpose. The generator network is usually an Up-CNN which can take the input having low dimensionality and produce the output with high dimensionality. The generator network architecture is suitably modified for different type of data and applications, such as encoder-decoder based networks are used for image-to-image translation \cite{pang2021image} and RNN-CNN based networks are used for text-to-image synthesis \cite{zhou2021survey}.
\subsection{Transformer Network}
The Transformer network utilizes a self-attention mechanism introduced by Vaswani et al. in 2017 \cite{vaswani2017attention}. Originally, the Transformer was proposed for a machine translation task where the Encoder and Decoder networks are built using Transformers as shown in Fig. \ref{fig:transformer} (\textit{left}). First, the feature embedding from input is computed and combined with the positional embedding to generate the input feature embedding of $k$ dimensionality for Transformer. A Transformer block in Encoder consists of a multi-head attention module followed by a feed forward module having skip connection and normalization. However, a Transformer block in Decoder includes an additional masked multi-head attention module. Basically, a Transformer block transforms an input feature embedding ($u \in \mathbb{R}^k$) into an output feature embedding ($v \in \mathbb{R}^k$). The Transformer block is repeated $N\times$ in Encoder and Decoder networks. The multi-head attention module is illustrated in Fig. \ref{fig:transformer} (\textit{right, top}). It is basically the concatenation of the output of several independent scaled dot-product attention followed by a linear layer. The use of multiple attention heads facilitates the extraction of features of different important regions or characteristics. The scaled dot-product attention mechanism is depicted in Fig. \ref{fig:transformer} (\textit{right, bottom}). The input ($u$) to a Transformer block is projected into the Query ($Q$), Key ($K$), and Value ($V$) using linear layers having weights $W_Q$, $W_K$, and $W_V$, respectively. The output of the scaled dot-product attention module is computed as,
\begin{equation}
\text{Attention}(Q,K,V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}
where $d_k$ is the dimensionality of $Q$ and $K$. The masking in scaled dot-product attention module is used only in Decoder network. The feed forward module consists of a linear layer followed by the ReLU activation function followed by another linear layer. 
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
Vision Transformer (ViT) \cite{vit} is a variant of the transformer network which facilitates to utilize it for the image input. Basically, it divides the images into patches. The feature embeddings extracted from these patches are used as the input to Transformer network as depicted in Fig. \ref{fig:vit}. Transformer Encoder consists of $N$ Transformer blocks. An additional token for class embedding is added in ViT which is used as the input to the Multi-layer Perceptron (MLP) head to generate the class scores for different classes. Though Transformer and ViT networks were originally proposed for the machine translation and image classification tasks, respectively, they are very heavily utilized for different problems of computer vision in recent years. In this paper, we provide the advancements in GAN models using Transformer networks for image and video synthesis and analyze from different perspectives.
\begin{table*}[!t]
\caption{A summary of Transformer-based GANs for image generation.}
\centering
\begin{tabular}{p{0.23\columnwidth}|p{0.15\columnwidth}|p{0.4\columnwidth}|p{0.32\columnwidth}|p{0.34\columnwidth}|p{0.32\columnwidth}}
    \hline
    \textbf{Model} & \textbf{Venue} & \textbf{Generator} & \textbf{Discriminator} & \textbf{Objective Function} & \textbf{Datasets} \\
    \hline
    GANsformer \cite{gansformer} & ICML'21 & Bipartite Transformer having simplex and duplex attention & Attention CNN-based discriminator  & Loss functions of StyleGAN & CLEVR \\
    \hline
    GANformer2 \cite{ganformer2} & NeurIPS'21 & Generator works in two stages: layout generation and layout to scene translation & One CNN for real vs. fake and one U-Net for semantic matching & Adversarial loss, Semantic-matching loss and Segment-fidelity loss & CLEVR, Bedrooms, CelebA, Cityscapes and COCO\\
    \hline
    TransGAN \cite{transgan} & NeurIPS'21 & A Transformer-based generator that progressively increases feature resolution & Transformer-based discriminator that takes input at multiple scales & WGAN-GP loss & CIFAR-10, STL-10, CelebA, CelebA-HQ and LSUN Church \\
    \hline
    HiT \cite{hit} & NeurIPS'21 & Multi-Axis Nested Transformer at low-resolution and Implicit Functions at high-resolution & ResNet-based discriminator & Non-saturating logistic GAN loss, R1 gradient penalty to only discriminator & ImageNet, CelebA-HQ and FFHQ\\
    \hline
    TokenGAN \cite{tokengan} & NeurIPS'21 & Visual Transformer with content and style tokens & Discriminator of StyleGAN2 & Non-saturating logistic adversarial loss, R1 regularization to only discriminator & FFHQ and LSUN Church \\
    \hline
    VQGAN \cite{vqgan} & CVPR'21 & CNN-based image constituents vocabulary and Transformer-based modeling of vocabulary composition within high-resolution image & CNN-based discriminator & Adversarial loss, reconstruction loss, commitment loss and perceptual reconstruction loss & ImageNet, ImageNet-Animal, LSUN Churches \& Towers, COCO-Stuff, ADE20K, CelebA-HQ and FFHQ \\
    \hline
    Styleformer \cite{styleformer} & CVPR'22 & Transformer with Styleformer Encoders having Increased Multi-Head Self-Attention & Discriminator of StyleGAN2-ADA & Losses of StyleGAN2-ADA &  CIFAR-10, STL-10, CelebA, LSUN-Church, CLEVR and Cityscapes \\
    \hline
    StyleSwin \cite{styleswin} & CVPR'22 & Style-based GAN with Transformer having double attention modules & Wavelet-based discriminator &  Non-saturating GAN loss with R1 gradient penalty and spectral normalization on the discriminator &  FFHQ, CelebA-HQ and LSUN Church\\
    \hline
    ViTGAN \cite{vitgan} & ICLR'22 & ViT-based ordered patch generator & ViT-based discriminator & Non-saturating logistic adversarial loss & CIFAR-10, CelebA and LSUN Bedroom \\
    \hline
    Unleashing Transformer \cite{unleashing_transformer} & ECCV'22 & Trained Transformer using Masked Vector-Quantized tokens prediction & Traditional discriminator & Vector-Quantized loss, generator loss and reconstruction loss & FFHQ, LSUN Bedroom and LSUN Churches \\ \hline
    Swin-GAN \cite{swin_gan_tvc} & TVC'22 & Swin Transformer-based generator & Swin Transformer-based multi-scale discriminator & WGAN-GP loss & CIFAR-10 and Anime images \\ \hline
    PGTCEGAN \cite{pgtcegan} & SMC'22 & Capsule Embedding based Progressive Growing Transformer & CNN with multi-scale input in different layer & WGAN-GP loss & CIFAR-10, CelebA and LSUN-Church  \\ \hline
    MedViTGAN \cite{medvitgan} & ICPR'22 & ViT Encoder-based generator & ViT Encoder-based discriminator in conditional GAN setup & WGAN-GP loss with adaptive hybrid loss weighting mechanism & Histopathology image dataset: PatchCamelyon (PCam) and BreakHis \\ \hline
    PTNet3D \cite{ptnet3d} & IEEE-TMI'22 & U-shape generator with performer encoder, transformer bottleneck and performer decoder & 3D ResNet-18 model pretrained on Kinetics-400 dataset & Adversarial loss, Perceptual loss and Mean square error & MRI datasets: Developing Human Connectome Project (dHCP) and longitudinal Baby Connectome Project (BCP) \\ \hline
    SLATER \cite{slater} & IEEE-TMI'22 & Generator uses cross-attention transformers with input from a mapper & CNN-based discriminator & Non-saturating logistic adversarial loss, gradient penalty for discriminator & MRI synthesis: brain MRI data from fastMRI\\ \hline
    SwinGAN \cite{swingan} & CBM'23 & Swin Transformer U-Net-based frequency-domain and image-domain generators & CNN-based discriminator & Adversarial loss, k-space loss and image domain loss & MRI reconstruction: IXI brain dataset \\ \hline
    3D Face Transformer \cite{transformer_3d_face_reconstruction} & IEEE-TCSVT'22 & Residual blocks followed by a multi-layer transformer encoder-based generator & Traditional discriminator & Adversarial loss, L1 loss, Edge loss, L1 loss on the transformer outputs and Self-supervised reprojection consistency loss & 3D Face reconstruction: 300W-LP, AFLW, AFLW2000-3D, NoW, In-the-wild images \\ \hline
\end{tabular}
\label{tab:image_generation}
\end{table*}
\section{Transformer-based GANs for Image Generation}
The image generation has been very important application of GANs. Several improvements in GAN have been validated for this task. The researchers have also exploited the Transformer-based GANs for image generation of different types, such as objects, scenes, medical, etc. A summary of the different models is presented in Table \ref{tab:image_generation} in terms of the generator, discriminator, losses and datasets.
\subsection{Image Generation}
In the initial attempts, Transformers are utilized in an auto-regressive manner \cite{gpp}, \cite{vqgan} for the image generation. However, these methods are not very efficient in terms of the inference speed.
Jiang et al. in 2021 conducted the initial study on a pure Transformer-based GAN, dubbed TransGAN \cite{transgan}, which is completely free of convolution operations. TransGAN contains a Transformer-based generator network that increases the resolution of features in a progressive manner and a Transformer-based discriminator network. Both the generator and discriminator networks utilize the grid-transformer blocks. TransGAN achieve Fréchet Inception Distance (FID) of 9.26 on CIFAR-10 dataset \cite{cifar}.
Hudson and Zitnick \cite{gansformer} proposed generative adversarial transformer (GANsformer) for image generation. GANsformer makes use of the bipartite transformer which is a stack of alternative simplex or duplex attention modules. The results of GANsformer are demonstrated on four benchmark datasets. GANsformer is further extended to GANformer2 by incorporating explicit and strong structural priors \cite{ganformer2}.
Researchers have experienced that training GANs with Transformers is challenging. Xu et al. \cite{stransg} proposed STrans-G generator network and STrans-D discriminator network for image generation by studying the characteristics of Transformer in GAN framework. STrans-G is CNN free network. It is noticed in \cite{stransg} that the residual connections in self-attention layers is not good for Transformer-based discriminators and conditional generators. Zeng et al. \cite{tokengan} proposed a TokenGAN which exploits a token-based transformer generator to assign the styles to the content tokens through attention mechanism for controlling the image synthesis. TokenGAN generates the high-fidelity images at high-resolution (i.e., 1024 × 1024 size).
Zhao et al. \cite{hit} proposed an efficient high-resolution image transformer (HiT) based GAN model for image generation. HiT exploits the multi-axis blocked self-attention module at low-resolution synthesis and removes the costly global self-attention module at high-resolution synthesis. In an another study, the Transformer-based generator network is utilized with Convolution-based discriminator by Durall et al. \cite{transG_convD} for image generation. The experimental results using the generator of TransGAN and discriminator of SNGAN in \cite{transG_convD} show an improved FID of 8.95 on the CIFAR-10 dataset. Park and Kim \cite{styleformer} introduced the Styleformer model which contains the Transformer structure-based generator network for synthesizing the images using style vectors. An attention style injection module is introduced in Styleformer for the modulation and demodulation of style with self-attention. Styleformer shows an outstanding performance for image generation on CIFAR-10 dataset with an FID score of 2.82. Recently, in 2022, a StyleSwin model is proposed by Zhang et al. \cite{styleswin} for image generation. StyleSwin is a  Swin Transformer-based GAN model in a style-based architecture for high-resolution synthesis. In order to exploit the local and shifted window contexts, StyleSwin works based on a double attention mechanism. The transformer is also exploited as the generator network with conditional GAN for image generation by Xi et al. \cite{transformer_cgan}. An unconstrained Transformer architecture is used as the backbone in \cite{unleashing_transformer} which performs the parallel prediction of Vector-Quantized tokens and achieves the competitive FID scores on benchmark datasets. 
A progressive growing transformer with capsule embedding GAN (PGTCEGAN) is introduced by Jiang et al. \cite{pgtcegan}. The generator network generates the images gradually using the transformer. The capsule encoder is utilized to generate the positional embedding. Wang et al. proposed a Swin-GAN by exploiting the shifted window attention mechanism-based transformer to gradually increase the resolution \cite{swin_gan_tvc}. Basically, the generator and discriminator networks of Swin-GAN use the Swin Transformer Blocks. The discriminator in Swin-GAN receives the input by dividing the image into patches of different sizes. Relative position coding, layer normalization and data enhancement are also exploited by Swin-GAN to improve the stability.
Lee et al. utilized the vision transformer in GAN framework and proposed ViTGAN \cite{vitgan}. It is noticed that the training of ViT discriminator is unstable due to the self-attention module not able to cope with the existing GAN regularization methods. ViTGAN exploits different regularization techniques to tackle this problem, such as the Lipschitzness of Transformer discriminator, improved spectral normalization, overlapping image patches and convolutional projection. By carefully designing the ViT generator, ViTGAN is able to converge with comparable performance to the leading CNN-based GAN models for image generation. It is also observed that by using the generator network of ViTGAN (ViTGAN-G) and a discriminator network of (StyleGAN2-D), the performance of the image generation task is improved. A HybridViT model is introduced in \cite{hybridvit} by integrating the ViT architecture into diffusion denoising probability models. HybridViT model is utilized for joint image generation and classification.
\begin{table}[!t]
    \caption{The results comparison of Transformer-based GANs for image generation in terms of FID score. \textbf{Best} and \textit{second best} results are highlighted in \textbf{bold} and \textit{italic}, respectively, even in other tables also.}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{p{0.23\columnwidth} p{0.13\columnwidth} p{0.1\columnwidth} p{0.08\columnwidth} p{0.08\columnwidth} p{0.1\columnwidth} p{0.1\columnwidth}}
    \hline
    Method & Venue & CIFAR10 ($32^2$) & STL10 ($48^2$) & CelebA ($64^2$) & ImageNet ($128^2$) \\
    \hline
    TransGAN \cite{transgan} & NeurIPS'21 & 9.26 & \textit{18.28} & 5.01 & - \\
    HiT \cite{hit} & NeurIPS'21 & - & - & - & \textit{30.83}\\
    STrans-G \cite{stransg} & arXiv'21 & \textbf{2.77} & - & \textbf{2.03} & \textbf{12.12} \\
    Styleformer \cite{styleformer} & CVPR'22 & \textit{2.82} & \textbf{15.17} & 3.92 & - \\
    PGTCEGAN \cite{pgtcegan} & SMC'22 & 8.43 & - & \textit{3.59} & - \\
    Swin-GAN \cite{swin_gan_tvc} & TVC'22 & 9.23 & - & - & - \\
    ViTGAN \cite{vitgan} & ICLR'22 & 4.92 & - & 3.74 & - \\
    \hline
    \end{tabular} }
    \label{tab:results_image_generation}
\end{table}
\subsection{Medical Image Synthesis}
The image synthesis in medical domain is a very demanding application of GANs. The synthetic medical images are generally useful for the data augmentation to tackle the requirement of large-scale datasets of deep learning models. Due to the improved quality of generated images, the Transformer-based GANs have also been extensively utilized for medical image synthesis of different modality.
In 2021, Korkmaz et al. \cite{gvtrans} proposed a generative vision transformer-based GVTrans model for synthesizing the MRI data from noise variables and latent space. GVTrans model is tested for unsupervised MRI reconstruction with promising performance. In 2022, Zhang et al. introduced a PTNet3D model \cite{ptnet3d} which uses the pyramid transformer network for the synthesis of 3D high-resolution longitudinal infant brain MRI data. PTNet3D model makes use of attention in transformer and performer layers. The synthesis accuracy of PTNet3D is superior with better generalization capability. The corrupted scan replaced by synthesized image through PTNet3D leads to better infant whole brain segmentation. A multi-contrast multi-scale transformer (MMT) is proposed by Liu et al. \cite{mmt} for MRI missing data imputation. In order to capture inter- and intra-contrast dependencies, MMT exploits the multi-contrast Swin transformer blocks. MMT is tested for image synthesis on two multi-contrast MRI datasets with appealing performance. 
In 2022, Li et al. proposed MedViTGAN for histopathological image synthesis for data augmentation \cite{medvitgan}. MedViTGAN is a vision transformer-based conditional GAN where both the generator and discriminator networks are designed based on the transformer encoder module. The performance of MedViTGAN is highly improved as compared to the CNN models, such as ResNet and DenseNet. Li et al. \cite{hvitgan} further extended the MedViTGAN using an auxiliary discriminator head for classification. Korkmaz et al. \cite{slater} proposed a zero-Shot Learned Adversarial TransformERs (SLATER) for unsupervised MRI reconstruction. SLATER utilizes cross-attention transformers with the deep adversarial network to project latent variables and noise into coil-combined MR images. 
\begin{table}[!t]
    \caption{The results comparison of Transformer-based GANs for high-resolution image generation in terms of FID score. Here, HiT-B refers to HiT with large model capacity.}
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{p{0.235\columnwidth} p{0.13\columnwidth} p{0.07\columnwidth} p{0.075\columnwidth} p{0.07\columnwidth} p{0.07\columnwidth} p{0.08\columnwidth}}
    \hline
    Method & Venue & FFHQ ($256^2$) & CelebA-HQ ($256^2$) & LSUN ($256^2$) & FFHQ ($1024^2$) & CelebA-HQ ($1024^2$)\\
    \hline
    VQGAN \cite{vqgan} & CVPR'21 & 11.40 & 10.70 & - & - & -\\
    TransGAN \cite{transgan} & NeurIPS'21 & - & 9.60 & 8.94 & - & - \\
    GANsformer \cite{gansformer} & ICML'21 & 7.42 & - & 6.51 & - & - \\
    GANformer2 \cite{gansformer} & NeurIPS'21 & 7.77 & - & 6.05 & - & - \\
    TokenGAN \cite{tokengan} & NeurIPS'21 & 5.41 & - & 5.56 & - & - \\
    STrans-G \cite{stransg} & arXiv'21 & 4.84 & - & - & - & - \\
    HiT-B \cite{hit} & NeurIPS'21 & \textit{2.95} & \textit{3.39} & - & \textit{6.37} & \textit{8.83}\\
    PGTCEGAN \cite{pgtcegan} & SMC'22 & - & - & \textit{3.92} & - & - \\
    StyleSwin \cite{styleswin} & CVPR'22 & \textbf{2.81} & \textbf{3.25} & \textbf{2.95} & \textbf{5.07} & \textbf{4.43} \\
    \hline
    \end{tabular}}
    \label{tab:results_hr_image_generation}
\end{table}
\subsection{Results Comparison and Analysis}
The results comparison of Transformer-based GANs is reported in Table \ref{tab:results_image_generation} and Table \ref{tab:results_hr_image_generation}. The Fréchet inception distance (FID) metric is compared to the different models on benchmark datasets. A lower FID score represents the better synthesized image quality. Table \ref{tab:results_image_generation} summarizes the FID scores on CIFAR10 \cite{cifar}, STL10 \cite{stl}, CelebA \cite{celeba} and ImageNet \cite{imagenet} datasets to generate the images of size $32\times32$, $48\times48$, $64\times64$ and $128\times128$, respectively. It is noticed that STrans-G \cite{stransg} provides best FID scores for CIFAR10, CelebA and ImageNet datasets. The Swin architecture at resolution more than $16\times16$ provides local attention in STrans-G which is beneficial for qualitative image generation. The performance of Styleformer is also promising with best on STL10 and second best on CIFAR10 dataset. The FID scores for high-resolution image generation are illustrated in Table \ref{tab:results_hr_image_generation} over Flickr-Faces-HQ (FFHQ) \cite{staylegan}, CelebA-HQ \cite{progressivegan} and LSUN Church \cite{lsun_church} datasets to generate the images of size $256\times256$. The results for generating the images of size $1024\times1024$ are also included over FFHQ and CelebA datasets. It is evident that StyleSwin \cite{styleswin} is the best performing model over all the datasets for high-resolution image synthesis as the capabilities of both StyleGAN and Swin transformer are inherited by StyleSwin. Moreover, the local-global positional encoding in StyleSwin is able to maintain a good trade-off between local and global context while generating the high-resolution images. The results of HiT \cite{hit} and PGTCEGAN \cite{pgtcegan} are also very promising.
\subsection{Summary}
Following is the summary from the survey of Transformer-based GANs for image generation:
\begin{itemize}
    \item The majority of the transformer-based GAN models use the transformer-based generator network and CNN-based discriminator network. However, few models also exploit the transformers in discriminator network.
    \item The uses of both local and global information are imnportant. Hence, several models try to exploit the CNN-based encoder and decoder modules along with transformer module in the generator network.
    \item ViT and Swin transformers are heavily exploited for the generator network. It is observed that ViT and Swin transformer are useful to preserve the local context.
    \item The objective function for different models generally includes Adversarial GAN loss and perceptual loss. Other losses such as L1 loss, gradient penalty and edge loss are also exploited by some models.
    \item The CIFAR10, STL10, CelebA, ImageNet, FFHQ, CelebA-HQ and LSUN Church are the most common benchmark datasets for image generation.
    \item The combination of StyleGAN with Swin transformer becomes very powerful to generate high-resolution images.
    \item The transformer-based GANs are very useful in tackling the data limitation problem in the medical domain by generating very realistic medical images.
\end{itemize}
\begin{table*}[!t]
\caption{A summary of Transformer-based GANs for image-to-image translation.}
\centering
\begin{tabular}{p{0.21\columnwidth}|p{0.15\columnwidth}|p{0.4\columnwidth}|p{0.25\columnwidth}|p{0.36\columnwidth}|p{0.4\columnwidth}}
    \hline
    \textbf{Model} & \textbf{Venue} & \textbf{Generator} & \textbf{Discriminator} & \textbf{Objective Function} & \textbf{Application \& Datasets} \\
    \hline
    InstaFormer \cite{instaformer} & CVPR'22 & Generator with ViT encoder blocks consisting of adaptive instance normalization & Traditional discriminator & Adversarial loss, Global and Instance-level content loss, Image and Style reconstruction loss & Image translation: INIT, Domain adaptation: KITTI to Cityscapes \\ \hline
    UVCGAN \cite{uvcgan} & WACV'23 & UNet-ViT Generator in CycleGAN framework (pre-trained on the image inpainting task) & CycleGAN discriminator with gradient penalty & GAN loss, cycle-consistency loss, and identity-consistency loss & Unpaired image translation: Selfie to Anime, Anime to Selfie, Male to Female, Female to Male, Remove Glasses, and Add Glasses\\ \hline
    SwinIR \cite{swinir} & ICCV'21 & Generator with residual Swin Transformer blocks & Traditional discriminator for super-resolution & Super-resolution: Pixel loss, GAN loss and perceptual loss & Super-resolution: Set5, Set14, BSD100, Urban100, Manga109, RealSRSet \\ \hline
    RFormer \cite{rformer} & IEEE-JBHI'22 & Transformer-based U-shaped generator with window-based self-attention block & Transformer-based discriminator with window-based self-attention block & Adversarial loss, Edge loss, Charbonnier loss and Fundus quality perception loss & Fundus image restoration: Real Fundus dataset \\ \hline
    3D Transformer GAN \cite{3d_transformer_gan} & MICCAI'21 & Generator consisting of Encoder CNN, Transformer and Decoder CNN & CNN with four convolution blocks & Adversarial loss and L1 loss & PET reconstruction: PET data \\ \hline
    3D CVT-GAN \cite{3dcvtgan} & MICCAI'22 & 3D convolutional vision transformer (CVT) based encoder and 3D transposed CVT based decoder & Patch-based discriminator embedded with 3D CVT blocks & Adversarial loss and L1 loss & PET reconstruction: PET data \\ \hline
    Low-Light Transformer-GAN \cite{lowlight_transformer_gan} & IEEE-SPL'22 & Transformer using multi-head multi-covariance self-attention and Light feature-forward module structures & Convolutional discriminator & Adversarial loss, smooth L1 loss, perceptual loss, and multi-scale SSIM loss & Low-light enhancement: LOL and SICE \\ \hline
    LightingNet \cite{lightingnet} & IEEE-TCI'23 & Fusion of CNN-based encoder-decoder and ViT-based encoder-decoder & CNN-based discriminator & Adversarial loss, smooth L1 loss, perceptual loss, and multi-scale SSIM loss & Low-light enhancement: LOL, SICE, ExDARK, DICM, LIME, MEF, and NPE \\ \hline
    ICT \cite{ict} & ICCV'21 & Bi-directional transformer guided CNN & CNN-based discriminator & Adversarial loss and L1 loss & Image completion: FFHQ and Places2 \\ \hline
    BAT-Fill \cite{batfill} & ACMMM'21 & Bidirectional and autoregressive transformer + CNN-based texture generation & CNN-based discriminator & Adversarial loss, perceptual loss and reconstruction loss & Image inpainting: CelebA-HQ, Places2 and Paris StreetView \\ \hline
    \textit{T}-former \cite{tformer} & ACMMM'22 & U-shaped generator with transformer blocks & Patch GAN discriminator & Adversarial loss, style loss, reconstruction loss and perceptual loss & Image inpainting: CelebA-HQ, Places2 and Paris StreetView \\ \hline
    APT \cite{apt} & ACMMM'22 & Atrous pyramid transformer and dual spectral transform convolution & CNN-based discriminator & Adversarial loss, perceptual loss, style loss and L1 loss for masked and preserved regions & Image inpainting: CelebA-HQ, Places2 and Paris StreetView \\ \hline
    MAT \cite{mat} & CVPR'22 & A convolutional head, a mask-aware transformer body and a convolutional tail & Traditional discriminator & Adversarial loss, perceptual loss and R1 regularization & Large hole image inpainting: CelebA-HQ and Places365-Standard \\ \hline
    ZITS \cite{zits} & CVPR'22 & Transformer-based structure restorer + CNN-based structure feature encoding and texture restoration & PatchGAN discriminator & Adversarial loss, L1 loss over unmasked region, feature match loss and high receptive field perceptual loss & Image inpainting: Places2, ShanghaiTech, NYUDepthV2 and MatterPort3D \\ \hline
    HAN \cite{han} & ECCV'22 & Generator with CNN encoder, hourglass attention structure blocks and CNN decoder & PatchGAN discriminator with spectral norm & Adversarial loss, style loss, reconstruction loss and perceptual loss & Image inpainting: CelebA-HQ, Places2 and Paris StreetView \\ \hline
    SRInpainter \cite{srinpaintor} & IEEE-TCI'22 & Resolution progressive CNN encoder, hierarchical transformer and CNN decoder & SNPatchGAN discriminator & Adversarial loss and super-resolved L1 loss & Image inpainting: CelebA-HQ, Places2 and Paris StreetView \\ \hline
    NDMAL \cite{ndmal} & WACV'23 & Nested deformable attention layer mixed with convolution and de-convolution layers & PatchGAN discriminator & Adversarial loss, perceptual loss, edge loss and L1 loss & Image inpainting: CelebA-HQ and Places2 \\ \hline
    Hint \cite{hint} & WACV'22 & ViT-based generated hint converts outpaining to inpainting & Traditional discriminator & Adversarial loss, style loss, perceptual loss and L1 loss & Image outpainting: SUN and Beach \\ \hline
    ColorFormer \cite{color_former} & ECCV'22 & Generator using transformer-based encoder and a color memory decoder & PatchGAN discriminator & Adversarial loss, perceptual loss and content loss & Image colorization: ImageNet, COCO-Stuff and CelebA-HQ \\ \hline
    SGA \cite{sga} & ECCV'22 & Generator with stop-gradient attention module between encoder and decoder & Conditional CNN discriminator & Adversarial loss, perceptual loss, reconstruction loss and style loss & Image colorization: Anime portraits and Animal FacesHQ \\ \hline
    VTGAN \cite{vtgan} & ICCV'21 & CNN-based generator at different resolution & ViT for discriminator and classification at different resolution & Adversarial loss, mean square error, perceptual loss, embedding feature loss and cross-entropy loss & Retinal image synthesis and disease prediction using fundus and fluorescein angiogram images \\ \hline
    ResViT \cite{resvit} & IEEE-TMI'22 &  Transformer-based generator using aggregated residual transformer blocks & Conditional PatchGAN discriminator & Adversarial loss, reconstruction loss and pixel loss & Multimodal medical image synthesis: IXI brain MRI, BRATS and MRI-CT \\ \hline
\end{tabular}
\label{tab:image_translation}
\end{table*}
\section{Transformer-based GANs for Image-to-Image Translation}
The image generation mechanism generates the artificial sample from a random latent vector in a given data distribution. However, the image-to-image translation aims to transform the image from one domain to another domain. Hence, the generator network in GAN is modified such that it can take an image as the input and produces an image as the output. Mainly, encoder-decoder based architecture serves this purpose. Recently, several researchers have tried to investigate the Transformer-based GANs for the image-to-image translation tasks. We provide a summary of Transformer-based GAN models for image-to-image translation in Table \ref{tab:image_translation} in terms of generator model, discriminator model, objective function, applications and datasets.
\begin{table*}[!t]
    \caption{The results comparison of Transformer-based GANs for PET reconstruction on NC subjects and MCI subjects datasets. }
    \centering
    \begin{tabular}{p{0.35\columnwidth} p{0.15\columnwidth} p{0.08\columnwidth} p{0.08\columnwidth} p{0.12\columnwidth} p{0.08\columnwidth} p{0.08\columnwidth} p{0.12\columnwidth} p{0.09\columnwidth} p{0.09\columnwidth}}
    \hline
    & & \multicolumn{3}{c}{NC subjects} & \multicolumn{3}{c}{MCI subjects} & Params & GFLOPs \\
    Method & Venue & PSNR & SSIM & NMSE & PSNR & SSIM & NMSE \\ \hline
    3D Transformer-GAN \cite{3d_transformer_gan} & MICCAI'21 & \textit{24.818} & \textit{0.986} & \textit{0.0212} & \textit{25.249} & \textbf{0.987} & \textit{0.0231} & \textit{76M} & \textbf{20.78}\\
    3D CVT-GAN \cite{3dcvtgan} & MICCAI'22 & \textbf{24.893} & \textbf{0.987} & \textbf{0.0182} & \textbf{25.275} & \textbf{0.987} & \textbf{0.0208} & \textbf{16M} & \textit{23.80}\\
    \hline
    \end{tabular}
    \label{tab:results_pet_reconstruction}
\end{table*}
\subsection{Image Translation}
Image translation from a source domain to a target domain is an attractive application of GANs, such as Pix2pix \cite{pix2pix} and CycleGAN \cite{cyclegan} are very popular for such tasks.
Recently, Kim et al. discovered InstaFormer which exploits the transformer by integrating the global information with instance information for unpaired image translation \cite{instaformer}. The instance-level features are generated with the help of bounding box detection. InstaFormer utilizes the adaptive instance normalization and instance-level content contrastive loss to improve the local context encoding. The ViT encoder block \cite{vit} is used as a backbone in the generator network of InstaFormer. An impressive results are observed using Instaformer in terms of FID \& SSIM of 84.72 \& 0.872 on sunny$\rightarrow$night and 71.65 \& 0.818 on night$\rightarrow$sunny datasets, respectively.
The generator network of CycleGAN \cite{cyclegan} is replaced by a Vision Transformer (ViT) \cite{vit} for unsupervised image-to-image transformation in UVCGAN by Torbunov et al. \cite{uvcgan}. It is noticed from the experiments of UVCGAN that the self-supervised pre-training and gradient penalty are important for the improvements. UVCGAN reports the state-of-the-art FID scores of 79.0, 122.8, 9.6, 13.9, 14.4, and 13.6 on Selfie to Anime, Anime to Selfie, Male to Female, Female to Male, Remove Glasses, and Add Glasses datasets, respectively. 
Using ViT as a generator in GAN framework is computationally challenging. Zheng et al. \cite{ittr} proposed ITTR model for image-to-image translation using transformer for unpaired scenario. ITTR reduces the computational complexity by utilizing a dual pruned self-attention mechanism. In order to utilize global semantics, ITTR performs the mixing of tokens of varying receptive fields through hybrid perception blocks. The FID scores achieved by ITTR are 45.1, 68.6, 33.6, 73.4, 93.7, and 91.6 on Scene$\rightarrow$Semantic Map (Cityscapes), Cat$\rightarrow$Dog, Horse$\rightarrow$Zebra, Selfie$\rightarrow$Anime, Face$\rightarrow$Metface, and Female$\rightarrow$Cartoon datasets, respectively. Wang et al. \cite{piti} performs generative pretraining for the diverse downstream tasks to generate a highly semantic latent space as tokens using a transformer from text-image pairs. An adversarial diffusion upsampler is utilized for increasing the resolution of generated samples.
The image translation has been also a popular choice for medical applications using GANs. A Swin transformer GAN (MMTrans) is proposed in \cite{swin_transformer_GAN} for multi-modal medical image translation. The generator network of MMTrans is followed by a registration network. MMTrans uses a convolution-based discriminator network. 
\subsection{Image Restoration}
Liang et al. proposed a Swin Transformer-based SwinIR model for image restoration \cite{swinir}. Basically, SwinIR utilizes several Swin Transformer layers together with a residual connection in a residual Swin Transformer block. SwinIR is tested for image super-resolution, image denoising and JPEG compression artifact restoration. It is reported that SwinIR leads to a reduction in the total number of parameters by up to 67\%, while achieves performance gain by up to 0.14$\sim$0.45dB.
Global-local stepwise GAN (GLSGN) is proposed by exploiting stepwise three local pathways and one global pathway based restoring strategy for high-resolution image restoration \cite{glsgn}. Interpathway consistency is applied for the mutual collaboration between four pathways of GLSGN. An impressive results are reported by GLSGN for high-resolution image dehazing, image deraining, and image reflection removal. However, the complexity of GLSGN is increased due to the uses of inputs at different resolution for different local pathways.
Transformer-based GAN (RFormer) is introduced by Deng et al. \cite{rformer} to clinical fundus images for restoration of the real degradation. In order to exploit the long-range dependencies and non-local self-similarity, a window-based self-attention block is utilized by RFormer. Moreover, a Transformer-based discriminator network is also used by RFormer. The experimental results with PSNR of 28.32 and SSIM of 0.873 are achieved for fundus image restoration by RFormer.
\subsection{Image Reconstruction}
Image reconstruction is very important w.r.t. biomedical applications, which can also be seen as image-to-image translation. Luo et al. \cite{3d_transformer_gan} proposed 3D Transformer-GAN for reconstruction of the standard-dose  positron emission tomography (SPET) image from the low-dose PET (LPET) image. The generator network in \cite{3d_transformer_gan} is developed as a CNN-based Encoder followed by a Transformer followed by a CNN-based Decoder. 3D Transformer-GAN shows the improved performance for the clinical PET reconstruction. A convolutional ViT based GAN, dubbed 3D CVT-GAN, is introduced by Zeng et al. for SPET reconstruction from LPET images \cite{3dcvtgan}. The encoder and decoder of 3D CVT-GAN use 3D CVT blocks for feature encoding and 3D transposed CVT (TCVT) blocks for SPET restoration, respectively. Table \ref{tab:results_pet_reconstruction} shows the results comparison between 3D Transformer-GAN \cite{3d_transformer_gan} and 3D CVT-GAN \cite{3dcvtgan} for PET reconstruction on normal control (NC) subjects and mild cognitive impairment (MCI) subjects datasets in terms of PSNR, SSIM, NMSE, parameters and GFLOPs. The higher values of PSNR \& SSIM and lower value of NMSE points out that 3D CVT-GAN is able to reconstruct the PET images with better quality. Moreover, 3D CVT-GAN is lighter than 3D Transformer-GAN in terms of the number of parameters. However, GFLOPs suggests that 3D Transformer-GAN leads to less number of operations as compared to 3D CVT-GAN.
A model-based adversarial transformer architecture (MoTran) is introduced for image reconstruction by Korkmaz et al. \cite{motran}. The generator of MoTran includes transformer and data-consistency blocks. MoTran reports better results than SLATER with best PSNR of 47.8 and SSIM of 0.992 on T2-weighted acquisitions of  IXI brain MRI dataset\footnote{http://brain-development.org/}. Zhao et al. \cite{swingan} proposed a SwinGAN which is a Swin Transformer-based GAN consisting of a frequency-domain generator and an image-domain generator for reconstruction of MRI images. SwinGAN achieves SSIM of 0.95 and PSNR of 32.96 on IXI brain dataset with 20\% undersampling rate, SSIM of 0.939 and PSNR of 34.50 on MRNet dataset of Knee \cite{mrnet}.
Chen et al. utilized a conditional GAN for cross-domain face synthesis and a mesh transformer for 3D face reconstruction \cite{transformer_3d_face_reconstruction}. Basically, conditional GAN translates the face images to a specific rendered style, which is exploited by transformer network to output 3D mesh vertices. The promising performance is reported in \cite{transformer_3d_face_reconstruction} for 3D face reconstruction using Transformer-based GAN.
\subsection{Low-light Image Enhancement}
Image-to-image translation is also useful for enhancing the low-light images. Wang et al. \cite{spgat} introduced a structural prior driven generative adversarial transformer (SPGAT) consisting of a structural prior estimator, a generator and two discriminators. The generator is a U-shaped transformer model. The efficacy of SPGAT is tested on both synthetic and real-world datasets for low-light image enhancement. In 2022, a Transformer-based GAN (Transformer-GAN) is used by Yang et al. \cite{lowlight_transformer_gan} for low-light image enhancement. In the first stage, the features are extracted by an iterative multi-branch network and image enhancement is performed in the second stage of image reconstruction. Basically, a ViT-based generator is combined with convolution-based discriminator in \cite{lowlight_transformer_gan}. Very recently, in 2023, Yang et al. proposed a LightingNet model for low-light image enhancement \cite{lightingnet}. LightingNet uses ViT-based low-light enhancement subnetwork along with a Res2Net-based complementary learning subnetwork. However, a CNN with 10 convolution layers is used as the discriminator network by LightingNet. The PSNR and SSIM scores of Transformer-based GANs are reported in Table \ref{tab:results_enhancement} for Low-light image enhancement over LOL \cite{lol} and SICE \cite{sice} datasets. Transformer-GAN \cite{lowlight_transformer_gan} achieves best PSNR and SSIM on LOL dataset. However, LightingNet \cite{lightingnet} performs better on the SICE dataset in terms of PSNR. It shows the suitability of transformers with GANs for image enhancement.
\begin{table}[!t]
    \caption{The results comparison of Transformer-based GANs for Low-light image enhancement over LOL \cite{lol} and SICE \cite{sice} datasets. }
    \centering
    \begin{tabular}{p{0.3\columnwidth} p{0.174\columnwidth} p{0.06\columnwidth} p{0.06\columnwidth} p{0.06\columnwidth} p{0.06\columnwidth} }
    \hline
    & & \multicolumn{2}{c}{LOL dataset} & \multicolumn{2}{c}{SICE dataset} \\
    Method & Venue & PSNR & SSIM & PSNR & SSIM \\ \hline
    SPGAT \cite{spgat} & arXiv'22 & 19.800 & 0.823 & - & - \\ 
    Transformer-GAN \cite{lowlight_transformer_gan} & IEEE-SPL'22 & \textbf{23.501} & \textbf{0.851} & \textit{21.902} & \textbf{0.878}\\
    LightingNet \cite{lightingnet} & IEEE-TCI'23 & \textit{22.735} & \textit{0.842} & \textbf{22.389} & \textit{0.801}\\
    \hline
    \end{tabular}
    \label{tab:results_enhancement}
\end{table}
\subsection{Image Super-resolution}
Transformer-based GANs have also shown its suitability for image super-resolution. SwinIR \cite{swinir}, proposed for image restoration, is also tested for image super-resolution on five benchmark datasets, resulting to outstanding performance. 
Kasem et al. utilized spatial transformer to develop a robust super-resolution GAN (RSR-GAN) \cite{spatial_transformer_gan}. Both the generator and discriminator networks of RSR-GAN use spatial transformer. The RSR-GAN shows promising performance on five benchmark super-resolution datasets. Du and Tian \cite{tgan} utilized the transformer and GAN (T-GAN model) for medical image super-resolution. The generator of T-GAN processes the input with two sub-networks, including a residual block based sub-network and a texture Transformer-based sub-network, and finally combines their output to generate the super-resolution image. The discriminator of T-GAN is a CNN-based network. The reported performance of T-GAN is PSNR of 34.92 \& SSIM of 0.94964 on Knee MRI images and PSNR of 34.69 \& SSIM of 0.9353 on Abdominal MRI images for super-resolution. Li et al. \cite{srinpaintor} proposed SRInpaintor by inheriting the characteristics of super-resolution using transformer for image inpainting. Coarse-to-fine information propagation and long-term relation encoding using hierarchical transformer are the key component of SRInpaintor. Very recently in 2023, Bao et al. proposed SCTANet model for face image super-resolution \cite{sctanet} which is a CNN-Transformer aggregation network by exploiting the spatial attention guidance. SCTANet utilizes a tail consisting of sub-pixel MLP-based upsampling module followed by a convolution layer. An outstanding performance is reported by SCTANet on CelebA and Helen face datasets for 4, 8 and 16 times super-resolution.
\begin{table}[!t]
    \caption{The results comparison of Transformer-based GANs for image inpainting over CelebA-HQ \cite{progressivegan} and Places2 \cite{places2} datasets. }
    \centering
    \begin{tabular}{p{0.06\columnwidth}p{0.217\columnwidth} p{0.174\columnwidth} p{0.05\columnwidth} p{0.06\columnwidth} p{0.05\columnwidth} p{0.06\columnwidth} }
    \hline
    & & & \multicolumn{2}{c}{CelebA-HQ} & \multicolumn{2}{c}{Places2} \\
    Mask & Method & Venue & SSIM & FID & SSIM & FID \\ \hline
    \multirow{4}{*}{30-40\%} & \textit{T}-former \cite{tformer} & ACMMM'22 &  \textbf{0.945} & \textbf{3.88} & 0.846 & \textit{26.56} \\ 
    & SRInpaintor \cite{srinpaintor} & IEEE-TCI'22 & 0.943 & 5.70 & \textit{0.862} & \textbf{11.24} \\
    & HAN \cite{han} & ECCV'22 & \textit{0.945} & \textit{3.93} & 0.839 & 28.85 \\
    
    & APT \cite{apt} & ACMMM'22 & - & - & \textbf{0.912} & - \\ 
    
    \hline
    \multirow{5}{*}{40-60\%} & ICT \cite{ict} & ICCV'21 & - & - & 0.739 & 34.21 \\
    & BAT-Fill \cite{batfill} & ACMMM'21 & 0.834 & \textbf{12.50} &  0.704 & \textbf{32.55} \\
    & PLSA \cite{plsa_vqgan} & HDIS'22 & - & - & \textbf{0.801} & \textit{33.14}\\
    & MAT \cite{mat} & CVPR'22 & \textit{0.847} & 13.12 & 0.726 & 35.81 \\
    & NDMAL \cite{ndmal} & WACV'23 & \textbf{0.858} & \textit{12.90} & \textit{0.776} & 37.89 \\
    \hline
    \end{tabular}
    \label{tab:results_inpainting}
\end{table}
\subsection{Image Inpainting and Outpainting}
Image inpainting/completion aims to fill the cracks and holes in the images. It is considered as an application of image-to-image translation. CNNs tend to suffer in understanding global structures for image completion due to some inherent characteristics (e.g., spatial invariant kernels and local inductive prior). Recently, transformers are also utilized for image inpainting as it can capture the global relationship.
In 2021, Wang et al. performed image inpainting using automatic consecutive context perceived transformer GAN (ACCP-GAN) \cite{accp_gan}. In the first stage, the broken areas are detected and repaired roughly using Convolution and gated Convolution-based sub-networks. In the second stage, the generated rough patches are refined by exploiting the serial perceive transformer which also exploits the information from neighboring images. ACCP-GAN is able to achieve 47.2252 FID on the N7 dataset for image inpainting. The transformer is combined with CNN in ICT \cite{ict} by first modeling the pluralistic coherent structures and some coarse textures using a transformer and then enhancing the local texture details using CNN. Impressive results on large-scale ImageNet dataset are obtained using ICT for image completion.
The recent methods exploit the masking with transformers \cite{mat}, \cite{zits}. 
A dynamic mask based attention module is utilized in Mask-Aware Transformer (MAT) \cite{mat} which aggregates non-local information only from partial valid tokens. MAT uses the transformer block between a convolutional head and a convolutional tail. MAT shows outstanding performance for high-resolution image inpainting. A Zero-initialized Residual Addition based Incremental Transformer Structure (ZITS) is proposed in \cite{zits} for image inpainting. ZITS exploits the orthogonal positional encoding in the masked regions. An impressive performance using ZITS is reported.
Hourglass attention network (HAN) and Laplace attention based transformer is proposed for image inpainting in \cite{han}. In 2022, \textit{T}-former is developed for image inpainting by Deng et al. \cite{tformer}. \textit{T}-former follows a linear attention based on Taylor expansion to reduce the computational complexity of self-attention for images. SRInpaintor performs the inpainting using transformer by exploiting the super-resolution characteristics \cite{srinpaintor}. 
Very recently, Phutke and Murala \cite{ndmal} proposed a nested deformable attention based transformer (NDMAL) for face image inpainting. The multi-head attention used in \cite{ndmal} consists of a deformable convolution leads to an efficient transformer model. Other Transformer-based GAN methods for image inpainting include atrous pyramid transformer and spectral convolution based model \cite{apt}, gated convolution and Transformer-based model \cite{generative_image_inpainting}, and visual transformers with multi-scale patch partitioning \cite{mspp}. The transformers have also been utilized for pluralistic image inpainting/completion \cite{ict}, \cite{plsa_vqgan}. The Swin Transformer-based models are used for image inpainting in \cite{sfiswin}, \cite{cswin_transformer}.
The results comparison in terms of SSIM and FID is performed in Table \ref{tab:results_inpainting} on CelebA-HQ \cite{progressivegan} and Places2 \cite{places2} datasets using different transformer driven GAN models for 30-40\% and 40-60\% mask. Higher SSIM and lower FID represent the better performance. It is noticed that \textit{T}-former \cite{tformer} performs best on CelebA-HQ dataset for 30-40\% mask. However, NDMAL \cite{ndmal} shows highest SSIM for 40-60\% mask on CelebA-HQ dataset. On Places2 dataset, APT \cite{apt} and PLSA \cite{plsa_vqgan} lead to highest SSIM for 30-40\% and 40-60\% mask, respectively. It is also observed that BAT-Fill \cite{batfill} is a better model to maintain a lower FID score across both the datasets.
Similar to image inpaining, image outpainiting is also performed as image-to-image translation where the outer area is reconstructed. Transformer-based GANs are also investigated for image outpaining, such as U-Transformer \cite{utransformer_outpainting} and ViT-based Hint \cite{hint}. U-Transformer exploits Swin transformer blocks for U-shaped encoder-decoder \cite{utransformer_outpainting}. Hint method attaches the representative hint at the boundaries and converts outpainting problem into inpainting problem \cite{hint}. The representative hint is generated using ViT on different patches in \cite{hint}. 
\subsection{Image Colorization}
Image colorization is performed as image-to-image translation. Recently, few Transformer-based GANs have shown promising performance for image colorization. A color memory powered hybrid-attention transformer (ColorFormer) is introduced by Ji et al. \cite{color_former} for image colorization. The encoder of ColorFormer is a global-local hybrid attention based transformer. The decoder of ColorFormer utilizes a color memory decoder for image-adaptive queries through semantic-color mapping. A stop-gradient attention (SGA) mechanism is utilized by Li et al. \cite{sga} which removes the conflicting gradient and becomes better suitable for reference-based line-art colorization. An impressive results are obtained for four datasets. SGA is also utilized for Anime line drawing colorization \cite{sga_ext}. A dual decoder based DDColor model consisting of a transformer-based color decoder and a multi-scale image decoder is investigated for image colorization in \cite{ddcolor}. Plausible results of image colorization are obtained using DDColor model.
A cycle swin transformer GAN (CSTGAN) is proposed in \cite{cstgan} for colorizing the infrared images in an unpaired scenario. CSTGAN combines two swin transformer blocks with one convolution block in a module and connect such modules with skip connections in the generator network.
\subsection{Medical Image Synthesis}
Li et al. \cite{slmtnet} performed cross-modality MR image synthesis using a self-supervised learning based multi-scale transformer network (SLMT-Net). A pretrained ViT encoder using edge information is followed by a multi-scale transformer U-Net to produce the image in target modality. Better PSNR scores are obtained using the SLMT-Net model for MR image synthesis.
A transformer generator is used to enhance the input image with global information followed by a CNN generator in TCGAN model \cite{tcgan}. Basically, TCGAN takes a positron emission tomography (PET) image as the input and synthesizes the corresponding computer tomography (CT) image as the output. In an another Residual Transformer Conditional GAN (RTCGAN) work, MR image is synthesized into the corresponding CT image by Zhao et al. \cite{rtcgan}. RTCGAN encodes the local texture information using CNN and global correlation using Transformer. RTCGAN achieves an impressive SSIM of 0.9105. A cycle-consistent Transformer (CyTran) model utilizing convolutional transformer block is used to synthesize the contrast CT images from the corresponding non-contrast CT images \cite{cytran}. 
The fundus retinal images are translated into corresponding angiogram images in VTGAN \cite{vtgan}, where two vision transformers are utilized as the discriminators-cum-classifiers for coarse and fine images, respectively. Basically, the vision transformer in VTGAN discriminates between the real and fake samples and at the same time outputs the normal vs. abnormal class labels. Different source-target modality configurations are combined into a ResViT model in \cite{resvit}. ResViT utilizes an Encoder-Decoder based generator which uses a residual convolutional and Transformer-based building blocks. ResViT enjoys the precision of convolution operators along with the contextual sensitivity of vision transformers. ResViT is demonstrated for the synthesis of missing frames in multi-contrast MRI, and CT images from MRI. A multi-view transformer-based generator by exploiting cross-view attention mechanism is introduced in \cite{mvt} for cardiac cine MRI reconstruction. The multi-view transformer model \cite{mvt} is able to focus on the important regions in different views for the reconstruction.
\begin{table*}[!t]
\caption{A summary of Transformer-based GANs for video applications.}
\centering
\begin{tabular}{p{0.18\columnwidth}|p{0.15\columnwidth}|p{0.5\columnwidth}|p{0.25\columnwidth}|p{0.36\columnwidth}|p{0.33\columnwidth}}
    \hline
    \textbf{Model} & \textbf{Venue} & \textbf{Generator} & \textbf{Discriminator} & \textbf{Objective Function} & \textbf{Application \& Datasets} \\
    \hline
    TATS \cite{tats} & ECCV'22 & Generator with time-agnostic 3D VQGAN and time-sensitive Trasformer & Two discriminators: a spatial discriminator and a temporal discriminator & VQGAN: Adversarial loss, matching loss, reconstruction loss, codebook loss, commit loss, Transformer: Negative log-likelihood & Video generation: UCF-101, Sky Time-lapse and Taichi-HD \\ \hline
    MAGViT \cite{magvit} & arXiv'22 & Generator consisting of the 3D-VQ Encoder, Bidirectional Transformer and 3D-VQ Decoder & StyleGAN-based 3D discriminator & GAN loss, image perceptual loss, LeCam regularization, reconstruction loss, refine loss, and masking loss &  Video generation: Multi-task with 10 tasks including prediction, interpolation, inpainiting, and outpainting \\ \hline
    ActFormer \cite{actformer} & arXiv'22 & Generator having action-conditioned interaction and temporal transformers & Graph Convolutional Network  & Conditional Wasserstein GAN loss & Motion generation: NTU-13, NTU RGB+D 120, BABEL, and GTA Combat \\ \hline
    FuseFormer \cite{fuseformer} & ICCV'21 & A sub-token fusion enabled Transformer with a soft split and composition method with CNN encoder and decoder & CNN-based video discriminator & Adversarial loss and reconstruction loss & Video inpainting: DAVIS and YouTube-VOS \\ \hline
    Style Transformer \cite{aast} & IEEE-TMM'22 & Generator with a deep encoder, axial attention block, transformer, and decoder & Temporal PatchGAN-based discriminator & Adversarial loss, L1 loss, and reconstruction loss & Video inpainting: DAVIS and YouTube-VOS \\ \hline
    DeViT \cite{devit} & ACMMM'22 & Generator with Encoder, Patch-based deformed Transformer, and decoder & Temporal PatchGAN-based discriminator & Adversarial loss and reconstruction loss on hole and valid pixels & Video inpainting: DAVIS and YouTube-VOS \\ \hline
    FGT \cite{fgt} & ECCV'22 & Generator with flow-guided content propagation, spatial and temporal Transformers & Temporal PatchGAN-based discriminator & Adversarial loss and reconstruction loss & Video inpainting: DAVIS and YouTube-VOS \\ \hline
    FGT++ \cite{fgt_extended} & arXiv'23 & FGT with flow-guided feature integration and flow-guided feature propagation modules & Temporal PatchGAN-based discriminator & Adversarial loss, spatial domain reconstruction loss and amplitude loss & Video inpainting: DAVIS and YouTube-VOS \\ \hline
    CT-D2GAN \cite{ct-d2gan} & ACMMM'21 & Convolutional transformer with encoder, temporal self-attention module and decoder & Two discriminators: 2D Conv and 3D Conv-based discriminators & Adversarial loss and pixel-wise L1 loss & Video anomaly detection: UCSD Ped2, CUHK Avenue, and ShanghaiTech Campus dataset \\ \hline
    Trajectory Transformer \cite{trajectory_prediction} & IJIS'21 & Transformer with multi-head convolutional self‐attention & Discriminator with a decoder and prediction module & Adversarial loss and L2 loss & Trajectory prediction: ETH and UCY datasets \\ \hline
    Bidirectional Transformer GAN \cite{btgan} & ACM-TMCCA'23 & Transformer-based motion generator for forward and backward processing & Frame-based and Sequence-based discriminators & Adversarial loss, inverse loss, and soft dynamic time warping (Soft-DTW) loss & Human motion prediction: public Human3.6M dataset \\ \hline
    MaskViT \cite{maskvit} & ICLR'23 & Generator with VQGAN and Bidirectional window transformer for variable percentage masked tokens prediction & VQ-GAN discriminator & Adversarial loss, perceptual loss and reconstruction loss & Video prediction: BAIR, RoboNet and KITTI datasets \\ \hline
    Recurrent Transformer Network \cite{rtn} & CVPR'22 & A bi-directional RNN architecture having temporal aggregation module in masked encoder and flow features and spatial restoration transformer followed by Conv layers & Temporal PatchGAN discriminator & Spatial-temporal adversarial loss, L1 loss and perceptual loss & Video colorization: DAVIS and REDS dataset\\
    \hline
\end{tabular}
\label{tab:video_applications}
\end{table*}
\subsection{Other Image-to-Image Translation Applications}
Transformer-based GANs are also utilized for different other image translation applications. A Transformer-based CycleGAN is proposed in \cite{transformer_liver_segmentation} for liver tumor segmentation. A CASTformer is proposed in \cite{castformer} for 2D medical image segmentation, which is a class-aware transformer for learning the important object semantic regions. Trans-CycleGAN is developed in \cite{transcyclegan} by utilizing the Transformer-based generator and discriminator networks for image style transfer. 
A symmetric and semantic aware transformer (SSAT) is exploited in \cite{ssat} for makeup transfer and removal by learning the semantic correspondences. Pose guided human image synthesis (PGHIS) is performed in \cite{pghis} by exploiting a transformer module between the encoder and decoder networks. The synthesis using PGHIS is performed on the decoupled human body parts (e.g., face, hair, feet, hands, etc.). Pedestrian synthesis is performed using a pose and color-gamut guided GAN (PC-GAN) in \cite{pc-gan}. The generator in the PC-GAN consists of a local displacement estimator, a color-gamut transformer, and a pose transporter. PC-GAN is used to improve the performance of person re-identification by augmenting the training set.
In \cite{asset}, a Transformer-based autoregressive semantic scene editing (ASSET) method is developed for high-resolution images based on the user's defined semantic map edits. ASSET uses dense attention in the transformer at lower image resolutions and sparsifies the attention matrix at high resolutions. Xu et al. proposed a Transformer-based GAN, named TransEditor, for facial editing \cite{transeditor}. TransEditor exploits dual-space editing and inversion strategy to enhance the interaction in a dual-space GAN to provide additional editing flexibility. The results indicate that  TransEditor is effective for highly controllable facial editing.
In 2023, a cycle transformer GAN (CTrGAN) is introduced by Mahpod et al. \cite{ctrgan} for Gait transfer having Transformer-based generator and discriminator networks. The source image is first converted into a dense pose which is then translated into the most natural dense pose of the target using CTrGAN. Finally, the image of the target is synthesized by rendering the generated pose. CTrGAN shows promising results for Gait transfer.
FusionGAN performs the depth estimation using Transformer-based GAN as a multimodal image translation \cite{fusiongan}. FusionGAN utilizes the late fusion of the features of the transformer block with the sparse depth map and RGB image, which is followed by residual in residual dense block and convolution layer to produce the dense depth map.
\subsection{Summary}
Following is the summary drawn from the Transformer-based GANs for image-to-image translation:
\begin{itemize}
    \item It is noticed that both global and local contexts play an important roles for image-to-image translation. Hence, the majority of the methods exploit both the Transformer and Convolution in the generator network.
    \item The generator generally consists of Encoder, Transformer and Decoder modules. Vision transformer based architectures are heavily utilized. Some methods also try to modify the self-attention module with task specific information.
    \item CNN-based discriminators are mostly exploited, such as PatchGAN discriminator. Some regularizers are also used by few models, such as gradient penalty and spectral normalization.
    \item The L1 loss, perceptual loss, reconstruction loss, and style loss are commonly used in the objective function along with adversarial loss.
    \item Transformer-based GANs have shown state-of-the-art results for several image-to-image translation applications, such as image translation, image reconstruction, image restoration, image enhancement, image super-resolution, image inpainting \& outpainting, image colorization, medical image synthesis, image segmentation, human and pose synthesis, image editing and depth prediction. 
\end{itemize}
\section{Transformer-based GANs for Video Applications}
In recent years, several attempts have been made to utilize the Transformer-based GANs for different video processing applications. A summary on Transformer-based GANs is presented in Table \ref{tab:video_applications} for different video applications in terms of generator network, discriminator network, objective function, application and datasets.
\subsection{Video Synthesis}
In 2022, Ge et al. \cite{tats} synthesized the longer videos of thousands of frames using Time-Agnostic VQGAN and Time-Sensitive Transformer (TATS). The hierarchical transformer in TATS helps to capture longer temporal dependencies. The experiments on benchmark UCF-101, Sky Time-lapse, and Taichi-HD datasets confirm the suitability of TATS \cite{tats} for generating the longer videos.
Yu et al. proposed a Masked Generative Video Transformer (MAGViT) \cite{magvit} by first 3D tokenizing the video and then applying an embedding method to model the masked video tokens. MAGViT is able to perform for diverse video generation from different visual appearances.
In 2022, Xu et al. proposed ActFormer model to generate the action-conditioned 3D human motion from a latent vector for the frames of single-person as well as multi-person interactions \cite{actformer}.
\subsection{Video Translation}
\subsubsection{Video Inpainting}
Video inpainting is a very important application of video translation. In 2021, Liu et al. proposed a Transformer-based FuseFormer model for video inpainting \cite{fuseformer}. FuseFormer exploits soft split and composition method to perform sub-token fusion. In 2022, a deformed vision transformer (DeViT) is proposed for video inpainting \cite{devit}. Deformed patch homography based patch alignment and mask pruning based patch attention are exploited by DeViT. The attention to spatial-temporal tokens is obtained by a spatial-temporal weighting adaptor module in \cite{devit}. A generator, consisting of deep encoder, axial attention block, transformer, and decoder, is utilized in Axial Attention-based Style Transformer (AAST) \cite{aast} for video inpainting. The transformer in \cite{aast} exploits the high-frequency patch information between temporal and spatial features. Flow guided transformer (FGT) is also utilized for video inpainting by integrating the completed optical flow into the transformers \cite{fgt}, \cite{fgt_extended}. 
The results comparison is performed in Table \ref{tab:results_video_inpainting} for video inpainting using transformer-based GAN models on YouTube-VOS \cite{youtube-vos} and DAVIS \cite{davis} datasets in terms of PSNR and SSIM. FGT++* with flow-guided content propagation \cite{fgt_extended} performs best for video inpainting. It is observed that the utilization of flow information with the transformer is very beneficial for video inpainting.
\subsubsection{Other Video Translation Applications}
Feng et al. proposed a CT-D2GAN model for future frame synthesis \cite{ct-d2gan}, consisting of a convolutional transformer generator, a 2D convolutional discriminator and a 3D convolutional discriminator. The generator in CT-D2GAN is made with a convolutional encoder followed by a temporal self-attention block followed by a convolutional decoder. Very recently, MaskViT is introduced for future frame prediction in videos \cite{maskvit}. MaskViT first converts the video frames into tokens using VQGAN and then randomly mask some tokens of the future frames and performs pre-training of a Bidirectional Window Transformer for masked token prediction. At the inference time, frames are predicted in an iterative refinement fashion with incrementally decreasing masking ratio. A recurrent transformer network (RTN) is introduced in \cite{rtn} for the restoration of degraded old films. RTN exploits the useful information from adjacent frames for ensuring temporal coherency and restoring the challenging artifacts.
A bidirectional transformer GAN (BTGAN) is proposed in \cite{btgan} for human motion generator. BTGAN follows the CycleGAN framework with motion sequences as input to generator and predicted motion sequences as output. A new soft dynamic time warping (Soft-DTW) loss is utilized by BTGAN for training the generator.
Transformer-based GANs are also utilized for pedestrian trajectory prediction in \cite{trajectory_prediction} by learning the pedestrian distribution to generate more reasonable future trajectories. 
\begin{table}[!t]
    \caption{The results comparison of Transformer-based GANs for video inpainting on YouTube-VOS \cite{youtube-vos} and DAVIS \cite{davis} datasets. The use of flow-guided content propagation is represented by *. }
    \centering
    \begin{tabular}{p{0.23\columnwidth} p{0.2\columnwidth} p{0.07\columnwidth} p{0.07\columnwidth} p{0.07\columnwidth} p{0.07\columnwidth}}
    \hline
    & & \multicolumn{2}{c}{Youtube-VOS} & \multicolumn{2}{c}{DAVIS} \\
    Method & Venue & PSNR & SSIM & PSNR & SSIM \\ \hline
    FuseFormer \cite{fuseformer} & ICCV'21 & 33.16 & 0.967 & 32.54 & 0.970 \\
    DeViT \cite{devit} & ACMMM'22 & 33.42 & 0.973 & 32.43 & 0.972\\
    AAST \cite{aast} & IEEE-TMM'22 & 33.23 & 0.967 & 32.71 & 0.972 \\
    FGT \cite{fgt} & ECCV'22 & 34.04 & 0.971 & 32.60 & 0.965 \\
    FGT* \cite{fgt} & ECCV'22 & 34.53 & \textit{0.976} & \textit{33.41} & \textit{0.974} \\
    FGT++ \cite{fgt_extended} & arXiv'23 & \textit{35.02} & \textit{0.976} & 33.18 & 0.971 \\
    FGT++* \cite{fgt_extended} & arXiv'23 & \textbf{35.36} & \textbf{0.978} & \textbf{33.72} & \textbf{0.976} \\
    \hline
    \end{tabular}
    \label{tab:results_video_inpainting}
\end{table}
\subsection{Summary}
Following is the summary on Transformer-based GANs for video applications:
\begin{itemize}
    \item Most of the models convert video into tokens using VQGAN-based Encoder and then apply the spatial and/or temporal transformers.
    \item The flow information is also exploited and utilized at different levels in the generator of some models.
    \item Temporal PatchGAN discriminator is utilized by a majority of the models. Convolution discriminators are also used by some models.
    \item The objective function usually contains adversarial loss, reconstruction loss, L1 loss and perceptual loss.
    \item Transformer-based GANs have shown promising performance for different video applications, such as video generation, video inpainting, video prediction, video anomaly detection and video colorization.
    \item It is noticed that the flow-guided feature integration and propagation is very important with transformers for video inpainting. 
\end{itemize}
\section{Transformer-based GANs for Miscelleneous Applications}
Apart from image and video synthesis, Transformer-based GANs have also been exploited for other applications, such as text-to-image generation, hyperspectral image classification, document image enhancement, etc.
\subsection{Text-to-Image Generation}
Transformer-based GANs have been utilized for text-to-image generation \cite{transformer_attngan}, \cite{dsegan}, \cite{layout_vqgan}, \cite{muse}. Naveen et al. performed text-to-image generation experiments with AttnGAN using different Transformer models such as BERT, GPT2, and XLNet \cite{transformer_attngan}. An improvement of 49.9\% in FID is observed in \cite{transformer_attngan}. Huang et al. proposed a dynamical semantic evolution GAN (DSE-GAN) \cite{dsegan} for text-to-image generation. The generator of DSE-GAN is composed of a single adversarial multi-stage structure having a series of grid-Transformer-based generative blocks which are weighted by a series of DSE modules at different stages. An object-guided joint decoding transformer is introduced by Wu et al. \cite{layout_vqgan} for generating the image and the corresponding layout from the text. The layout is encoded and decoded using a Layout-VQGAN to extract additional useful features for the synthesis of the complex scenes. In 2023, a masked generative transformer, named Muse, is investigated for text-to-image generation \cite{muse}. Muse is trained on a masked modeling task to predict randomly masked image tokens using pre-trained large language model (LLM). Muse shows very promising performance on the benchmark datasets, including FID of 7.88 on zero-shot COCO evaluation using a 3B parameter model and FID of 6.06 on CC3M dataset using a 900M parameter model.
\subsection{Hyperspectral Image Classification}
The ViT-based generator, discriminator, and classifier networks are utilized in HyperViTGAN model \cite{hypervitgan} for hyperspectral image (HSI) classification. The classifier network classifies the input HSI patch into one of the class categories, whereas the discriminator network classifies the input HSI patch into real or fake category. The improved results on three benchmark HSI datasets are obtained by HyperViTGAN. Similarly, a transformer-based generator network is used for synthesizing the class-specific HSI patches in \cite{transgan_hsi} to alleviate the data imbalance problem in HSI classification.
\subsection{Other Applications}
Kodym and Hradiš \cite{tg2} generated high-quality text line image from the corresponding degraded low-quality text line image and the corresponding transcription as an input. The low-quality image is translated into the high-quality image using an Encoder-Decoder network. The transcription information is aligned with the output of Encoder using transformer consisting of multi-head attention modules. A promising performance is recorded for the document image restoration tasks, including inpainting, debinarization, and deblurring.
A transformer model is used for anomaly detection in \cite{transformer_based_gan} by exploiting the holistic features of different classes. In order to encode global semantic information, self-attention modules are used in generator of GAN in \cite{transformer_based_gan}. 
A cross-modal Transformer-based GAN (CT-GAN) is utilized in \cite{ct-gan} for synthesizing the multi-modal brain connectivity. Basically, CT-GAN fuses the structural information and functional information from different imaging modalities. Two decoders are used to extract the functional connectivity and structural connectivity from the fused connectivity. Two discriminators are also employed in \cite{ct-gan} corresponding to two decoders.
\subsection{Summary}
Overall, it is noticed that Transformer-based GANs are very successful for different image applications, including text-to-image generation, hyperspectral image classification, document analysis, anomaly detection, etc. The use of transformer-based GANs is still limited to various computer vision applications leaving to a huge scope for future research in this domain. 
\section{Conclusion and Future Directions}
\label{conclusion}
\subsection{Conclusion and Trend}
This paper presents a comprehensive survey of Transformer-based Generative Adversarial Networks (GANs) for computer vision applications. As most of the Transformer-based GANS are very recent, the progress and advancements in the past few years, i.e., 2020-2023 are presented. The survey is conducted for different type of applications, including image generation, image-to-image translation, video generation, video translation and other computer vision applications. The detailed discussion is provided with further application specific categorization, such as translation, restoration, reconstruction, inpainting, super-resolution, etc. A summary of different Transformer-based GAN models is presented in terms of generator architecture, discriminator architecture, objective functions, applications and datasets. The performance analysis using the state-of-the-art Transformer-based GAN models are also presented in terms of FID/PSNR/SSIM for different applications, such as image generation, high-resolution image generation, PET image reconstruction, low-light image enhancement, image inpainting and video inpainting. 
The research trend in image and video synthesis points out that the Transformer-based GANs are the latest driving factor in the progress of computer vision methods and applications. The generator with recently developed Transformer, Vision Transformer and Swin Transformer models has shown the superior performance for image and video synthesis. The utilization of both local context and global context using the Convolution and Transformer networks is heavily utilized by several models. For translation task, U-shaped generator with Convolution-based Encoder followed by Transformer followed by Convolution-based Decoder is very well exploited. The StyleGAN with Swin Transformer has shown outstanding performance for image generation. For video translation task, the flow information is exploited with Transformers. Mostly, Transformer-based GANs rely on CNN-based discriminator. However, few models also tried to exploit the Transformers for discriminator. The utilization of suitable loss functions with adversarial loss is also the recent trend in order to improve the synthesis quality, perceptual quality, etc. Other trends include prediction of masked tokens, designing of novel attention module, utilization of cross-attention and convolutional transformers, etc.
\subsection{Future Directions}
The future works in Transformer-based GANs include development of generator and discriminator architectures for different applications. There is a huge scope to find better ways to fuse the Convolution and Transformer features in order to exploit the local and global information. The modification in self-attention module with task specific characteristics is also a potential future direction. Other future direction involves the advancements in Vision and Swin Transformers by exploiting the power of state-of-the-art GAN models, such as StyleGAN, etc. The masking-based pre-training of transformer model can also be further explored. A major future aspect is to utilize the Transformer-based GANs for different applications of image and video processing in computer vision. Exploitation of flow information with Transformers can be explored further for video applications. The pre-training of Transformer-based GANs with diverse applications using large-scale datasets, development of light-weight models, utilization of self-supervision, etc. are the further scope to progress. The development of better loss functions and the identification of suitable combination of losses in objective function can also be performed in the future. The exploration to find the better hyperparameter settings to stabilize the training and to increase the generalization capability of the Transformer-based GAN models.
{\small
{IEEEtran}
}
\begin{IEEEbiography}[{[width=1in,height=1.25in,clip,keepaspectratio]{srd.jpg}}]{Shiv Ram Dubey} is with the Indian Institute of Information Technology (IIIT), Allahabad since July 2021, where he is currently the Assistant Professor of Information Technology. He was with IIIT Sri City as Assistant Professor from Dec 2016 to July 2021 and Research Scientist from June 2016 to Dec 2016. He received the PhD degree from IIIT Allahabad in 2016. Before that, from 2012 to 2013, he was a Project Officer at Indian Institute of Technology (IIT), Madras. He was a recipient of several awards including the Best PhD Award in PhD Symposium at IEEE-CICT2017, Early Career Research Award from SERB, Govt. Of India and NVIDIA GPU Grant Award Twice from NVIDIA. Dr. Dubey is serving as the Treasurer of IEEE Signal Processing Society Uttar Pradesh Chapter.
His research interest includes Computer Vision and Deep Learning.
\end{IEEEbiography}
\begin{IEEEbiography}[{[width=1in,height=1.25in,clip,keepaspectratio]{sks.png}}]{Satish Kumar Singh}
is with the Indian Institute of Information Technology Allahabad, as an Associate Professor from 2013 and heading the Computer Vision and Biometrics Lab (CVBL). Earlier, he served at Jaypee University of Engineering and Technology Guna, India from 2005 to 2012. His areas of interest include Image Processing, Computer Vision, Biometrics, Deep Learning, and Pattern Recognition. Dr. Singh is proactively offering his volunteer services to IEEE for the last many years in various capacities. He is the senior member of IEEE. Presently Dr. Singh is the Section Chair IEEE Uttar Pradesh Section (2021-2022) and a member of IEEE India Council (2021). He also served as the Vice-Chair, Operations, Outreach and Strategic Planning of IEEE India Council (2020) \& Vice-Chair IEEE Uttar Pradesh Section (2019 \& 2020). Prior to that Dr. Singh was Secretary, IEEE UP Section (2017 \& 2018), Treasurer, IEEE UP Section (2016 \& 2017), Joint Secretary, IEEE UP Section (2015), Convener Web and Newsletters Committee (2014 \& 2015). 
Dr. Singh is also the technical committee affiliate of IEEE SPS IVMSP and MMSP and presently the Chair of IEEE Signal Processing Society Chapter of Uttar Pradesh Section. 
\end{IEEEbiography}
"
0,"\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}
\typeout{** loaded for the language `#1'. Using the pattern for}
\typeout{** the default language instead.}
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{resilience}
K.~J. McIntyre, C.~A. LaFleur, N.~Chatterjee, R.~F. Powelson, and R.~Glick,
  ``Grid resilience in regional transmission organizations and independent
  system operators,'' \emph{United Stated of America Federal Energy Regulatory
  Commission}, 2018.

\bibitem{1995AFF}
J.~Ancona, ``A framework for power system restoration following a major power
  failure,'' \emph{IEEE Transactions on Power Systems}, vol.~10, pp.
  1480--1485, 1995.

\bibitem{ElectricGB}
J.~O'Brien, M.~Cassiadoro, T.~Becejac, G.~B. Shebl{\'e}, J.~Follum, U.~Agrawal,
  E.~Andersen, M.~Touhiduzzaman, and J.~E. Dagle, ``Electric grid blackstart:
  Trends, challenges, and opportunities,'' 2021.

\bibitem{Ding2022ASB}
T.~Ding, Z.~Wang, M.~Qu, Z.~Wang, and M.~Shahidehpour, ``A sequential
  black-start restoration model for resilient active distribution networks,''
  \emph{IEEE Transactions on Power Systems}, vol.~37, pp. 3133--3136, 2022.

\bibitem{Shi2022ATS}
S.~Shi, J.~Zhou, Y.~Su, X.~Wei, S.~Zhou, and H.~Liu, ``A two-stage stochastic
  distribution network restoration model with waste incineration power plants
  as black-start power resources,'' \emph{2022 7th Asia Conference on Power and
  Electrical Engineering (ACPEE)}, pp. 1689--1693, 2022.

\bibitem{Zhu2021ATD}
Y.~Zhu, Y.~Zhou, Z.~Wang, C.~Zhou, and B.~Gao, ``A terminal distribution
  network black-start optimization method based on pruning algorithm
  considering distributed generators,'' \emph{Energy Reports}, 2021.

\bibitem{2016OptimalBS}
F.~Qiu, J.~Wang, C.~Chen, and J.~Tong, ``Optimal black start resource
  allocation,'' \emph{IEEE Transactions on Power Systems}, vol.~31, pp.
  2493--2494, 2016.

\bibitem{2018OptimalBS}
G.~Patsakis, D.~Rajan, I.~Aravena, J.~Rios, and S.~S. Oren, ``Optimal black
  start allocation for power system restoration,'' \emph{IEEE Transactions on
  Power Systems}, vol.~33, pp. 6766--6776, 2018.

\bibitem{cranking}
D.~Wang, X.~Gu, G.~Zhou, S.~Li, and H.~Liang, ``Decision‐making optimization
  of power system extended black‐start coordinating unit restoration with
  load restoration,'' \emph{International Transactions on Electrical Energy
  Systems}, vol.~27, 2017.

\bibitem{2018WindCG}
P.~Maloney, Q.~Xu, J.~D. McCalley, B.~F. Hobbs, Sara, Daubenberger, and
  A.~Johnson, ``Wind capacity growth in the northwest us : co-optimized vs
  sequential generation and transmission planning,'' vol. 43(6), 2019, pp.
  573--595.

\bibitem{3568461}
\BIBentryALTinterwordspacing
P.~(https://math.stackexchange.com/users/756404/pablo), ``linear programming
  set a variable the max between two another variables,'' Mathematics Stack
  Exchange, uRL:https://math.stackexchange.com/q/3568461 (version: 2021-11-23).
  [Online]. Available: \url{https://math.stackexchange.com/q/3568461}
\BIBentrySTDinterwordspacing

\bibitem{2013LocalSO}
W.~A. Bukhsh, A.~Grothey, K.~I.~M. McKinnon, and P.~A. Trodden, ``Local
  solutions of the optimal power flow problem,'' \emph{IEEE Transactions on
  Power Systems}, vol.~28, pp. 4780--4788, 2013.

\end{thebibliography}
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}
\typeout{** loaded for the language `#1'. Using the pattern for}
\typeout{** the default language instead.}
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{resilience}
K.~J. McIntyre, C.~A. LaFleur, N.~Chatterjee, R.~F. Powelson, and R.~Glick,
  ``Grid resilience in regional transmission organizations and independent
  system operators,'' \emph{United Stated of America Federal Energy Regulatory
  Commission}, 2018.

\bibitem{1995AFF}
J.~Ancona, ``A framework for power system restoration following a major power
  failure,'' \emph{IEEE Transactions on Power Systems}, vol.~10, pp.
  1480--1485, 1995.

\bibitem{ElectricGB}
J.~O'Brien, M.~Cassiadoro, T.~Becejac, G.~B. Shebl{\'e}, J.~Follum, U.~Agrawal,
  E.~Andersen, M.~Touhiduzzaman, and J.~E. Dagle, ``Electric grid blackstart:
  Trends, challenges, and opportunities,'' 2021.

\bibitem{Ding2022ASB}
T.~Ding, Z.~Wang, M.~Qu, Z.~Wang, and M.~Shahidehpour, ``A sequential
  black-start restoration model for resilient active distribution networks,''
  \emph{IEEE Transactions on Power Systems}, vol.~37, pp. 3133--3136, 2022.

\bibitem{Shi2022ATS}
S.~Shi, J.~Zhou, Y.~Su, X.~Wei, S.~Zhou, and H.~Liu, ``A two-stage stochastic
  distribution network restoration model with waste incineration power plants
  as black-start power resources,'' \emph{2022 7th Asia Conference on Power and
  Electrical Engineering (ACPEE)}, pp. 1689--1693, 2022.

\bibitem{Zhu2021ATD}
Y.~Zhu, Y.~Zhou, Z.~Wang, C.~Zhou, and B.~Gao, ``A terminal distribution
  network black-start optimization method based on pruning algorithm
  considering distributed generators,'' \emph{Energy Reports}, 2021.

\bibitem{2016OptimalBS}
F.~Qiu, J.~Wang, C.~Chen, and J.~Tong, ``Optimal black start resource
  allocation,'' \emph{IEEE Transactions on Power Systems}, vol.~31, pp.
  2493--2494, 2016.

\bibitem{2018OptimalBS}
G.~Patsakis, D.~Rajan, I.~Aravena, J.~Rios, and S.~S. Oren, ``Optimal black
  start allocation for power system restoration,'' \emph{IEEE Transactions on
  Power Systems}, vol.~33, pp. 6766--6776, 2018.

\bibitem{cranking}
D.~Wang, X.~Gu, G.~Zhou, S.~Li, and H.~Liang, ``Decision‐making optimization
  of power system extended black‐start coordinating unit restoration with
  load restoration,'' \emph{International Transactions on Electrical Energy
  Systems}, vol.~27, 2017.

\bibitem{2018WindCG}
P.~Maloney, Q.~Xu, J.~D. McCalley, B.~F. Hobbs, Sara, Daubenberger, and
  A.~Johnson, ``Wind capacity growth in the northwest us : co-optimized vs
  sequential generation and transmission planning,'' vol. 43(6), 2019, pp.
  573--595.

\bibitem{3568461}
\BIBentryALTinterwordspacing
P.~(https://math.stackexchange.com/users/756404/pablo), ``linear programming
  set a variable the max between two another variables,'' Mathematics Stack
  Exchange, uRL:https://math.stackexchange.com/q/3568461 (version: 2021-11-23).
  [Online]. Available: \url{https://math.stackexchange.com/q/3568461}
\BIBentrySTDinterwordspacing

\bibitem{2013LocalSO}
W.~A. Bukhsh, A.~Grothey, K.~I.~M. McKinnon, and P.~A. Trodden, ``Local
  solutions of the optimal power flow problem,'' \emph{IEEE Transactions on
  Power Systems}, vol.~28, pp. 4780--4788, 2013.

\end{thebibliography}
"
29,"\importpackages{}
\graphicspath{ {./images/} }


\maketitle
\section{Introduction}
Delivering impactful ML-based solutions for real-world applications in domains like health care and recommendation systems requires access to sensitive personal data that cannot be readily used or shared without risk of introducing ethical and legal implications.
Replacing real sensitive data with private synthetic data following the same distribution is a clear pathway to mitigating these concerns \citep{patki2016synthetic,dankar2021fake,chen2021synthetic}.
However, despite their theoretical appeal, general-purpose methods for generating useful and provably private synthetic data remain a subject of active research \citep{dockhorn2022differentially,mckenna2022aim,torfi2022differentially}.
The central challenge in this line of work is how to obtain truly privacy-preserving synthetic data free of the common pitfalls faced by classical anonymization approaches \citep{stadler2021synthetic}, while at the same time ensuring the resulting datasets remain useful for a wide variety of downstream tasks, including statistical and exploratory analysis as well as machine learning model selection, training and testing.
\begin{figure*}
    \centering
    \includegraphics[width = \textwidth]{figures/all_images.pdf}
    \caption{DP diffusion models are capable of producing high-quality images. More images can be found in Figures \ref{fig:mnist}, \ref{fig:camelyon}, \ref{fig:cifar}.}
    \label{fig:images}
\end{figure*}
It is tempting to obtain synthetic data by training and then sampling from  well-known generative models like variational auto-encoders \citep{kingma2013auto}, generative adversarial nets \citep{goodfellow2020generative}, and denoising diffusion probabilistic models \citep{song2019generative, ho2020denoising}.
Unfortunately, it is well-known that out-of-the-box generative models can potentially memorise and regenerate their training data points\footnote{A model does not contain its training data, but rather has ``memorised'' training data when the model is able to use the rules and attributes it has learned about the training data to generate elements of that training data.} and, thus, reveal private information.
This holds for variational autoencoders \citep{hilprecht2019monte}, generative adversarial nets \citep{hayes2017logan}, and also diffusion models \citep{carlini23extracting, somepalli2022diffusion, mia-diff-23,https://doi.org/10.48550/arxiv.2302.01316}.
In particular, diffusion models have recently gained a lot of attention, with pre-trained models made available online \citep{dhariwal2021diffusion, rombach2022high}, and being fine-tuned on applications involving potentially sensitive data such as chest X-rays \citep{chambon2022adapting, ali2022spot, chambon2022roentgen} and brain MRIs \citep{rouzrokh2022multitask, pinaya2022brain}.
Mitigating the privacy loss from sharing synthetic data produced by generative models trained on sensitive data is not straightforward.
Differential privacy (DP) \citep{dwork2006calibrating} has emerged as the gold standard privacy mitigation when training ML models, and its application to generative models would provide guarantees on the information the model (and synthetic data sampled from it) can leak about individual training examples.
Yet, scaling up DP training methods to modern large-scale models remains a significant challenge due to the slow down incurred by DP-SGD (the standard workhorse of DP for deep learning) \citep{wang2017differentially} and the stark utility degradation often observed when training large models from scratch with DP \citep{zhang2022no, stadler2021synthetic, stadler2022search}.
Most previous works on DP generative models worked around these issues by focusing on small models, low-complexity data \citep{xie2018differentially, torkzadehmahani2019dp, harder2021dp} or using non-standard models \citep{harder2022differentially}.
However, for DP applications to image classification it is known that using models pre-trained on public data is a method for attaining good utility which is compatible with large-scale models and complex datasets \citep{bu2022scalable, de2022unlocking, cattan2022fine, xu2022learning, tramer2022considerations, bu2022automatic}.
\paragraph{Contributions.}
In this paper we demonstrate how to accurately train standard diffusion models with differential privacy. Despite the inherent difficulty of this task, we propose a simple methodology that allows us to generate high-quality synthetic image datasets that are useful for a variety of important downstream tasks.
In particular, we privately train denoising diffusion probabilistic models \citep{ho2020denoising} with more than 80M parameters on CIFAR-10 and Camelyon17 \citep{wilds2021}, and evaluate the usefulness of synthetic images for downstream model training and evaluation.
Crucially, we show that by pre-training on publicly available data (i.e.\ ImageNet), it is possible to considerably outperform the results of extremely recent work on a similar topic \citep{dockhorn2022differentially}.
With this method, we are able to accurately train models 45x larger than \citet{dockhorn2022differentially} and to achieve a high utility on datasets that are significantly more challenging (e.g. CIFAR-10 and a medical dataset – instead of MNIST). Please refer to \autoref{tab:compare-dokhorn} for a detailed comparison of our works. 
Our contributions can be summarized as follows: 
\begin{itemize}
    \item We demonstrate that diffusion models can be trained with differential privacy to sufficient quality that we can create accurate classifiers based on the synthesized data only. To do so, we leverage pre-training, and we demonstrate large state-of-the-art improvements even when there exists a significant distribution shift between the pre-training and the fine-tuning data sets. 
    \item We propose simple and practical improvements over existing methods to further boost the performance of the model. Namely, we employ both image and timestep augmentations when using augmentation multiplicity, and we bias the diffusion timestep sampling so as to encourage learning of the most challenging phase of the diffusion process.
    \item With this approach, we fine-tune a differentially private diffusion model with more than 80 million parameters on CIFAR-10, and beat the previous state-of-the-art by more than 50\%, decreasing the Fr\'{e}chet Inception Divergence (FID) from 26.8 to 9.8. Furthermore, we privately fine-tune the same model on histopathological scans of lymph node tissue available in the Camelyon17 dataset and show that a classifier trained on synthetic data produced by this model achieves 91.1\% accuracy (the highest accuracy reported on the WILDS leaderboard \citep{wilds2021} is 96.5\% for a non-private model trained on real data).
    \item We demonstrate that the accuracy of downstream classifiers can be further improved to a significant extent by leveraging larger synthetic datasets and ensembling, which comes at no additional privacy cost. Finally, we show that hyperparameter tuning downstream classifiers on the synthetic data reveals trends that are also reflected when tuning on the private data set directly.
\end{itemize}
\paragraph{Paper outline.} We start by comparing to related work in \autoref{sec:related}, before we provide a brief introduction to diffusion models, differential privacy, and DP-SGD in \autoref{sec:background}. In \autoref{sec:method}, we describe effective strategies to fine-tune DP diffusion models, and then present our results on CIFAR-10 and Camelyon17 in \autoref{sec:finetuning}. In \autoref{sec:model-select}, we assess the utility of synthetic data for model selection. 
\section{Related Work}
\label{sec:related}
\paragraph{Differentially private synthetic image generation.}
DP image generation is an active area of research \citep{fan2020survey, croft2022differentially, chen2022dpgen}. Most efforts have focused on applying a differentially private stochastic gradient procedure on popular generative models, i.e. 
generative adversarial networks \citep{xie2018differentially,jordon2018pate,torkzadehmahani2019dp,augenstein2019generative,xu2019ganobfuscator,liu2019ppgan,chen2020gs,yoon2020anonymization,chen2021differentially}, 
or variational autoencoders \citep{pfitzner2022dpd,jiang2022dp}. Only one other work has so far analysed the application of differentially private gradient descent on diffusion models \citep{dockhorn2022differentially} which we contrast against in \autoref{tab:compare-dokhorn}. Others have instead proposed custom architectures \citep{chen2022private, wang2021datalens, cao2021don, harder2021dp, harder2022differentially}. \citet{harder2022differentially}, for instance, pre-train a perceptual feature extractor using public data, then privatize the mean of the feature embeddings of the sensitive data records, and use the privatised mean to train a generative model.
\paragraph{Limitations of previous work.} DP image generation based on custom training pipelines and architectures that are not used outside of the DP literature do not profit from the constant research progress on public image generation. Other works that instead build upon popular public generative approaches have been shown to not be differentially private despite such claims. This could be either due to faulty implementations or proofs. See \citet{stadler2021synthetic} for successful privacy attacks on DP GANs, or Appendix B of \citet{dockhorn2022differentially} for an illustration on why DPGEN \citep{chen2022dpgen} does not actually satisfy DP guarantees.
\paragraph{Limited success on natural images.} DP synthesizers have found applications on tabular electronic healthcare records \citep{zhang2021feddpgan,torfi2022differentially,fang2022dp,yan2022multifaceted}, mobility trajectories \citep{alatrista2022geolocated} and network traffic data \citep{fan2021dpnet}. In the space of image generation, positive results have only been reported on MNIST, FashionMNIST and CelebA (downsampled to $32\times32$) \citep{harder2021dp,wang2021datalens,liew2021pearl, bie2022private}. These datasets are relatively easy to learn thanks to plain backgrounds, class separability, and repetitive image features within and even across classes. 
Meanwhile, CIFAR-10 has been established as a considerably harder generation task than MNIST, FashionMNIST or CelebA \citep{radiuk2017impact}. 
The images are not only higher dimensional than MNIST and FashionMNIST ($32\times32\times3$ compared to $28\times28$ feature dimensions), but the dataset has wider diversity and complexity. This is reflected by more complex features and textures, such as lightning conditions, object orientations, and complex backgrounds \citep{radiuk2017impact}. 
Moreover, MNIST and FashionMNIST are considerably lower dimensional than CIFAR-10 and Camelyon ($28\times28$ vs $32\times32\times3$ features), and CelebA is downsampled to the same feature dimensionality as CIFAR-10 but has more than 3 times as many samples as CIFAR-10 (50k vs 162k) which considerably reduces the information loss introduced by DP training. 
As far as we know, only two other concurrent works have attempted DP image generation on CIFAR-10. While \citet{dockhorn2022differentially} achieve a FID of only 97.7 by training a DP diffusion model from scratch, \citet{harder2022differentially} used pre-training on ImageNet and achieved the SOTA with a FID of 26.8, and a downstream accuracy of only 51\%.
\paragraph{Limited targeted evaluation.} The evaluation carried out on DP synthetic datasets is often not sufficiently targeted towards their utility in practice. The performance of DP image synthesizers is commonly evaluated on two types of metrics: 1) perceptual difference measures between the synthetic and real data distribution, such as FID, and 2) predictive performance of classifiers that are trained on a synthetic dataset of the size of the original training dataset and tested on the real test data. The former metric is known to be easy to manipulate with factors not related to the image quality, such as the number of samples, or the version number of the inception network \citep{kynkaanniemi2022role}. At the same time, it is not obvious how to jointly incorporate the information from both metrics given that they may individually imply different conclusions. \citet{dockhorn2022differentially}, for instance, identify different diffusion model samplers to minimise either the FID or the downstream test loss.
Further, recent research has identified use cases where synthetic data is not able to capture important first or second order statistics despite reportedly scoring highly on those metrics \citep{stadler2021synthetic, stadler2022search}. In this paper, we set out to provide examples of additional downstream evaluations.
\section{Background}
\label{sec:background}
\subsection{Denoising Diffusion Probabilistic Models} 
Denoising diffusion models \citep{sohl2015deep, song2019generative, ho2020denoising} are a class of likelihood-based generative models that have recently established new SOTA results across diverse computer vision problems \citep{dhariwal2021diffusion,rombach2022high,lugmayr2022ntire}. 
Given a forward Markov chain that sequentially perturbs a real data sample $x_0$ to obtain a pure noise distribution $x_T$, diffusion models parameterize the transition kernels of a backward chain by deep neural networks to denoise $x_T$ back to $x_0$.
Given $x_0\sim q(x_0)$, one defines a forward process that generates gradually noisier samples $x_1, ..., x_T$ using a transition kernel $q(x_t|x_{t-1})$ typically chosen as a Gaussian perturbation.
At inference time, $x_T$, an observation sampled from a noise distribution, is then gradually denoised $x_{T-1}, x_{T-2}, ...$ until the final sample $x_0$ is reached. 
\citet{ho2020denoising} parameterize this process by a function $\epsilon_{\theta}(x_t , t)$ which predicts the noise component $\epsilon$ of a noisy sample $x_t$ given timestep $t$. They then propose a simplified training objective to learn $\theta$, namely
\begin{align}
    L(\theta) = \mathbb{E}_{t, x_0, \epsilon}[\|{
    \epsilon-\epsilon_{\theta}(
    x_t
    , t)
    }\|^2], \label{eq:loss}
\end{align}
with $t\sim \mathcal{U}[0, T]$, where $T$ is the pre-specified maximum timestep, and $\mathcal{U}[a, b]$ is the discrete uniform distribution bounded by $a$ and $b$. The noisy sample $x_t$ is computed by $x_t=\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$ where $\bar{\alpha}_t$ is defined such that $x_t$ follows the pre-specified forward process. Most importantly, $\bar{\alpha}_t$ is a decreasing function of timestep $t$. Thus, the larger $t$ is, the noisier $x_t$ will be.
\subsection{Differential Privacy}  
Differential Privacy (DP) is a formal privacy notion that, in informal terms, bounds how much a single observation can change the output distribution of a randomised algorithm. More formally:
\begin{definition}[Differential Privacy \citep{dwork2006calibrating}]
Let $A$ be a randomized algorithm, and let $\varepsilon > 0$, $\delta \in [0, 1]$.
We say that $A$ is $(\varepsilon, \delta)$-DP if for any two neighboring datasets $D, D'$ differing by a single element, we have that
\begin{align*}
\forall \: S \subset \mathcal{S},\,\, \mathbb{P}[A(D) \in S] \leq \exp(\varepsilon) \mathbb{P}[A(D') \in S] + \delta ,
\end{align*}
where $\mathcal{S}$ denotes the support of $A$.
\end{definition}
The privacy guarantee is thus controlled by two parameters, $\varepsilon$ and $\delta$. 
While $\varepsilon$ bounds the log-likelihood ratio of any particular output that can be obtained when running the algorithm on two datasets differing in a single data point, $\delta$ is a small probability which bounds the occurrence of infrequent outputs that violate this bound (typically $1 / n$, where $n$ is the number of training examples). The smaller these two parameters get, the higher is the privacy guarantee. We therefore refer to the tuple $(\varepsilon, \delta)$ as privacy budget.
\subsection{Differentially Private Stochastic Gradient Descent} 
Neural networks are typically privatised with Differentially Private Stochastic Gradient Descent (DP-SGD) \citep{abadi2016deep}, or alternatively a different DP optimizer like DP-Adam \citep{mcmahan2018general}. At each training iteration, the mini-batch gradient is clipped per example, and Gaussian noise is added to it. More formally, let $l_i(w):=\mathcal{L}(w,x_i,y_i)$ denote the learning objective given model parameters $w\in\mathbb{R}^p$, input features $x_i$ and label $y_i$. Let $\texttt{clip}_C(v): v\in \mathbb{R}^p \mapsto \min\left\{1,  \tfrac{C}{\|v\|_2}\right\} \cdot v\in \mathbb{R}^p$ denote the clipping function which re-scales its input to have a maximal $\ell_2$ norm of $C$. For a minibatch $\mathcal{B}$ with $|\mathcal{B}|=B$ samples, the ""privatised"" minibatch gradient $\hat{g}$ takes on the form 
\begin{align*}
    \hat{g} =\frac{1}{B} \sum\limits_{i \in \mathcal{B}} \texttt{clip}_C \left(\nabla l_i (w) \right) + \frac{\sigma C}{B} \xi,
\end{align*}
with $\xi \sim \mathcal{N}(0, I_p)$ and $I_p\in\mathbb{R}^{p\times p}$ being the identity matrix. In practice, the choice of noise variance $\sigma>0$, batch-size $B$ and maximum number of training iterations are constrained by the predetermined privacy budget  $(\varepsilon, \delta)$. Crucially, the choice of hyper-parameters can have a large impact on the accuracy of the resulting model, and overall DP-SGD makes it challenging to accurately train deep neural networks. 
On CIFAR-10 for example, the highest reported test accuracy for a DP model trained with $\epsilon=8$ was 63.4\% in 2021 \citep{yu2021large}. \citet{de2022unlocking} improved performance on image classification tasks and in particular obtained nearly 95\% test accuracy for $\epsilon=1$ on CIFAR-10, using notably 1) pre-training, 2) large batch sizes, and 3) augmentation multiplicity. 
As part of this paper, we analyze to what extent these performance gains transfer from DP image classification to DP image generation.
Diffusion models are inherently different model architectures that exhibit different training dynamics than standard classifiers which makes introduces additional difficulties in adapting DP training. 
First, diffusion models are significantly more computationally expensive to train. 
Indeed, they operate on higher dimensional representations than image classifiers, so that they can output full images instead of a single label.
This makes each update step much more computationally expensive for diffusion models than for classification ones.
In addition, diffusion models also need more epochs to converge in public settings compared to classifiers. For example, for a batch size of 128 samples \citet{ho2020denoising} train a diffusion model for 800k steps on CIFAR-10, while \citet{zagoruyko2016wide} train a Wide ResNet for classification in less than 80k steps.
This high computational cost of training a diffusion model makes it difficult to finetune the hyperparameters, which is known to be both challenging and crucial for good performance \citep{de2022unlocking}.
Second, and related to sample inefficiency, the noise inherent to the training of diffusion models introduces an additional variance that compounds with the one injected by DP-SGD, which makes training all the more challenging. Thus overall, it is currently not obvious how to efficiently and accurately train diffusion models with differential privacy.
\section{Improvements towards Fine-Tuning Diffusion Models with Privacy}
\label{sec:method}
\paragraph{Recommendations from previous work.} \citet{de2022unlocking} identify pre-training, large batch sizes, and augmentation multiplicity as effective strategies in DP image classification. We adopted their recommendations in the training of DP diffusion models, and confirmed the effectiveness of their strategies to the task of DP image generation. In contrast to the work of \citet{dockhorn2022differentially}, where the batch-size is only scaled up to 2,048 samples, we implemented virtual batching which helps us to scale to up to 32,768 samples per batch. 
\paragraph{Pre-training.} Pre-training is especially integral to generating realistic image datasets, even if there is a considerable distribution shift between the pre-training and fine-tuning distributions. Unless otherwise specified, we thus pre-train all of our models on ImageNet32 \citep{chrabaszcz2017downsampled}. ImageNet has been a popular pre-training dataset used when little data is available \citep{raghu2019transfusion}, to accelerate convergence times \citep{liu2020towards}, or to increase robustness \citep{hendrycks2019using}.
\paragraph{Augmentation multiplicity with timesteps.} \citet{de2022unlocking} observe that data augmentation, as it is commonly implemented in public training, has a detrimental effect on the accuracy in DP image classification. Instead they propose the use of augmentation multiplicity \citep{fort2021drawing}. In more detail, they augment each unique training observation within a batch, e.g. with horizontal flips or random crops, and average the gradients of the augmentations before clipping them. Similarly to \citet{dockhorn2022differentially}, we extend augmentation multiplicity to also sample multiple timesteps $t$ for each mini-batch observation in the estimation of \autoref{eq:loss}, and average the corresponding gradients before clipping. In contrast to \citet{dockhorn2022differentially} where only timestep multiplicity is considered, we combine it with traditional augmentation methods, namely random crops and flipping. As a result, while \citet{dockhorn2022differentially} find that the FID plateaus for around 32 augmentations per image, we see increasing benefits the more augmentation samples are used (see Figure \ref{fig:augmult-sweep}). For computational reasons, we limit augmentation multiplicity to 128 samples.
\paragraph{Modified timestep sampling.}
The training objective for diffusion models in \autoref{eq:loss} samples the timestep $t$ uniformly from $[0, T]$ because the model must learn to de-noise images at every noise level.
However, it is not straightforward that uniform sampling is the best strategy, especially in the DP setting where the number of model updates is limited by the privacy budget.
In particular, in the fine-tuning scenario, a pre-trained model has already learned that at small timesteps the task is to remove small amounts of Gaussian noise from a natural-looking image, and at large timesteps the task is to project a completely noisy sample closer to the manifold of natural-looking images.
The model behavior at small and large timesteps is thus more likely to transfer to different image distributions without further tuning.
In contrast, for medium timesteps the model must be aware of the data distribution at hand in order to compose a natural-looking image.
A similar observation has been recently made for membership inference attacks \citep{carlini23extracting, mia-diff-23}: the adversary has been shown to more likely succeed in membership inference when it uses a diffusion model to denoise images with medium amounts of noise compared to high- or low-variance noised images. 
This motivates us to explore modifications of the training objective where the timestep sampling distribution is not uniform, and instead focuses on training the regimes that contribute more to modelling the key content of an image.
Motivated by this reasoning, we considered replacing the uniform timestep distribution with a mixture of uniform distributions $t\sim\sum_{i=1}^{K} w_i \mathcal{U}[l_i, u_i]$ where $\sum_{i=1}^Kw_i=1, 0\leq l_0, u_K\leq T$ and  $u_{k-1}\leq l_{k}$ for $k\in\{2,..., K\}$. On CIFAR-10, we found the best performance for a distribution with probability mass focused within $[30, 600]$ for $T=1,000$ where timesteps outside this interval are assigned a lower probability than timesteps within this interval. 
We assume this is due to ImageNet-pre-trained diffusion models being able to denoise other (potentially unseen) natural images if only a small amount of noise is added. Training with privacy for small timesteps can then decrease the performance because more of the training budget is allocated to the timesteps that are harder to learn and because of the noisy optimization procedure.
Even when training from scratch on MNIST, we observe that focusing the limited training capacity on the harder-to-learn moderate time steps increases test performance.
\section{Empirical Results on Differentially Private Image Generation\\ and their Evaluation}
\label{sec:finetuning}
\subsection{Current Evaluation Framework for DP Image Generators}
The FID \citep{heusel2017gans} has been the most widely used metric for assessing the similarity between artificially generated and real images \citep{dhariwal2021diffusion}, and has thus been widely applied in the DP image generation literature as the first point of comparison \citep{dockhorn2022differentially}. 
While the FID is the most popular metric, numerous other metrics have been proposed, including the Inception Score \citep{salimans2016improved}, the Kernel Inception Distance \citep{binkowski2018demystifying}, and Precision and Recall \citep{sajjadi2018assessing}. 
For the calculation of these metrics, the synthetic and real images are typically fed through an inception network that has been trained for image classification on ImageNet, and a distance between the two data distributions is computed based on the feature embeddings of the final layer.
Even though these metrics have been designed to correlate with the visual quality of the images, they can be misleading since they highly vary with image quality unrelated factors such as the number of observations, or the version number of the inception network \citep{kynkaanniemi2022role}. They also reduce complex image data distributions to single values that might not capture what practitioners are interested in when dealing with DP synthetic data. 
Most importantly, they may not effectively capture the nuances in image quality the further apart the observed data distribution is from ImageNet. For example, CIFAR-10 images have to be upscaled significantly, to be fed through the inception network which will then capture undesirable artifacts introduced by upsampling. In contrast, MNIST images are digit-based and thus exhibit other variations than natural images, further diminishing the effectiveness of the evaluation. 
An alternative way of comparing DP image generation models is by looking at the test performance of a downstream classifier trained on a synthetic dataset of the same size as the real dataset \citep{xie2018differentially, dockhorn2022differentially}, and tested on the real dataset. We argue that the way DP generative models are currently evaluated downstream, i.e. by evaluating a single model or metric on a limited data set, needs to be revisited. Instead, we propose to explore how synthetic data can be most effectively used for prediction model training and hyperparameter tuning.
This line of thinking motivates the proposal of an evaluation framework that focuses on \textit{how} DP generative models are used by practitioners. As such, our experiments focus on two specific use cases for private synthetic data: \textit{downstream prediction tasks}, and \textit{model selection}. \textit{Downstream prediction tasks} include classification or regression models trained on synthetic data and evaluated on real samples. This corresponds to the setting where a data curator aims to build a production-ready model that achieves the highest possible performance at test time while preserving the privacy of the training samples. \textit{Model selection} refers to a use case where the data curator shares the generative model with a third party that trains a series of models on the synthetic data with the goal to provide guidance on the model ranking when evaluated on the sensitive real data records.
We hope that with these two experiments we cover the most important downstream tasks and set an example for future research on the development of DP generative models. After presenting our results within the evaluation framework that is commonly used in the current literature, we investigate the utility of the DP image diffusion model trained on CIFAR-10 for the aforementioned tasks in \autoref{sec:model-select}.
\subsection{Experimental Setup}
We now evaluate the empirical efficiency of our proposed methods on three image datasets: MNIST, CIFAR-10 and Camelyon17.
Please refer to \autoref{tab:all-results} for an overview on our main results.
For CIFAR-10, we provide additional experiments to prove the utility of the synthetic data for model selection in \autoref{sec:model-select}.
We emphasize that while these benchmarks may be considered small-scale by modern non-private machine learning standards, they remain an outstanding challenge for image generation with differential privacy at this time.
\begin{table*}[b]
    \centering
    \caption{
    A summary of the best results provided in this paper when training diffusion models with DP-SGD. 
    We report the test accuracy of classifiers trained on different data sets. The highest reported current \textit{SOTA} corresponds to classifiers trained on DP synthetic data, as reported in the literature. Here, \citetalias{dockhorn2022differentially} refers to Appendix F Rebuttal Discussions in \citet{dockhorn2022differentially}, and \citetalias{harder2022differentially} to \citet{harder2022differentially}.  
    Our test accuracy (\textit{Ours}) denotes the accuracy of a classifier trained on a synthetic dataset that was generated by a DP diffusion model and is of the same size as the original training data. Note that we also report the model size of our generative models (\textit{Diffusion M. Size}). The \textit{\textcolor{gray}{Non-synth}} test accuracy corresponds to the test accuracy of a DP classifier trained on the real dataset, using the techniques introduced by \citet{de2022unlocking}. $^*$\citetalias{de2022unlocking} {This number is taken from \citet{de2022unlocking} for $\epsilon=8$.}
    }
    \label{tab:summary_results}
    \begin{tabular}{lccclllcc}
    \toprule
    \multirow{2}{*}{Dataset} & \multirow{2}{*}{Image} & \multirow{2}{*}{Diffusion} &\multirow{2}{*}{Pre-Training} & \multicolumn{3}{c}{Test Accuracy (\%)} & Privacy \\
    \cmidrule(lr){5-7}
    & Resolution &M. Size & Data & SOTA & Ours & \textcolor{gray}{Non-synth} & $(\epsilon,\delta)$ \\
    \midrule
    MNIST & $28\times28$ & 4.2M  & --   & 
    {98.1}
     $_{\text{\citetalias{dockhorn2022differentially}}}$
    & {98.6} & \textcolor{gray}{99.1} & (10, $10^{-5}$) \\
    CIFAR-10 & $32\times32\times3$ & 80.4M & ImageNet32 & {51.0}
    $_{\text{\citetalias{harder2022differentially}}}$
    & {88.0} & \textcolor{gray}{96.6 $^*_{\text{\citetalias{de2022unlocking}}}$} & (10, $10^{-5}$) \\
    Camelyon17   & $32\times32\times3$ & 80.4M & ImageNet32 & {-}& {91.1} & \textcolor{gray}{90.5} & (10, $3 \cdot 10^{-6}$) \\
    \bottomrule
    \end{tabular}
    \label{tab:all-results}
\end{table*}
We train diffusion models with a U-Net architecture as used by \citet{ho2020denoising} and \citet{dhariwal2021diffusion}. In contrast to \citet{dockhorn2022differentially}, we found that classifier guidance led to a drop in performance.
Unless otherwise specified, all diffusion models are trained with a privatized version of Adam \citep{mcmahan2018general,kingma2014adam}. The clipping norm is set to $10^{-3}$, since we observed that the gradient norm for diffusion models is usually small, especially in the fine-tuning regime. 
The privacy budget is set to $\epsilon=10$, as commonly considered in the DP image generation literature.
The same architecture is used for the diffusion model across all datasets. 
More specifically, the diffusion is performed over 1,000 timesteps with a linear noise schedule, and the convolutional architecture employs the following details: a variable width with 2 residual blocks per resolution, 192 base channels, 1 attention head with 16 channels per head, attention at the 16x16 resolution, channel multipliers of (1,2,2,2), and adaptive group normalization layers for injecting timestep and class embeddings into residual blocks as introduced by \citet{dhariwal2021diffusion}.
When fine-tuning, the model is pre-trained on ImageNet32 \citep{chrabaszcz2017downsampled} for 200,000 iterations.  
All hyperparameters can be found in \autoref{tab:params-diff}.
As a baseline, we also train DP classifiers directly on the sensitive data, using the training pipeline introduced by \citet{de2022unlocking}, and additionally hyperparameter tuning the learning rate. Please refer to \autoref{tab:params-dpwrn} for more details. It is not surprising that these results partly outperform the image generators as the training of the DP classifiers is targeted towards maximising downstream performance.
\subsection{Training from Scratch ($\emptyset \rightarrow$ MNIST)} 
The MNIST dataset \citep{lecun1998mnist} consists out of 60,000 $28\times28$ training images depicting the 10 digit classes in grayscale. 
Since it is the most commonly used dataset in the DP image generation literature, it is included here for the sake of completeness.
\paragraph{Method.} 
We use a DP diffusion model of 4.2M parameters without any pre-training, with in particular 64 channels, and channel multipliers (1,2,2).
The diffusion model is trained for 4,000 iterations at a constant learning-rate of $5\cdot 10^{-4}$ at batch-size 4,096, with augmult set to 128, and a noise multiplier of 2.852.
The timesteps are sampled uniformly within [0, 200] with probability 0.05, within [200, 800] with probability 0.9, and within [800, 1000] with probability 0.05.
To evaluate the quality of the images, we generate 50,000 samples from the diffusion model. 
Then we train a WRN-40-4 on these synthetic images (hyperparameters given in \autoref{tab:params-wrn}), and evaluate it on the MNIST test set.
\paragraph{Results.}
As reported in Table \ref{tab:all-results}, this yields a state-of-the-art top-1 accuracy of 98.6\%, to be compared to the 98.1\% accuracy obtained by \citet{dockhorn2022differentially}.
Crucially, we find that to obtain this state-of-the-art result, it is important to bias the timestep sampling of the diffusion model at training time: this allows the model to get more training signal from the challenging phases of the generation process through diffusion.
Without this biasing, we obtain a classification accuracy of only 98.2\%.
\subsection{Fine-tuning on a Medical Application (ImageNet32 $\rightarrow$ Camelyon17)} 
To show that fine-tuning works even in settings characterised by dataset shift from the pre-training distribution, we fine-tune a DP diffusion model on a medical dataset. Camelyon17 \citep{bandi2018detection, wilds2021} comprises 455,954 histopathological $96\times96$ image patches of lymph node tissue from five different hospitals. The label signifies whether at least one pixel in the center $32\times32$ pixels has been identified as a tumor cell. Camelyon17 is also part of the WILDS \citep{wilds2021} leaderboard as a domain generalization task: The training dataset contains 302,436 images from three different hospitals whereas the validation and test set contain respectively 34,904 and 85,054 images from a fourth and fifth hospital. Since every hospital uses a different staining technique, it is easy to overfit to the hues of the training data with empirical risk minimisation. At the time of writing, the highest accuracy reported in the leaderboard of official submissions is 92.1\% with a classifier that uses a special augmentation approach \citep{gao2022out}. The SOTA that does not fulfill the formal submission guidelines achieves up to 96.5\% by pre-training on a large web image data set and finetuning only specific layers of the classification network \citep{kumar2022fine}.
\paragraph{Method.} First we pre-train an image diffusion model on ImageNet32, before finetuning it with  $(10, 3\cdot 10^{-6})$-DP on the training data, downsampled to $32\times32$, with a batch size of 16,384 for 200 steps. We tuned the hyperparameters to achieve the lowest FID on the training data, and used the out-of-distribution validation data to tune the downstream classifiers. The timestep is sampled with 0.015 probability from [0, 30], with a probability of 0.785 in [30, 600], and with 0.2 in [600, 1000].
Since the diffusion model is pre-trained on ImageNet, we assume that the data is also available for pre-training the classifier. The pre-trained classifier is then further fine-tuned on a synthetic dataset of the same size as the original training dataset, which we find to systematically improve results. For both the classifier trained on augmented data where the augmentations include flipping, rotation and color-jittering.
\paragraph{Results.}
We achieve close to state-of-the-art classification performance with 91.1\% by training only on the synthetic data whereas the best DP classifier we trained on the real dataset achieved only 90.5\%. 
So while the synthetic dataset is not only useful for in-distribution classification, training on synthetic data is also an effective strategy to combat overfitting to domain artifacts and generalise in out-of-distribution classification tasks, as noted elsewhere in the literature \citep{zhou2020deep,gheorghitua2022improving}. 
\subsection{Fine-tuning on Natural Images (ImageNet32 $\rightarrow$ CIFAR-10)}
CIFAR-10 \citep{krizhevsky2009learning} is a natural image dataset of 10 different classes with 50,000 RGB images of size $32\times32$ during training time.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\paragraph{Method.} 
We use the same pre-trained model as for Camelyon17, that is an image diffusion model with more than 80M parameters trained on ImageNet32.
We tune the remaining hyperparameters by splitting the training data into a set of 45,000 images for training, and 5,000 images for assessing the validation performance based on FID. 
As for Camelyon17, we found that sampling the timestep with probability 0.15 in [0,30], with 0.785 in [30, 60] and 0.2 in [600, 1,000] gives us the lowest FID. More details can be found in \autoref{tab:params-diff}.
\paragraph{Results.}
We improve the SOTA FID with ImageNet pre-training \citep{harder2022differentially} from 26.8 to 9.8, which is a drop of more than 50\%, and increase the downstream accuracy from 51.0\% to 72.9\% without pre-training the classifier. With pre-training the classifier on ImageNet32, we can achieve a classification accuracy of 86.6\% with a single WRN. 
Modifying the timestep distribution led to a reduction in the FID from 11.6 to 9.8. 
Note that we achieved the results for different privacy levels by linearly scaling the number of iterations proportionally with $\epsilon$, and adjusting the noise level to the given privacy budget, keeping all parameters the same. 
As detailed in \autoref{fig:cifar10-eps-sweep}, we obtain state-of-the-art accuracy for a variety of privacy levels. 
Even for a budget as small as $\epsilon=1$, the FID obtained with our method is smaller than the current SOTA for $\epsilon=10$.
These results can also be compared with the very recent work by \citet{dockhorn2022differentially}, who report an FID of 97.7 when training diffusion models with differential privacy on CIFAR-10 without any pre-training. 
Due to the difficulty of the task of learning the diffusion model with differential privacy from scratch, the model did not learn to generate CIFAR-10 like samples, and the generated images do not seem to display any clear CIFAR-10 class instances at all.
We believe that such mixed results are a clear motivation for our proposed method of pre-training on public data, which make the learning problem significantly more tractable and realistic, and allows to obtain useful image generation.
To further confirm that the diffusion model has correctly learned the distribution shift, we trained a ResNet18 model to discriminate images from CIFAR-10 and ImageNet32 achieving a test accuracy of 98.0\% on that task. We then evaluated it on 50,000 synthetically generated images out of which 92\% were classified as CIFAR-10 images. This supports our hypothesis that the fine-tuned diffusion model does generate images that are more similar to CIFAR-10 than to the pre-training data of ImageNet.
\subsection{Maximizing Downstream Prediction Performance by Sampling Arbitrary Many Data Records\\ (ImageNet32 $\rightarrow$ CIFAR-10)}
\paragraph{Dataset sample size.}
One benefit of synthetic data generators is their ability to render infinitely many synthetic images. As such, there is no reason why the comparison of real and synthetic samples should be limited to predictive models trained on the same number of training samples. We, therefore, investigate whether the performance of a downstream predictor increases with more training images. 
In \autoref{fig:cifar10-num-samples}, we observe that the downstream classification accuracy constantly increases the more synthetic training observations are generated. In particular, we increase the downstream classification accuracy from 72.9\% to 86.0\% by sampling 1M instead of 50K images-- without pretraining the classifier. We note that this difference is much more significant on the more challenging dataset of CIFAR-10 than e.g. MNIST, where we find that increasing the number of samples offers virtually no benefit in terms of downstream accuracy.
We note that the classifier can also be pre-trained on the pre-training distribution for performance increases for smaller data set sizes. On 50k samples, we achieve a classification accuracy of 86.6\%. The predictive performance does not increase significantly when fine-tuning on more samples (see the appendix - \autoref{fig:numsample-pretrained}). The benefit of pre-training the downstream classification performance thus diminishes for 1M synthetic samples.
\paragraph{Ensembling.} We observe that we can further improve the classification accuracy given by a single WRN classifier by instead ensembling five different networks that differ only in the subsampling of the minibatches.
As reported in \autoref{tab:all-results}, we can achieve a test accuracy of 88\% on CIFAR-10 using this approach (see \autoref{fig:cifar10-num-samples}).
\section{Model Selection (ImageNet32 $\rightarrow$ CIFAR-10)}
\label{sec:model-select}
One important benefit of training a DP image generator over a DP classifier is the potential to use the generated data repeatedly for training a range of different prediction models and choosing the best one across them. Each experiment training a model on the data comes with a privacy cost, thus tuning a large number of DP classifiers increases the required privacy budget \citep{papernot2021hyperparameter}.
In this section we consider how synthetic data can be used to gain initial insights on the choice of model, and to reduce the number of experiments run on the sensitive data records. The goal here is to identify the model that performs best on real data, while only having access to synthetic data. This becomes particularly relevant and useful when synthetic data needs to be released for research purposes \citep{chambon2022adapting}, or data challenges \citep{de2014d4d,feuerverger2012statistical,mcfee2012million}.
For this purpose, we train 3 different model architectures that are commonly employed on CIFAR-10 \citep{de2022unlocking, bu2022scalable}: a WRN-28-8 \citep{zagoruyko2016wide}, a ResNet50 \citep{he2016deep}, and a VGG \citep{simonyan2014very}. For each architecture, we sweep over combinations of three to five different learning rates and three different values of weight decay. Please refer to \autoref{tab:params-model-select} for more details. 
We now assess the utility of synthetic data for model selection in two stages of increasing difficulty. 
First, we check whether models -- trained on the synthetic data -- rank similarly on the real and synthetic test data. This corresponds to the application setting where a third party tunes a model on the synthetic data, and releases a model that is trained on the same data.
Once we have established that the test performance on real and synthetic data is sufficiently correlated to ensure that a model ranks similarly no matter which data set it was evaluated on, we train each model separately on 50K real and on the same number of synthetic samples. In both cases, models are tested on the same source of data they have been learned on (with sources being real or synthetic here). We then assess whether the test performance on the real and synthetic data is still correlated between models of the same architecture and hyperparameter constellation. This corresponds to the setting where a third party is responsible for finding the best model pipeline, but the data curator trains the final model with the optimal hyperparameters for their own internal use.
\begin{figure*}
\centering
\subfloat[Test accuracy on synthetic data vs real data of models trained on synthetic data ]{\includegraphics[height = \textwidth/3]{figures/train-synth-eval-diff.pdf}}
\hspace{5mm}
\subfloat[Test accuracy of models trained and evaluated on synthetic data vs models trained and evaluated on real data]{\includegraphics[height = \textwidth/3]{figures/train-diff-eval-test.pdf}}
\caption{We observe that models rank similarly when evaluated on synthetic and real data. This suggests that findings on hyperparameter selection made on synthetic data can be transferred to the private dataset.}
\label{fig:cifar10-modelselect}
\end{figure*}
In \autoref{fig:cifar10-modelselect}, we report our findings. We see that we can select the best or second best hyperparameter constellation in both application settings. More generally, we find that models rank similarly on both synthetic and real data, suggesting that findings with respect to relative model performance might transfer from the DP data to the original real data. However, we also notice that models overfit to the synthetic data distribution and that within one model group it is not obvious which hyperparameter constellation is the best. We therefore advice that synthetic data is used for a high-level orientation of the research direction. Note that the absolute test performance is not of interest here, only the correlation between the test performance on real and synthetic data as the best model will be released externally. 
\section{Conclusion}
\label{sec:conclusion}
DP image generation has long attracted interest as a way of sharing synthetic data sets in sensitive application domains. Because of the degradation in performance introduced by DP-SGD, successful results on DP image generation have been limited to small and low-complexity data sets, like MNIST. In this paper, we set out to scale DP image generation to $32\times32\times3$ RGB image datasets. We proposed a methodology for DP diffusion models based on pre-training, timestep augmentation multiplicity, and a modified timestep sampling scheme. We are the first to train a DP image generator on a medical dataset where we achieved a downstream classification accuracy of 91.1\% that is close to the SOTA of 96.5\% with training on the real data. What is more, we also increased the SOTA downstream classification accuracy on CIFAR-10 from 51.0\% to 88.0\%. Recently proposed methods like latent diffusion models \citep{rombach2022high} constitute a promising model class for DP fine-tuning on higher dimensional datasets, and we hope that our findings can contribute to future research exploring this direction.
Finally, we questioned how DP synthetic data has been currently evaluated in the literature, and proposed an evaluation framework that is more suited to the needs of practitioners who would use the DP synthetic data as a replacement of the private dataset. For this purpose, we first considered maximising the downstream prediction performance by generating up to 1M data samples, and training ensembles. Second, we showed that findings from hyperparameter tuning on synthetic data translate to the corresponding findings on the real data. 
\section*{Acknowledgements}
The authors would like to thank Sylvestre-Alvise Rebuffi for feedback on an earlier version of this manuscript; Zahra Ahmed for project management support; and Judy Shen, David Stutz, Isabela Albuquerque, Simon Geisler, Arnaud Doucet, Taylan Cemgil and Pushmeet Kohli for useful discussions throughout the project. 
{abbrvnat}
\nobibliography*
\newpage
\onecolumn
\section*{Supplementary Materials}
\appendix
\section{Comparison to Related Work}
Please refer to \autoref{tab:compare-dokhorn} for a summary of details that differentiate our work from that of \citet{dockhorn2022differentially}. 
\begin{table}[h]
    \setlength\tabcolsep{4pt}
    \begin{center}
    \begin{small}
    \begin{tabular}{lccc}
    \toprule
     & \citet{dockhorn2022differentially} & \textbf{Ours}\\
    \midrule
    \midrule
    Analysed for finetuning & False & True \\
    \hline
    Maximal parameter count & 1.8M & 80.4M \\
    \hline
    Time step scale & discrete & continuous\\
    \hline
    Classifier guidance & True & False\\
    \hline
    Modifications to time step sampling & False & True \\
    \hline
    Augmentation strategies & time step & time step, random crop, flipping \\
    \hline
    Number of samples for augmentation multiplicity & 32 & 128 \\
    \hline
    Maximal batch size & 2,048 & 16,384\\
    \hline
    \multirow{3}{*}{Evaluation} & \multirow{3}{*}{FID, downstream accuracy} &  {FID, downstream accuracy,} \\
    &&maximal downstream performance\\
    &&hyperparameter tuning\\
    \bottomrule
    \end{tabular}
    \end{small}
    \end{center}
    \caption{Comparison of our work and \citet{dockhorn2022differentially}.}
    \label{tab:compare-dokhorn}
\end{table}
\section{Additional Experimental Details and Results}
\label{app:model-select}
Here we present additional experimental details to back up the findings presented in the main paper. We present our hyperparameters in Tables \ref{tab:params-diff}, \ref{tab:params-wrn}, \ref{tab:params-dpwrn} and \ref{tab:params-model-select}. Some of our DP synthetic images can be found in Figures \ref{fig:mnist}, \ref{fig:camelyon} and \ref{fig:cifar}. As a baseline, we also added samples from \citet{dockhorn2022differentially} when training from scratch on CIFAR-10 in \autoref{fig:dokhorn-cifar10}. This illustrates the importance of pre-training in the large scale DP image generation setting. Please refer to \autoref{fig:augmult-sweep} for an illustration of the utility of augmentation multiplicity.
Finally, we show with class-wise metrics in \autoref{fig:classmetrics} that our model is not overfitting to a single class.
\input{appendix/hyperparams}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\begin{figure*}
\centering
\subfloat[Percentage of samples per class classified as CIFAR-10 images by ResNet18 model trained to discriminate CIFAR-10 and ImageNet32 images.]{\includegraphics[width = 5\textwidth/5]{figures/fid-per-class.pdf}}
\hspace{5mm}
\subfloat[FID per class.]{\includegraphics[width = 5\textwidth/5]{figures/is_cifar-per-class.pdf}}
\caption{Class-wise metrics on CIFAR-10.}
\label{fig:classmetrics}
\end{figure*}
"
43,"\importpackages{}
\graphicspath{ {./images/} }


\title{Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making}
\author{Luke Guerdan}
\affiliation{
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\email{lguerdan@cs.cmu.edu}
\author{Amanda Coston}
\affiliation{
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\author{Zhiwei Steven Wu}
\affiliation{
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\author{Kenneth Holstein}
\affiliation{
  \institution{Carnegie Mellon University}
  \city{Pittsburgh}
  \country{USA}
}
\renewcommand{\shortauthors}{}
\begin{abstract}
A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve \khedit{decision-making}\khdelete{the quality of decisions}. \khedit{Research}\khdelete{Existing research} in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on ``ground truth’’ labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans \khdelete{commonly }reason about broader phenomena of interest in a decision – including latent constructs that are not directly observable, such as disease status, the ``toxicity'' of online comments, or future ``job performance'' – predictive models target \textit{proxy} labels that are readily available in existing datasets. Predictive models' reliance on simplistic proxies \khdelete{for these nuanced phenomena }makes them vulnerable to various sources of statistical bias. In \khedit{this paper}\khdelete{particular}, we identify five sources of \textit{target variable bias} that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a \khedit{causal framework to disentangle}\khdelete{causal diagram that disentangles} the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the \khdelete{experimental }designs of prior human subjects \khedit{experiments that investigate}\khdelete{studies investigating} human-AI decision-making, finding that only \khedit{a small fraction of studies}\khdelete{6 of 72 reviewed studies } examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future \khdelete{human-AI decision-making }research. 
\end{abstract}
\keywords{algorithmic decision support, measurement, validity, causal diagrams, label bias, human-AI decision-making}
\maketitle
\section{Introduction}
A growing body of research combines predictive machine learning models with human judgment to improve decision-making processes. This work often pursues the goal of \textit{complementary performance}: configurations of humans and models that yield higher-quality decisions than either would make in isolation. In the machine learning community, model-level improvements have been proposed to address gaps in human judgment (e.g., \citep{madras2018predict, wilder2020learning, tan2018investigating, hilgard2021learning}). In the human-computer interaction community, behavioral interventions have been developed to help \textit{humans} better incorporate model outputs into their decision-making (e.g., \citep{bansal2021does, buccinca2021trust}). \khdelete{
}However, current evaluations of complementary performance make the \textbf{key assumption that the label targeted by a predictive model adequately reflects the goals of human decision-makers}. \khedit{D}\khdelete{Specifically, d}ecision quality is frequently operationalized by predictive performance, measured via accuracy, AU-ROC, or similar statistical measures computed with respect to \textit{``ground truth”} \khdelete{or \textit{``gold standard”} }labels \khedit{that are readily available in existing}\khdelete{recorded in available} data. The relative performance of human judgment, model predictions, and hybrid combinations of the two are then ranked according to these metrics. Yet \khedit{such}\khdelete{these notions of decision quality hinge on the definition of \textit{``ground truth""} considered by the model. Consequently, } comparisons of human and model performance are only valid insofar as the \textit{``ground truth""} label targeted by the model reflects the underlying objectives of human decision-makers. 
\khedit{I}\khdelete{Yet i}n real-world human-AI decision-making settings, labels are often imperfect proxies for the target outcomes considered by decision-makers. While making decisions, humans frequently consider latent constructs such as the ``toxicity’' of an online comment, ``cardiovascular disease risk’’ of a patient, or future ``job performance’’ of a candidate. Because observed labels (e.g., toxicity annotations, diagnostic test results, and performance reviews) serve as indirect measurements of these phenomena \citep{jacobs2021measurement}, they can be subject to \textit{measurement error}. Additionally, humans often select among multiple possible actions (e.g., medical treatments, social welfare interventions) in hopes of improving a downstream outcome of interest. Because an outcome is only observed for the selected action, labeled data does not contain the counterfactual outcome that \textit{would} occur had a different option been chosen instead. This introduces a set of additional challenges, including selective labels \citep{lakkaraju2017selective}, intervention effects \citep{shalit2017estimating}, and selection bias, which interact with measurement error in nuanced ways depending on the nature of the decision-support task. We refer to this collection of challenges – which can be characterized as sources of statistical bias impacting labels – as \textit{target variable bias}.\footnote{The term Target Variable Bias was introduced in \citep{chouldechova2018case, fogliato2020fairness}. We use this as an umbrella term describing sources of statistical bias known to impact proxy labels in decision support tasks.}
Target variable bias has been widely documented in real-world deployments of algorithmic systems\khdelete{ (e.g., Table \ref{tab:evidence})}. Predictive models impacted by target variable bias have contributed to unwarranted firing of teachers \citep{chalfin2016productivity}, perpetuated historical disparities in access to medical resources \citep{obermeyer2019dissecting}, and raised concerns among social workers investigating allegations of child abuse and neglect \citep{kawakami2022improving, cheng2022disparities}. Surprisingly, existing modeling efforts and human subjects experiments in the human-AI decision-making literature have largely overlooked this challenge. Left unaddressed, this disconnect could undermine the ultimate goal of human-AI decision-making research: to develop algorithmic systems that meaningfully improve decision-making in real-world contexts. In this paper, we aim to bridge the gap between challenges encountered in real-world deployments of predictive models and current human-AI decision-making research practices by (i) raising awareness of target variable bias, (ii) identifying blind spots in previously published modeling approaches and human subjects experiments, and \lgedit{(iii)} providing guidelines for improved research practices going forward. 
Based on an examination of numerous real-world deployments of \lgedit{predictive models}, we construct a \khedit{causal framework that captures possible}\khdelete{\textit{causal diagram} \khedit{based framework that captures possible}}\khdelete{ representing the} data generating process\khedit{es in human-AI decision-making}\khdelete{ in ADS}. \khedit{Our framework}\khdelete{This diagram} (1) distills high-level structural differences between \khedit{various real-world }decision support tasks \khdelete{(e.g., content moderation, recidivism prediction, and child welfare hotline screening, among others) }and (2) disentangles which sources of bias are of concern in each setting. \khdelete{Informed by discussions with researchers across several disciplines, w}\khedit{W}e map our \khedit{framework}\khdelete{causal diagram} to existing terminology and methods used to characterize target variable modeling assumptions \khedit{across multiple disciplines}\khdelete{in learning sciences, biostatistics, psychology, and economics}. \khedit{Using our framework, we then}\khdelete{We use our framework to} identify strategies for better-addressing target variable bias through two lines of human-AI decision-making research:
\begin{itemize}
\item \textbf{Model development}. We develop a \textit{measurement} and \textit{prediction} decomposition that articulates target variable modeling assumptions. We use our decomposition to create a taxonomy of model-level improvements proposed in previous literature. We also propose a set of recommended measurement model evaluation strategies. 
\item \textbf{Experimental human subjects studies}. \khedit{Using our framework, we}\khdelete{We} re-examine the design of prior human subjects experiments studying human-AI decision-making\khdelete{ through the lens of our causal \khedit{framework}\khdelete{diagram}}. Our analysis identifies systematic blind spots in our current \khedit{understanding of human-AI decision-making}\khdelete{knowledge of algorithm-assisted \textit{human} decision-making} due to target variable bias.
\end{itemize}
\section{Related work}
We begin by introducing the body of human-AI decision-making research our framework is designed to inform. We then summarize modeling challenges and broader validity concerns that draw current \khdelete{human-AI decision-making }research practices (i.e., modeling assumptions, experimental study designs, and measures of decision quality) into question. 
\subsection{Human-AI decision-making}
Recent machine learning research proposes \lgedit{techniques} \lgdelete{designed} to complement the limitations of human judgement. Drawing from a long line of work showing that actuarial risk assessments can outperform expert judgement in many prediction tasks \citep{grove2000clinical, dawes1989clinical}, methods have been proposed that learn to complement humans by adaptively routing decision instances \citep{madras2018predict, gao2021human}, leveraging heterogeneity in human and machine decision performance \citep{tan2018investigating, wilder2020learning, donahue2022human, charusaie2022sample}, leveraging consistency in expert decisions \citep{de2021leveraging}, and adapting to \citep{hilgard2021learning} and training \citep{mozannar2022teaching} human mental representations of model outputs. Yet \lgedit{these techniques} operate on a set of \textit{simplifying assumptions} about the world, which may or may not hold in a given deployment context. We provide a framework for articulating modeling assumptions, and show that many \lgdelete{common} assumptions made by prior work involving proxy labels are unlikely to hold in practice. Recent research has also studied opportunities for human-AI complementary in algorithm-assisted \textit{human} decision-making \citep{lai2019human, green2019principles, buccinca2021trust, gajos2022people, cai2021onboarding, lai2020chicago}. This work investigates the potential for tools such as training protocols \citep{cai2021onboarding, lai2020chicago, cai2019hello}, explanations \citep{liu2021understanding, bussone2015role}, and other behavioral interventions \citep{buccinca2021trust, gajos2022people}, to improve how humans make use of model outputs. While many online experimental studies have been focused on interventions to improve predictive performance, little work to date has experimentally studied other key factors that are present in real-world deployment contexts, such as asymmetric access to information \citep{holstein2022toward, hemmer2022effect}, measurement error \citep{gordon2022jury}, and omitted payoffs \citep{green2021algorithmic}.
\subsection{Modeling challenges in algorithmic decision support}\label{subsec:modeling_challenges}
\lgedit{Prior} work has surfaced a litany of challenges impacting predictive models \lgedit{designed for} algorithmic decision support \lgedit{(ADS)}, including unobservables \citep{kleinberg2018human}, selective labels \citep{lakkaraju2017selective}, selection bias \citep{singh2021fairness, de2018learning}, and intervention effects \citep{coston2020counterfactual}. Additional work has examined the quality of proxy labels in decision support tasks. \lgedit{For example,} \citet{obermeyer2019dissecting} surfaced \textit{``label choice bias''}, in which racial disparities in access to health resources were introduced by poor \khedit{label} selection decisions. \textit{``Omitted payoffs bias''} \lgdelete{has been used to} describes factors of interest to humans that are incompletely reflected by predictive models targeting available labels \citep{de2021leveraging, chalfin2016productivity, kleinberg2018human}. While this bias describes challenges specific to prediction (e.g., model unobservables, measurement error \citep{kleinberg2018human}), this term also applies when humans care about a broader set of decision-making factors beyond \lgedit{predictive risk} \citep{chalfin2016productivity, green2019principles}. In this work, we \lgedit{use} the \khedit{lens}\khdelete{framework} of \textit{measurement\khdelete{ error}} \khdelete{and \textit{validity} }\lgedit{to examine} systematic differences between target outcomes of interest to humans and proxy labels observed in data \citep{jacobs2021measurement}. \khedit{In adopting this lens, we draw upon}\khdelete{We use this framework because it draws upon} a rich set of existing knowledge and methodologies from adjacent disciplines (e.g., psychology, political science, sociology) designed to evaluate how latent phenomena of interest to humans are quantified in data \citep{roberts1985measurement}.
\subsection{Measurement and validity in algorithmic systems}
Recent work has raised broader concerns regarding whether algorithmic systems successfully achieve their purported function \citep{jacobs2021measurement, bao2021s, coston2022validity}. Synthesizing concepts from measurement theory in the quantitative social sciences, \citet{jacobs2021measurement} argue that \textit{``algorithmic fairness''} is a latent construct that is imperfectly operationalized by statistical fairness measures. \citet{bao2021s} examine statistical biases present in criminal justice datasets (e.g., ProPublica's COMPAS Dataset \citep{angwin2016machine}) used in fairness benchmarks of algorithmic Risk Assessment Instruments (RAIs). This analysis identified several biases in the \textit{outcome variable} $Y$ predicted by models, which we further characterize in this work. \citet{coston2022validity} also highlight a host of validity concerns impacting RAIs, including many discussed in Section \ref{subsec:modeling_challenges}. Recent work has also surfaced validity issues \lgedit{in} content moderation \citep{gordon2022jury} and recommender systems \citep{milli2021optimizing, stray2022building}. 
Despite \khedit{this }growing awareness\khdelete{of the importance of measurement and validity in algorithmic systems}, we currently lack a holistic understanding of validity threats to prediction targets in human-AI decision-making. Addressing this gap is critical for preventing algorithmic harms in real-world deployment contexts\khdelete{ (e.g., Table \ref{tab:evidence})}. Therefore, in this work, we \khdelete{use causal diagrams to examine the relationship between measurement error and additional modeling challenges (i.e., Section \ref{subsec:modeling_challenges}) that impact the validity of prediction targets in real-world decision support settings. To our knowledge, our work offers}\khedit{offer} the first holistic \khedit{examination}\khdelete{discussion} of how measurement error, unobservables, selection bias, and treatment effects interact to impact target variable validity in real-world ADS deployments.
\section{Framework}\label{sec:framework}
We now describe our framework scope (\lgedit{Section} \ref{sec:scope}) and development process (Section \ref{sec:process}) before introducing our causal diagram (\lgedit{Section} \ref{sec:causal_diagram}). We then use our framework to \lgedit{identify structural differences between different ADS tasks (Section \ref{sec:task_specific_structures}) and} disentangle sources of target variable bias (Section \ref{sec:challenges}).
\subsection{Scope}\label{sec:scope}
Our framework \lgedit{applies to settings in which a supervised learning model is introduced to augment human decision making by predicting} (i) a future event (e.g., medical \citep{obermeyer2019dissecting}, criminal justice \citep{dieterich2016compas}, child welfare \citep{chouldechova2018case}, or real estate \citep{holstein2022toward, poursabzi2021manipulating} related outcomes); (ii) a subjective human annotation (e.g., perceived content toxicity \citep{gordon2022jury}); or (iii) factual information (e.g., food nutrition \citep{buccinca2021trust}). In these settings, model predictions are combined with human decision-making, either by showing \khdelete{model }predictions to a human (i.e., algorithm-in-the-loop \citep{green2019principles}), who makes the final decision, or via a \khdelete{more nuanced }hybrid flow of \khedit{agency}\khdelete{autonomy} (e.g., deferral-based learning \citep{madras2018predict}, learning with bandit feedback \citep{gao2021human} \lgdelete{, or conditional delegation \citep{lai2022human}}). Given our focus on prediction-based decision tasks, we do not directly examine decision-support settings involving unsupervised learning (e.g., clustering), tasks relying upon \textit{generative models} (e.g., text or image generation), or sequential settings with time \lgedit{dependency} \lgdelete{-dependent target variables} (e.g., reinforcement learning) in this work. 
\subsection{Framework development}\label{sec:process}
\lgdelete{Prior studies of real-world ADS deployments have surfaced numerous examples of target variable bias impacting predictive models \citep{kleinberg2018human, fogliato2021validity, bao2021s, kawakami2022improving, obermeyer2019dissecting, mullainathan2017does, chalfin2016productivity}. Yet, these challenges have been acknowledged to varying degrees in prior modeling work (i.e., Table \ref{tab:modeling_table}) and they are rarely examined in \lgedit{experimental  human-AI decision-making studies} (i.e., Figure \ref{fig:experimental_studies_dag}). A key barrier to better understanding and addressing these challenges is that we currently lack a holistic view of the statistical biases that \lgdelete{can} impact target variables. A second barrier arises because different ADS tasks \lgdelete{(e.g., content moderation, forest cover prediction, heart attack prediction)} are subject to distinct \lgedit{sources of target variable bias} \lgdelete{sets of challenges}. \lgedit{For instance,} tasks involving \lgedit{factual information prediction} \lgdelete{the prediction of factual information} (e.g., food \lgedit{nutrition} \citep{buccinca2021trust}) introduce a \khedit{narrower}\khdelete{\lgedit{more narrow}}\lgdelete{different} set of challenges than those involving the prediction of \lgdelete{subjective or} socially contested outcomes (e.g., child welfare outcomes \cite{chouldechova2018case}).} Understanding which statistical biases are of concern in a given ADS task requires \textit{examining the historical data generating process} that gave rise to the model training dataset. Causal diagrams\lgedit{, which are} \khedit{graphs}\khdelete{Directed Acyclic Graphs (DAGs)} that show causal relationships between nodes via connected edges \citep{pearl1995causal}\lgedit{, are tools specifically designed for this purpose}. A \khedit{causal diagram}\khdelete{DAG} shows each variable being modeled as a discrete \textit{node}. Causal connections between nodes are shown via \textit{edges}. If the direction of a causal pathway is known, this is shown via a directed arrow from the parent to child node. An undirected edge is used to connect nodes when the causal direction is unknown or varies \citep{pearl1995causal}. Our framework \khedit{constructs a causal diagram to} examine\khdelete{s} challenges impacting the labels available in data. Therefore,\khdelete{in our causal diagram,} we specifically consider variables (i.e., \textit{nodes}) and relationships (i.e., \textit{edges}) that directly relate to the target variable\khedit{; we \textit{abstract away}}\khdelete{. As such, our diagram \textit{abstracts away}} other \khedit{important}\khdelete{key} factors\khdelete{ that can vary across contexts}, such as the training \citep{cai2021onboarding, lai2020chicago, cai2019hello}, decision-making process \citep{green2021algorithmic}, and workflow \citep{green2019principles} of the human \khedit{decision-makers}\khdelete{experts} using the predictive model. While prior work has examined these factors in detail \citep{cai2021onboarding, lai2020chicago, cai2019hello, green2021algorithmic, green2019principles}, our \khedit{framework}\khdelete{diagram} foregrounds factors most salient for \lgdelete{understanding} target variable bias. In Section \ref{subsec:scaffolding}, we outline how our approach can be extended to systematically examine a broader set of components beyond target variables in ADS research. 
\begin{figure*}
  [width=.55\textwidth]{fig/ads_dag.pdf}
  \caption{Our causal diagram represents a space of causal graphs, spanning different possible relationships between predictors, decisions, target variables, and their proxies in human-AI decision-making settings. Edges with directionality that can vary across different real-world human-AI decision settings are indicated via undirected edges. Observed variables are shown with solid lines, while unobserved variables are shown in dotted lines. An arrow pointing to a shaded box is shorthand for separate arrows pointing from the source to nodes contained within the box.} 
  \label{fig:dag}
\end{figure*}
Our causal diagram was developed and refined through an iterative series of discussions among the authors and external researchers spanning a range of disciplines. Based on a review of real-world case studies \khedit{(see Appendix A)}\khdelete{(i.e., Table \ref{tab:evidence})}, we synthesized candidate \khedit{causal diagrams}\khdelete{models} that could adequately characterize the target variable of interest across settings, and then stress-tested these \khedit{diagrams}\khdelete{models} by attempting to identify counterexamples. Through our discussions with external researchers, we also cross-referenced our framework with existing terminology and methods developed in adjacent disciplines, such as medical diagnostic testing, educational assessment, behavioral health, and statistics. 
\subsection{Causal diagram}\label{sec:causal_diagram}
\subsubsection{Diagram structure} \lgedit{Figure \ref{fig:dag} shows our proposed causal diagram, which represents \textbf{a space of directed acyclic graphs (DAGs)} describing different possible relationships between predictors, decisions, target variables, and their proxies in human-AI decision-making settings.} 
\textbf{Predictors.} $X$ describes \textit{covariates} used to generate model predictions. Covariates are often drawn from administrative data sources (e.g., medical records, lending history) \lgdelete{that are} available to an organization for model development. In ADS settings, humans can also make use of \textit{unobserved contextual information} $Z$ while making decisions. For example, a physician might consider real-time medical test results (e.g., electrocardiograms \citep{mullainathan2019machine}) unavailable to a model, while a social worker might weigh contextual factors described via phone calls while deciding whether to recommend investigation of child maltreatment allegations \cite{kawakami2022improving}. In some cases, human decision-makers can also be unaware of a subset of covariates (e.g., due to organizational policy or prohibitively large datasets) \citep{holstein2022toward}. Figure \ref{fig:dag} refers to $X$ and $Z$ as \textit{model observables} and \textit{model unobservables}, respectively, based on whether the predictors are available to a model.
\textbf{Decisions.} The blue shaded box in Figure \ref{fig:dag} shows the joint human-algorithm decision $D$. We decompose this node into separate variables for human decisions ($D_H$) and algorithm predictions ($D_A$). Prior to deployment of an algorithm, decisions result solely from human judgement $D_H$. In some cases, decisions post-deployment result from humans incorporating predictions into their decision-making (i.e., algorithm-in-the-loop \citep{green2019principles}). In other cases, the joint decisions result from a learned combination of $D_H$ and $D_A$ \citep{gao2021human, madras2018predict, wilder2020learning, tan2018investigating, hilgard2021learning, charusaie2022sample, donahue2022human}. 
\textbf{Target variables.} The node $Y^*$ describes the unobserved target outcome of interest to human decision-makers. For example a model might be introduced to weigh risk of unobserved constructs such as ``medical need'' , ``recidivism'', ``creditworthiness'', or ``job performance.'' $Y$ describes the \textit{observed proxy} that is targeted by a model in place of $Y^*$. For example, a model might predict ``cost of medical care'' \citep{obermeyer2019dissecting}, ``re-arrest'' \citep{fogliato2020fairness}, ``loan default'', or ``supervisor performance reviews'' in place of the targets listed previously. The grey box in Figure \ref{fig:dag} represents a \textit{measurement model} mapping the unobserved construct to the observed proxy targeted by a predictive model (see Section \ref{challenge:outcome_measurement_error}).
\textbf{Edges}. \textit{We now describe the space of possible \khedit{relationships}\khdelete{node connections} across ADS tasks, before narrowing in on several special cases in Section \ref{sec:task_specific_structures}}. Across ADS tasks, covariates and unobservables both contribute to human decisions $D_H$, while algorithmic predictions $D_A$ are only influenced by covariates $X$. For example, a physician might make use of medical records ($X$) and real-time test results ($Z$), while an algorithm only has access to medical records ($X$). We show these relationships via directed arrows $X \rightarrow D$ and $Z \rightarrow D_H$. Decisions $D$ can also influence the target \textit{and} proxy outcomes $Y^*$ and $Y$ (e.g., as when medical treatments impact health status ($Y^*$) and cost ($Y$)). The direction of the causal relationship between covariates ($X$), unobservables ($Z$), and prediction targets ($Y$ and $Y^*$) can vary across specific ADS domains. We convey ambiguity in the direction of causation via undirected edges and describe this nuance below.
\subsection{Special cases of our causal diagram}\label{sec:task_specific_structures} 
The generalized causal diagram we propose in Figure \ref{fig:dag} can be used to articulate key structural differences between decision support tasks studied in human-AI decision-making literature. Identifying the task-specific diagram applicable to a given decision support setting is critical for (1) identifying relevant sources of target variable bias (Section \ref{sec:challenges}), (2) articulating modeling assumptions (Section \ref{sec:modeling}), and (3) designing ecologically valid \lgdelete{experimental} studies (Section \ref{sec:experimental_evaluations}). 
\subsubsection{Decision-dependent target variables} A decision support task contains decision-dependent target variables when \textit{the decision informed by an algorithm also impacts the downstream outcomes $Y$ and $Y^*$}. Real-world deployments of risk assessments often contain decision-dependent targets. For example, re-arrest is only observed among defendants released on bail \citep{kleinberg2018human}. Child welfare screening decisions influence the underlying risk of child maltreatment, in addition to observed proxies (i.e., placement in foster care) \citep{coston2020counterfactual}. More generally, real-world decisions informed by algorithms often constitute \textit{risk mitigating interventions} (e.g., medical treatments, educational programs) or \textit{opportunities} (e.g., loans, new candidate hires) that change the likelihood of the target outcome (e.g., disease prognosis, educational attainment). Settings with decision-dependent outcomes contain the orange arrow from $D$ to $Y$ and $Y^*$ shown in Figure \ref{fig:dag}.
\begin{figure*}[t]
  [width=\linewidth]{fig/sources_of_bias_horz.pdf}
  \caption{Sub-graphs of the diagram in Figure \ref{fig:dag} introducing statistical biases that impact the target variable $Y^*$. \textit{Outcome measurement error} (\textbf{A}) can occur in settings with both decision-dependent and independent target variables. In decision-dependent settings, \textit{intervention effects} (\textbf{B}), \textit{selection bias} (\textbf{C}), \textit{selective labels} (\textbf{D}), and \textit{confounding} (\textbf{E}) are also of concern. }
  \label{fig:challenges}
\end{figure*}
\subsubsection{Decision-independent target variables} In contrast, the target variable is \textit{not} influenced by the proposed decision in \textit{decision-independent} ADS tasks. While this setting is uncommon in real world deployments of algorithmic systems, \lgdelete{experimental} human-AI decision-making studies often adopt tasks with decision-independent target variables. For instance, studies have examined models that predict factual content (e.g., food nutrition \citep{buccinca2021trust}) and perceptual information (e.g., counts of objects \citep{park2019slow}, geometric shapes \citep{zhang2022you}). In these settings, the prediction target (i.e., food nutrition, \lgdelete{object quantity,} geometric shape) is \textit{not} influenced by the prediction made by a human and/or model. Therefore, settings with a decision-independent target variable \textit{do not contain} the arrow from $D$ to $Y$ and $Y^*$ in Figure \ref{fig:dag}.
\subsubsection{Subjective annotation} In some contexts, the target variable in question is a latent construct (e.g., toxicity or emotional state) operationalized by \textit{subjective human annotations}. Here, human annotators serve as a \textit{measurement model} linking the target construct (e.g., toxicity, hate speech) to a proxy (i.e., individual annotation) \citep{jacobs2021measurement}. Because humans can disagree considerably on subjective tasks \citep{gordon2022jury}, labels elicited from specific individuals ($Y$) are an imperfect proxy for the broader construct of interest ($Y^*$). In subjective human annotation tasks, $D$  constitutes decisions made at \textit{runtime} (i.e., after deployment of system predicting the target construct). For example, these decisions can involve removal of content flagged as ``toxic'' or reversal of ``damaging'' Wikipedia edits \citep{halfaker2020ores, kumar2021designing}.
\subsubsection{Considerations of reverse causality}\label{subsub:reverse_causality} Traditional notions of prediction hold that covariates ($X$) and unobservables ($Z$) contribute to downstream outcomes ($Y$) and ($Y^*$) via a domain-specific causal pathway \citep{fairmlook}. In our diagram, this flow of information would be communicated via edges from $X$ to ($Y, Y^*$), and from $Z$ to ($Y, Y^*$). However, in some cases, the causal pathway can be \textit{reversed} \citep{hardt2022backward}. For example, this is possible if a patient's unobserved disease status ($Y^*$) contributes to their medical history ($X$) and real-time test results ($Z$). This ambiguity can also exist for target and proxy outcomes. Therefore, we communicate this domain-specific variation in causal directionality via undirected edges in Figure \ref{fig:dag}. \footnote{In order for the causal diagram to remain valid (i.e., a directed \textit{acyclic} graph), one of the edges connecting nodes must remain disconnected in these settings (i.e., when target variables are decision-dependent and $Y^* \rightarrow X$, $Y \rightarrow X$, $Y^* \rightarrow Z$, or $Y \rightarrow Z$). While this requirement is consistent with the scope of our framework, which considers non-sequential settings, feedback loops are an important factor to consider in sequential settings \cite{ensign2018runaway}.}
\subsection{Sources of target variable bias}\label{sec:challenges}
We now use our causal diagram (Figure \ref{fig:dag}) to examine five sources of target variable bias that can threaten the validity of prediction targets. Critically, \textit{our analysis shows that tasks with decision-dependent outcome variables introduce a broader set of challenges than tasks with decision-independent outcomes}. This suggests that greater care may be required during model development (Section \ref{sec:modeling}) and experiment design (Section \ref{sec:experimental_evaluations}) in decision-dependent outcome tasks.
\subsubsection{Outcome measurement error}\label{challenge:outcome_measurement_error}
Human \lgedit{experts and policy-makers} often make decisions involving unobserved, latent constructs such as \textit{``recidivism risk''}  and \textit{``job performance.''} These latent constructs are not directly observable in the world, but can be operationalized via a \textit{measurement model} \citep{hand2004measurement, jacobs2021measurement}. Adopting a label observed in \lgdelete{historical} data as a proxy for an unobserved latent construct serves as a \textit{de facto} measurement model. For instance, in criminal justice settings, defendant re-arrest is commonly adopted as a proxy for recidivism risk \citep{fogliato2021validity, bao2021s}, while in commercial hiring settings, manager reviews are frequently adopted as a proxy for future job performance.  Outcome measurement error (Figure \ref{fig:challenges}.A) occurs when there is a systematic difference between the target variable of interest to \lgedit{experts and policy-makers} \lgdelete{human decision-makers} ($Y^*$) and its operationalization by a proxy ($Y$). \lgdelete{One notable case of measurement error occurred in the medical domain, where a \lgdelete{widely-deployed} clinical decision support tool intended to inform physician decision-making used \textit{``cost of medical care''} as a proxy for patient medical need \citep{obermeyer2019dissecting}. However, due to systemic disparities in healthcare access, Black patients historically received less care (and thus had lower medical costs) than similarly sick white patients. As a consequence, these patients were turned away from much-needed medical resources.} This challenge has been extensively documented in \lgdelete{other ADS deployments in} judicial \citep{fogliato2020fairness, butcher2022racial}, child welfare \citep{kawakami2022improving, cheng2022disparities}, and hiring \citep{chalfin2016productivity} \lgedit{ADS} domains. 
Critically, \textbf{proxy labels impacted by measurement error offer an incomplete reflection of the actual goals of human decision-makers, and therefore serve as an incomplete measure of human-AI decision quality}. Therefore, before adopting a proxy \khedit{to evaluate}\khdelete{as a measure of decision quality in} human-AI decision-making\khdelete{research}, it is critical to assess whether it serves as a satisfactory approximation of the target variable of interest. Measurement theory in the quantitative social sciences provides tools to conduct this assessment by weighing the \textit{construct validity} and \textit{\khdelete{construct }reliability} of observed labels \citep{hand2004measurement, jacobs2021measurement} (see Section \ref{subsec:construct_validity}). \lgdelete{ Establishing construct validity and reliability requires evaluating whether an operationalization wholly and consistently represents the latent construct it is purported to measure. This evaluation requires a holistic assessment of numerous converging sources of evidence.} \lgedit{In practice,} measurement error in proxies is often studied via  \textit{measurement error models}. These models make \textit{assumptions on the relationship} between the target outcome ($Y^*$) and its proxy ($Y$) \khedit{(see Appendix B)}.
\subsubsection{Intervention effects}\label{sec:treatment}
\lgedit{In many ADS tasks, decisions serve as \textit{risk mitigating interventions} intended to improve the chances of a favorable policy-relevant outcome \citep{barabas2018interventions, kube2019allocating, coston2020counterfactual}. As a result, past human decisions $D_H$ influence the probability of the target outcome $Y^*$ and its proxy $Y$ (Figure \ref{fig:challenges}.B). However, many existing predictive \lgedit{techniques} mistakenly assume that decisions $D$ and outcomes $Y$, $Y^*$ are statistically independent \citep{barabas2018interventions, kube2019allocating, coston2020counterfactual}. This practice can be traced back to formulation of ADS as a prediction-policy problem \citep{kleinberg2015prediction}, in which models are trained to maximize predictive performance with respect to observed outcomes without considering causal effects from $D$ to $Y$ and $Y^*$. Yet, we argue that accounting for the causal connection between decisions and outcomes is of central interest in many ADS tasks. For instance, consider two distinct policy problems that arise in tasks with decision-dependent target variables:}
\begin{itemize}
    \item \textbf{Selective Intervention (SI):} In \lgedit{this policy setting}\lgdelete{\textit{selective intervention} (SI) settings}, organizations provide resources to individuals who are at \textit{high baseline risk under no intervention}. For example, developers of the Allegheny Family Screening Tool (AFST) introduced the tool with the goal of assessing \textit{``latent risk''} of maltreatment prior to county child welfare interventions \citep{vaithianathan2019allegheny}. Similarly, predictive models have been introduced in educational settings to identify students  at-risk of failing given no tutoring resources \citep{smith2012predictive}. \lgedit{This task requires causal inference because it involves inferring what \textit{would occur} if an individual does not receive the proposed intervention.} \lgdelete{ of {predicting} estimating risk under the \textit{hypothetical scenario} in which the decision subject does not receive the intervention under consideration.}
    
    \item \textbf{Selective Opportunity (SO):} In \lgedit{this policy setting}, \lgdelete{\textit{selective opportunity} (SO) settings,} an organization grants an opportunity (e.g., a new loan, or pre-trial release on bail) to decision subjects while trying to minimize risk of an adverse outcome (e.g., loan default, recidivism) given an individual receives the opportunity.  \lgdelete{Typically, decision subjects are granted the opportunity when the risk under the proposed opportunity is deemed acceptable when balanced with other competing factors \citep{green2021algorithmic}.} This prediction task \lgedit{requires causal inference because it involves predicting what would occur under the \textit{hypothetical scenario} that an individual receives the opportunity under consideration}.  \lgdelete{also amounts to a causal inference challenge of predicting risk \lgdelete{($Y^*$)} under the \textit{hypothetical case} that an individual receives the opportunity under consideration \lgdelete{ ($D=1$)}.}
\end{itemize}
\lgedit{Naively structuring an ADS task as a prediction policy problem in SI and SO settings}\lgdelete{In SI and SO settings where intervention effects are at play,naively framing structuring a decision-making task as a prediction policy problem} can lead to misleading assessments of model performance. For example, \citet{coston2020counterfactual} demonstrate that predicting observational modeling and evaluation in SI settings \lgdelete{by treatment effects}systematically underestimates the risk for high-risk individuals who would respond most favorably to the intervention. \lgdelete{In these settings, counterfactual evaluations and metrics such as policy risk \citep{athey2015machine} provide a more robust evaluation of model performance than naively assessing accuracy via observational labels. Accounting for treatment effects in predictive modeling is under active investigation in causal inference \citep{johansson2020generalization} and performative prediction literature \citep{perdomo2020performative}.} The underlying modeling and evaluation challenge introduced by intervention effects stems from the fact that \lgdelete{in decision-dependent outcome settings,} downstream outcomes are only observed under one of the possible decisions \citep{pearl2009causal, perdomo2020performative, johansson2020generalization}. This challenge is closely related to an additional source of target variable bias: \textit{selective labels} \citep{lakkaraju2017selective}.
\subsubsection{Selective labels}\label{sec:selective_labels}
Another challenge introduced by the edge from $D$ to $Y$, $Y^*$ in Figure \ref{fig:challenges} is \textit{selective labels}. This bias has been widely discussed in connection to pre-trial risk assessments, where recidivism-related proxy outcomes (e.g., re-arrest\lgdelete{, failure to appear}) are only observed among defendants released on bail \citep{kleinberg2018human, fogliato2021validity, bao2021s, lakkaraju2017selective}. Selective labels also occur in child welfare settings, in which \lgedit{some} outcomes (e.g., placement in foster care\lgdelete{, acceptance for welfare services}) are only observed among cases screened-in for investigation \citep{de2018learning}. \lgdelete{In fact, the} Selective labels maps directly to \textit{selective intervention} and \textit{selective opportunity} policy problems \lgdelete{introduced in Section \ref{sec:treatment}. This is} because we never observe how an individual \textit{would have} benefited from a missed opportunity (SO), or how an intervention \textit{would have} impacted \lgdelete{the risk for} an individual who historically received no additional resources. \lgedit{Selective labels pose the greatest challenge} when \textit{selection bias} was also present in the historical data generating process. 
\subsubsection{Selection bias}\label{sec:selection_bias}
This bias, which occurs when features ($X$) or unobservables ($Z$) influenced past decisions ($D$) (Figure \ref{fig:challenges}.C), \lgedit{complicates selective labels and intervention effects}. Because a previous decision-making policy may have been more likely to intervene (SI) or grant opportunities (SO) to some \lgedit{sub-populations, these groups may} be systematically over- or under- represented in historical outcome data. As a result, ADS models trained on historical data will not perform equally well on all sub-populations during deployment \citep{berk1983introduction}. This effect has been well-documented in recidivism prediction settings, in which models predicting re-arrest outcomes have worse performance among \lgedit{sub-populations historically denied bail} \citep{kallus2018residual}. While re-weighting techniques can correct for selection bias \citep{swaminathan2015batch}, \lgedit{these approaches typically assume that} no model unobservables $Z$ \lgedit{are present, which can be unreasonable in many ADS domains }\lgdelete{(discussed in the following section)}. \textit{The connection between selection bias and other downstream issues (e.g., intervention effects, selective labels) underscores the importance of considering the full \textit{data generating process} while diagnosing sources of bias impacting proxy labels.}
\subsubsection{Confounding bias}\label{sec:confounding} 
In causal inference settings, confounding bias occurs when unmeasured variables influence both the treatment and outcome variable \citep{pearl1995causal}. \lgedit{Confounding impacts ADS tasks} when unobservables $Z$ influenced past decisions $D$ and downstream outcomes $Y^*$ and $Y$ (Figure \ref{fig:challenges}.E) \citep{pearl1995causal}. When confounding impacts ADS models \lgdelete{in an ADS task}, it is not possible to \lgedit{fully mitigate} \lgdelete{adequately account for} treatment effects and selective labels via traditional causal inference techniques \citep{pearl2009causal}. Yet, confounding is \textit{not} introduced by model unobservables $Z$ in decision-independent settings because there is no arrow from decisions $D$ to outcomes $Y$ and $Y^*$. In these settings, unobservables may serve as an \textit{opportunity for complementarity} between humans and models arising from asymmetric access to information \citep{holstein2022toward, hemmer2022effect}. This differential role of \lgedit{unobservables across tasks with decision-dependent and decision-independent target variables underscores the importance of considering the structural causal model underlying the ADS task}.
\begin{table*}[t]
\centering
{\renewcommand{\arraystretch}{1.2}
\begin{tabular}{>{\centering\arraybackslash} m{2.3cm}>{\centering\arraybackslash} m{3.4cm}>{\centering\arraybackslash}m{2.8cm}>{\centering\arraybackslash}m{3.1cm}>{\centering\arraybackslash}m{2cm}} 
\toprule
\textbf{Work} & \textbf{Measurement ($F_m$)} & \textbf{Prediction ($F_p$)} & \textbf{Assumptions}  &  \textbf{Bias}  \\ 
\hline
\citet{gao2021human}  &  \multirow{5}{1.9cm}[0.1em]{$\hat{Y}^* = F_m[Y]$}  & \multirow{5}{3.0cm}[1em]{\centering \newline\newline$\hat{Y} = \hat{F}_p[X, D_H]$\newline Human decisions $D_H$ available at runtime }  &  \multirow{5}{2.9cm}[0.1em]{\centering Proxy and target variables are equivalent $Y^* = Y$} &   \multirow{5}{2cm}[0.2em]{\centering None}   \\ 
 
\citet{madras2018predict}   &   &        &     &     \\ 
\citet{wilder2020learning}    &    &     &   &  \\ 
\citet{tan2018investigating} &   &   &   &  \\ 
\citet{hilgard2021learning}  &   &    &   &    \\ 
\midrule
\citet{de2021leveraging}  & $\hat{Y}^* = F_m[X, D, Y]$, where $\hat{Y}^* = D$ \textit{expert consistency} instances and $Y$ otherwise &  \multirow{5}{3.2cm}[-3.2em]{\centering $\hat{Y} = \hat{F}_p[X]$ \newline Human decisions $D_H$ unavailable at runtime } & Expert consistency assumption & Measurement error, Selection bias \\ 
\citet{lakkaraju2017selective} & $\hat{Y}^* = F_m[Y]$ &   & Heterogeneous acceptance rates & Selection bias \\ 
\citet{coston2020counterfactual}  & $\hat{Y}^* = F_m[Y_d]$, where $Y_d$ is a potential outcome  &  &  Causal identifiability conditions & Intervention effects \\ 
\citet{coston2020confounding}                                              & $\hat{Y}^* = F_m[Y_d, Z_r]$, where $Y_d$ is a potential outcome  &  &  Causal identifiability conditions & Intervention effects, Unobservables   \\ 
\citet{wang2021fair}  & $\hat{Y}^* = F_m[Y]$,  where $Y$ error is group-dependent  &  &  Confident learning assumptions (see \citep{northcutt2021confident}) & Measurement error \\ 
\midrule
\midrule
\textbf{Label noise} \citet{menon2015learning}  & $\hat{Y}^* = F_m[Y]$, where $Y$ class-conditional or positive and unlabeled & ERM with surrogate loss (see \citep{natarajan2013learning}) & Weak separability  & Measurement error \\ 
\midrule
\textbf{Latent Class Analysis} \citet{mccutcheon1987latent}  & $\hat{Y}^* = F_m[Y]$, where $Y=\{Y^1, ..., Y^K\}$ are independent factors & 3-step LCA with covariates (see \citep{vermunt2010latent}) & $Y^i \bigCI Y^j \mid Y^*$  & Measurement error \\ 
\midrule
\textbf{Hui-Walter Framework} \citet{hui1980estimating}  & $\hat{Y}^* = F_m[Y]$, where $Y=\{Y^1, ..., Y^K\}$ are diagnostic tests & N/A & Test Se/Sp identifiability assumptions & Measurement error\\ 
\bottomrule
\end{tabular}
\caption{Taxonomy of measurement and prediction approaches. Top: methods proposed in ADS literature. Bottom: 
 methods applied in machine learning, social sciences, and bio-statistics. \textit{Bias} specifies which sources of target variable bias the approach addresses. }\label{tab:modeling_table}
}
\vspace{-2mm}
\end{table*}
\section{Model Development}\label{sec:modeling}
We now provide a framework for surfacing target variable modeling assumptions during predictive model development. We argue that predictive modeling for ADS involves two distinct steps: \textit{measurement} and \textit{prediction}. During the measurement step, tool developers construct a \textit{measurement model} that operationalizes the target outcome of interest $Y^*$ using readily available datasets. During the second step, tool designers train a \textit{prediction model} that targets the \textit{proxy outcome} returned by the measurement model. We now discuss each of these modeling steps in detail. 
\subsection{Measurement model}\label{sec:measurement}
During the measurement step, the unobserved outcome of interest ($Y^*$) is approximated using historical data\khdelete{ from the causal diagram in Figure \ref{fig:dag}}. This step involves establishing a \textit{measurement hypothesis} ($\hat{Y}^*$) using observed information: covariates $X$, past decisions $D$, and one or more outcome proxies $Y$ \khedit{(Figure \ref{fig:dag})}. In some settings, a subset of unobservables are \khdelete{also }available during model development, but unavailable \khedit{in}\khdelete{during} deployment. \khedit{Such}\khdelete{These} runtime confounders $Z_r \subseteq Z$ \khedit{may be}\khdelete{are also} present\khedit{. For example,} when protected attributes (e.g., race, gender) are available during \khdelete{model }development, but \khedit{not in deployment}\khdelete{are unavailable at deployment time} for legal \khedit{reasons}\khdelete{purposes} \citep{diana2022multiaccurate, coston2020runtime}. Given information $X$, $Z_r$, $D$, $Y$ recorded in existing data\khdelete{sets}, we can construct a measurement model \lgedit{approximating $Y^*$}: 
\begin{equation}\label{eq:measurement}
  \hat{Y}^* = F_m[X, Z_r, D, Y]
\end{equation}
Unlike statistical models commonly used in machine learning contexts, a measurement model cannot be learned from past data because the target outcome $Y^*$ is unobserved. Instead, $F_m$ relies on \textit{measurement assumptions} concerning the relationship between the unobserved outcome of interest and recorded information available for modeling. Therefore, it is not possible to assess the quality of $\hat{Y}^*$ by comparing against held-out data, as is common in prediction settings. Instead, evaluating measurement models requires a multifaceted approach, including assessments of construct validity, synthetic experiments, sensitivity analyses, and other evaluation strategies described in Section \ref{sec:measurement_model_evaluation}.  
All predictive models in ADS introduce a measurement model. However, this model is often \textit{implicitly defined} and makes tacit assumptions on the relationship between available data sources ( $X$, $Z_r$, $D$, $Y$) and the target variable ($Y^*$). Table \ref{tab:modeling_table} provides a detailed list of the measurement models assumed by existing ADS approaches. This table reifies often-implicit measurement assumptions adopted by prior work. In the bottom three rows, we apply our taxonomy to workhorse methods used in machine learning \citep{menon2015learning, natarajan2013learning}, quantitative social sciences \citep{mccutcheon1987latent}, and bio-statistics \citep{hui1980estimating} literature. 
\subsection{Prediction model}\label{sec:prediction}
After establishing a measurement model to estimate $Y^*$ given ($X$, $Z_r$, $D$, $Y$), tool designers then develop a \textit{prediction model} for use in decision-support settings. This prediction model takes observed covariates ($X$) and predicts the measurement hypothesis ($\hat{Y^*}$) established during the preceding measurement step. Because $Z_r$ and $Y$ are unavailable during deployment, these are not included in the prediction model. Most often, prediction models do not assume human decisions $D$ are available at runtime (i.e., algorithm-in-the-loop \citep{green2019principles}). However, in some more nuanced decision-making workflows, models may also assume that human decisions are available at run-time as an additional input (i.e., \citep{madras2018predict, wilder2020learning, gao2021human, tan2018investigating}). Given $X$ and optionally $D$ available at runtime, the prediction model \lgedit{estimates the \textit{measurement hypothesis} $\hat{Y}^*$}:
\begin{equation}\label{eq:prediction}
  \hat{Y} = \hat{F}_p[X, D]
\end{equation}
Whereas a measurement model is \textit{constructed via measurement assumptions}, the prediction model $\hat{F}_p$ is a \textit{learned mapping} from $X$ (and in some cases $D$) to the measurement hypothesis $\hat{Y}^*$. Therefore, it is appropriate to evaluate generalization of $\hat{F}_p$ to held-out data via the standard slate of evaluation metrics (e.g., accuracy, AU-ROC, or statistical fairness measures). Critically, this evaluation is conducted with respect to the measurement hypothesis established during the measurement step rather than the target outcome directly. Thus, showing strong performance of $\hat{F}_p$ \textit{is not sufficient to claim a model generates valid predictions for the target outcome} $Y^*$.
\subsection{Measurement model evaluation}\label{sec:measurement_model_evaluation}
Because the target outcome $Y^*$ is unobserved, measurement model evaluation requires a holistic, multifaceted approach leveraging converging sources of evidence. Informed by methods used in statistics, quantitative social sciences, and learning sciences, we provide a recommended set of approaches for validating measurement models in ADS tasks. 
\subsubsection{Construct reliability and validity}\label{subsec:construct_validity}
Measurement theory offers a comprehensive set of criteria for assessing the quality of a measurement model. \textit{Construct reliability} describes the degree to which a latent phenomena is consistently reflected by a measurement model (e.q. \ref{eq:measurement}) over time. Threats to construct reliability have been well documented in settings in which target variables are assigned via subjective human annotations. In these settings, assignment of target outcomes can vary substantially based on rater identity \citep{denton2021whose, denton2021whose}, context \citep{pavlopoulos2020toxicity}, and specification of the annotation protocol \citep{poletto2021resources}. Construct validity describes the extent to which a measurement model adequately captures an unobserved phenomenon of interest. Thus, while construct reliability is roughly analogous to the notion of statistical variance in $F_m$, construct validity is analogous to statistical \textit{bias} in $F_m$ . We refer the reader to \citep{jacobs2021measurement, coston2022validity} for a detailed discussion of sub-components of construct reliability and  validity that pertain to risk assessment development and evaluation. 
\subsubsection{Outcome cross-validation}
In many ADS settings, multiple \lgedit{proxies} \lgdelete{outcomes} are available that are believed to be related to the target outcome of interest. \lgedit{In criminal justice settings, courts often track multiple recidivism-related outcomes (e.g., 2-year general and violent recidivism, failure to appear); in child welfare settings, government agencies may track substantiation of abuse allegations, acceptance for welfare services, agency re-referral, placement in foster care, and hospitalization \citep{vaithianathan2019allegheny}.}  When multiple reference outcomes are available, \textit{outcome cross-validation} can be used to train a model to predict one proxy \lgdelete{$Y^1$}, then evaluate this model on a slate of \textit{additional reference variables} that modelers expect may be reasonable proxies for the outcome of interest. If targeting \lgedit{a proxy} \lgdelete{$Y^1$}\textit{also} results in strong performance across \lgedit{other reference variables}\lgdelete{held-out proxies}, this provides evidence \lgedit{suggesting that a proxy may serve as a suitable measurement model.} \lgdelete{that adopting the proxy under consideration may serve as a suitable measurement model.} \lgedit{Outcome cross-validation has been independently used by analyses of proxy outcomes in learning analytics \citep{rachatasumrit2021toward}, criminal justice \citep{kleinberg2018human}, child welfare \citep{de2021leveraging}, and healthcare \citep{obermeyer2019dissecting} domains.} Special cases of outcome cross-validation map to  sub-components of construct validity. \lgdelete{For example,} A model demonstrates \textit{predictive validity} if its predictions correlate with a reference outcome known to be related to the target phenomena of interest \citep{hand2004measurement}. \lgedit{A model demonstrates \textit{discriminant validity} if its predictions are \textit{not} correlated with an unrelated reference outcome.} 
\subsubsection{Sensitivity analyses}
Sensitivity analyses enable assessing the degree of \textit{measurement model misspecification} permissible before evaluation of a \textit{prediction model} is invalidated. This analytic technique was traditionally developed for causal inference settings to estimate the magnitude of unobserved confounding (i.e., unobservables $Z$) necessary to invalidate a causal effect estimate \citep{rosenbaum2005sensitivity, diaz2013sensitivity}. More recently, sensitivity analyses have been developed for predictive modeling settings. For instance, \citep{fogliato2020fairness} proposed a sensitivity analyses framework that examines the degree of outcome measurement error permissible before fairness-related analyses are invalidated. Future work in ADS would benefit from sensitivity analysis frameworks that examine multiple sources of target variable bias in parallel. 
\subsubsection{Synthetic evaluation}
A key limitation of leveraging real-world datasets for measurement model validation is that one never knows the actual relationship between $Y$ and $Y^*$ in naturalistic data. Model-level evaluations in ADS typically circumvent this issue via \textit{synthetic evaluations} that test whether proposed approaches are robust to experimentally manipulated bias \citep{de2021leveraging, coston2020counterfactual, menon2015learning, wang2021fair}. Yet, one  limitation of synthetic evaluations is that they require assuming a specific measurement error model. If the data generating process adopted by a synthetic evaluation does not reflect real-world conditions, this can lead evaluation blind-spots. This concern is salient because synthetic evaluations are often designed with bespoke data generating processes intended to highlight the \lgedit{specific challenge being addressed by the technique}. 
\subsubsection{The Oracle Test}
\citet{chouldechova2018case} propose a conceptual tool called the \textit{``Oracle Test''}, which can surface unforeseen sources of target variable bias. This thought experiment supposes that we have access to an oracle model that can predict a proxy $Y$ with perfect accuracy. The key question posed by this test is: \textit{``What concerns remain given access to an oracle?''}. Because we have a ``perfect'' prediction model (i.e., e.q. \ref{eq:prediction}), remaining concerns are often related to measurement and validity. For example, \citet{chouldechova2018case} surface concerns related to measurement error when they apply the Oracle Test to examine RAIs designed for ADS models deployed for child welfare. \citet{green2021algorithmic} also leverage the Oracle Test to argue that improvements to predictive accuracy do not equate to improved policy outcomes when competing factors in addition to risk (i.e., defendant liberty) play a role in judicial decision-making.
\begin{figure*}
\begin{minipage}{0.4\textwidth}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{\begin{tabular}{>{\centering\arraybackslash} m{3cm}>{\centering\arraybackslash} m{2cm}>{\centering\arraybackslash}m{2.5cm}} 
        \toprule
        
        \textbf{Work} & \textbf{Setting} &  \textbf{Sub-region of causal diagram}  \\ 
        \midrule
        
        \citet{hemmer2022effect} & \multirow{2}{2cm}[-.2em]{\centering House price prediction} & \multirow{2}{2.5cm}[-.1em]{\centering Unobservables:\\  $D \leftarrow Z \rightarrow  Y$} \\
        
        \citet{holstein2022toward} & & \\
        \midrule
        \citet{gordon2022jury} & Toxicity detection & Measurement error: $Y^* \rightarrow Y$  \\
        \midrule
        \citet{peng2019you} & Hiring & Selection bias: $X \rightarrow D$  \\
        \midrule
        \citet{green2021algorithmic} & \multirow{2}{2cm}[-.2em]{\centering Judicial} & \multirow{2}{2.5cm}[-.1em]{\centering Omitted payoffs: $Q \rightarrow D_H$}  \\
        \citet{fogliato2021impact} &  & \\
        \midrule
        
        \end{tabular}}
    \caption{Experimental studies examining the under-studied sub-region provided in Figure \ref{fig:dag}}\label{tab:tvb_studies}
\end{table}
\end{minipage}
\end{figure*}
\section{Assessing Gaps and Opportunities for Experimental Research }\label{sec:experimental_evaluations}
Our causal diagram can be used to assess the extent to which existing in vitro studies consider measurement error, intervention effects, and related challenges (Section \ref{subsec:mapping_existing}). By mapping existing studies to corresponding sub-regions of our proposed diagram, we surface key blind-spots in our current understanding of human-AI decision-making. Our causal diagram can be used to evaluate whether experimental findings are likely to hold \textit{ecological validity} in a given real-world ADS deployment (Section \ref{subsec:ecological}). More broadly, the methodology of causal diagrams can also serve as a scaffolding for structuring empirical knowledge beyond concerns related to target variable bias (Section \ref{subsec:scaffolding}).
\subsection{Mapping existing experimental study designs to our causal diagram}\label{subsec:mapping_existing}
To assess the extent to which existing studies examine factors related to target variable bias in their study design, we revisit a comprehensive literature review conducted by \citet{lai2021towards} through the lens of our causal diagram (Figure \ref{fig:dag}). \citet{lai2021towards} review over one hundred experimental studies of algorithm-assisted decision-making published in premiere venues between 2018 and 2021. Our follow-up analysis extends this review to studies published in 2022 at the same set of venues, in addition to other recently published pre-prints. We further limit selection criteria applied by \citet{lai2021towards} to studies examining human-AI decision-making in prediction-based decision-making settings (i.e., scope  outlined in Section \ref{sec:scope}). Thus, we exclude studies included in the initial review with a focus on NLP-related tasks.
\textbf{We find that 66 out of 72 ($\approx$ 92\%) studies satisfying our criteria conduct experimental evaluations focusing on a narrow sub-graph of our causal diagram}. These studies investigate a modification to the joint human-AI decision-making process (i.e., the blue $D_H$ and $D_A$ region) using observed attributes $X$ and an outcome proxy $Y$ (Figure \ref{fig:experimental_studies_dag}). Such studies assume that (1) the target variable and proxy are equivalent (i.e., no measurement error or validity concerns), (2) all predictive attributes of interest are observed by both the algorithm and the human (i.e., no model unobservables), and (3) decisions and outcomes are statistically independent (i.e., no intervention effects).
Six of the remaining studies we review examine different sub-regions of the causal diagram described in Figure \ref{fig:dag}. Table \ref{tab:tvb_studies} groups these studies by the sub-region under study, including unobservables \citep{hemmer2022effect, holstein2022toward}, measurement error \citep{gordon2022jury}, selection bias \citep{peng2019you}, and omitted payoffs \citep{green2021algorithmic, fogliato2021impact}. While these studies offer early insight into how target variable bias can impact algorithm-assisted human decision-making, our empirical understanding of these challenges remains limited compared to the joint human-AI decision region investigated by $\approx92\%$ of studies. Critically, no work in our review experimentally manipulated factors related to \textit{intervention effects} or examined \textit{multiple intersecting sources of bias} in parallel. \lgedit{Given the prevalence of compounding challenges in real-world settings\khdelete{(e.g., Table \ref{tab:evidence})}, \textit{this gap opens a broad space of open questions and future research opportunities} for research on human-AI decision-making}. 
\subsection{Assessing ecological validity of in vitro studies}\label{subsec:ecological}
The gap we identify between in vivo challenges \khdelete{(i.e., Table \ref{tab:evidence}) }and in vitro studies (i.e., Figure \ref{fig:experimental_studies_dag}) carries implications for the ecological validity of experimental studies. Threats to ecological validity may be most acute when findings from a controlled study conducted under simplified conditions are \textit{generalized} to real-world ADS deployments in which multiple sources of target variable bias are present. In these settings, measurement error and intervention effects could impact whether findings gathered via controlled experiments also apply in more complex real-world conditions. 
Fortunately, our causal diagram provides a tool for assessing whether findings from an \textit{in vitro} study are likely to generalize to a given \textit{in vivo} ADS tool deployment. The first step in this process involves mapping the decision support task to its corresponding DAG (e.g., identifying whether the task involves decision-dependent vs. decision-independent outcomes). Next, based on domain expertise, one can identify whether different sources of bias are likely to be relevant in the given real-world deployment. For instance, a model deployed to allocate tutoring resources (selective intervention) may need to account for measurement error in learning outcomes and intervention effects from historical tutoring decisions. In contrast, a model deployed for a perceptual assessment task (e.g., predicting current forest cover from limited image data) may not need to address these concerns. After identifying the DAG and relevant sources of bias, one can assess whether an experimental study is likely to generalize to this setting by examining whether the study used a similar prediction task (i.e., also tested decision-dependent or decision-independent outcomes).  
To demonstrate how causal diagrams can be used to assess ecological validity of lab-based studies, consider a previous lab-based assessment conducted by \citet{park2019slow}. This study -- which is sampled from the 66 studies covered by the blue sub-region of the causal diagram provided in Figure \ref{fig:experimental_studies_dag} -- examines whether introducing a delay between when humans view observed features $X$ and algorithmic recommendations $D_H$ improves their performance on a perceptual jellybean counting task. Because the true quantity of jellybeans does not depend on the decision under consideration, this setting involves decision-independent outcomes. Further, the influence of human-only observed attributes $Z$ and measurement error is limited in this task. Therefore, findings from this work may most readily generalize to real-world decision-making settings with limited interference from measurement error, model unobservables, and treatment effects. 
\subsection{Scaffolding a science of human-AI decision-making}\label{subsec:scaffolding}
Our work leverages causal diagrams to characterize sources of bias impacting target variables. However, beyond this focus, causal diagrams also offer a powerful scaffolding for studying other aspects of human-AI decision-making. While we model the target variable ($Y^*$) endogenously as a function of $(X, Z, D, Y)$, one could also construct a causal diagram examining the joint human-AI decision-making process. For example, \citet{green2021algorithmic} specify such a DAG that models how humans weigh risk against other competing factors (e.g., culpability, value of defendant freedom) during pre-trial release decisions. The authors then experimentally verify a \textit{hypothesized} edge in this DAG via a controlled online study. Through a series of such studies, it may be possible to develop a more generalized \textit{theory} of AI-assisted human decision-making across decision-support tasks. This process of specifying, testing, and refining causal models is central to existing empirical disciplines, including psychology and sociology \citep{pearl1995causal}. 
\section{Discussion}
Our work surfaces a disconnect between the challenges that arise in \textit{real-world deployments} of algorithmic systems versus \textit{current research practices} adopted in the literature on human-AI decision-making (i.e., experimental study designs, modeling assumptions, measures of human-AI decision quality). Addressing this disconnect is critical in light of the grave real-world impacts introduced by target variable bias, including misallocation of medical resources \citep{obermeyer2019dissecting} and perpetuation of historical bias in the criminal justice system \citep{fogliato2021validity, akpinar2021effect, bao2021s}, among many others \khedit{(see Appendix A)}\khdelete{ (Table \ref{tab:evidence})}.
Our work provides a critical first step for addressing this disconnect by clarifying the relationship between measurement error, intervention effects, unobserved confounding, and selection bias via intuitive causal diagrams. Going forward, we hope that this framework will support more comprehensive assessment of modeling assumptions (Sections \ref{sec:measurement} and \ref{sec:prediction}), rigorous evaluation of measurement-models (Section \ref{sec:measurement_model_evaluation}), and experimental human subjects studies that investigate the implications of  bias in algorithm-assisted \textit{human} decision-making (Section \ref{sec:experimental_evaluations}). However, our work is only the first step towards comprehensively addressing target variable bias in human-AI decision-making.
In particular, future research should develop holistic measures of decision-quality that reflect factors beyond statistical performance computed via a single outcome proxy. These measures should reflect both \textit{process-oriented} considerations (i.e., how multiple decision-relevant factors are weighted \citep{green2019principles}, and adherence to procedural, interpersonal, and informational justice) in addition to \textit{outcome-oriented} considerations (i.e., whether a decision led to a beneficial outcome). Where possible, outcome-related measures should draw upon \textit{multiple decision-relevant proxies} to better account for limitations of adopting any single proxy in isolation. \khdelete{While this practice is standard in disciplines such as learning sciences, diagnostic medical testing, and psychology, t}\khedit{T}o date, human-AI decision-making research has primarily adopted outcome-oriented measures that hinge upon on a single potentially flawed proxy. 
Our work also motivates exciting new lines of human-AI decision-making research. For instance, our review of experimental human subjects research (Section \ref{subsec:mapping_existing}) identifies an open space of rich empirical questions to be investigated through future studies. We currently have an incomplete understanding of how measurement error impacts human trust and reliance on algorithmic predictions. Further, while counterfactual prediction methods have been proposed in the modeling literature, we currently have limited knowledge of how these methods can be leveraged to support more effective human decision-making practices. More generally, our causal framework provides a set of tools for (i) identifying open empirical questions, (ii) designing studies with robust ecological validity, and (iii) synthesizing findings from multiple experimental studies into a complete scientific understanding of human-AI decision-making. We hope that our work will raise awareness of target variable bias in the human-AI decision-making research community and spur efforts to better align research practices with the complex challenges encountered in real-world ADS deployments.
\section{Acknowledgements}
We thank Stevie Chancellor, Steven Dang, Maria De-Arteaga, Shamya Karumbaiah, Ken Koedinger, and attendees of the NeurIPS 2022 Workshop on Human-Centered Machine-Learning for their helpful feedback. This work was supported by an award from the UL Research Institutes through the Center for Advancing Safety of Machine Intelligence (CASMI) at Northwestern University, the Carnegie Mellon University Block Center for Technology and Society (Award No. 53680.1.5007718), and the National Science Foundation Graduate Research Fellowship Program (Award No. DGE-1745016). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the grantors.
{ACM-Reference-Format}
\newpage
\section{Appendix}
\begin{table*}[t]
\centering
{\renewcommand{\arraystretch}{.5}
\begin{tabular}{>{\centering\arraybackslash} m{2.8cm}>{\centering\arraybackslash} m{1.2cm}>{\centering\arraybackslash}m{10cm}} 
\toprule
\textbf{Work} & \textbf{Domain}  & \textbf{Bias Reported}  \\ 
\hline
 \citet{kleinberg2018human}  &   \multirow{4}{1.2cm}[-.6em]{\centering Judicial}  & {\raggedleft Unobservables, selection bias, and outcome measurement error impacting pre-trial risk assessments }\\ 
 
\citet{fogliato2021validity}   &          &  {\raggedleft Measurement error introduced by adopting re-arrest as a proxy for re-offense}  \\ 
\citet{bao2021s}      &     &  Selection bias and measurement error impacting by recidivism RAIs \\ 
\citet{butcher2022racial} &     & Measurement error in re-arrest proxy outcomes introduced by differential arrest rates among Black and white defendants \\ 
\midrule
\citet{kawakami2022improving} \citet{cheng2022disparities}  &  Child Welfare  &  
\multirow{1}{10cm}[.9em]{\centering Documents social worker concerns that measurement error and unobservables impact the quality of ADS predictions}
  \\ 
\midrule
\citet{obermeyer2019dissecting}  &  \multirow{3}{1.3cm}[-1.1em]{\centering Medical}  &  Measurement error arising from adopting \textit{``cost of care''} as a health proxy  \\ 
 
 
 \citet{mullainathan2017does}  &   &  Measurement error introduced when using medical records as a proxy for stroke outcomes \\ 
 
 
 \citet{mullainathan2019machine}  &   & Unobservables, selection bias, and measurement error in clinical decision support \\ 
\midrule
 \citet{chalfin2016productivity}  & Hiring & Omitted payoffs, measurement error, and selection bias arising in \textit{teacher value-add} proxy used for educator hiring \\ 
 
\bottomrule
\end{tabular}
}
\caption{Documented examples of target variable bias impacting predictive models across numerous ADS domains.}
\label{tab:evidence}
\end{table*}
\subsection{Descriptions of widely-studied outcome measurement error models}
\begin{itemize}
    \item \textbf{Uniform} error \lgdelete{This error model} assumes that the target outcome is randomly corrupted \lgedit{by additive noise (i.e., $Y^* = Y + \epsilon$) \citep{pischke2007lecture}}. This setting is \lgdelete{also sometimes} called \textit{classical measurement error} in statistics and economics \lgdelete{as it assumes that the error term is additive . The machine learning literature more often refers to this setting as} \lgedit{and \textit{symmetric label noise} in machine learning \citep{angluin1988learning}}. Because it is possible to learn an unbiased estimate for $Y^*$ \lgedit{given proxy labels $Y$} \lgdelete{when minimizing many common classification performance measures (e.g. misclassification risk) with respect to $Y$} in uniform error settings \cite{menon2015learning}, this error model poses fewer threats to validity than others discussed below.
    
    \item \textbf{Class-dependent} error \lgdelete{This error model} assumes that positive \lgdelete{($Y^*=1$)} and negative \lgdelete{($Y^*=0$)} target outcomes are misclassified at different rates. \lgedit{As with uniform error,}, measurement error in this setting is uncorrelated with covariates ($Y \CI Y^* | X$) and model unobservables ($Y \CI Y^* | Z$). This model is referred to as \textit{asymmetric} or \textit{class conditional label noise} in machine learning literature \citep{scott2013classification}, and \textit{nondifferential mismeasurement} in statistics and epidemiology \lgdelete{literature} \citep{ogburn2013bias}. In contrast to uniform error settings, training a model to predict a proxy ($Y$) impacted by class dependent error \textit{will} lead to \lgedit{biased} estimates for the target outcome ($Y^*$) when \lgedit{optimizing accuracy} \cite{menon2015learning}.
    
    \item \textbf{Feature-dependent} error \lgdelete{This model reflects settings in which measurement error} occurs \textit{differentially} across sub-populations based on covariates ($Y \nCI Y^* | X$) or model unobservables ($Y \nCI Y^* | Z$)\lgedit{. This model is called} \textit{differential mismeasurement} in statistics and \textit{feature-dependent label noise} in machine learning literature \citep{frenay2013classification}. This setting is also called \textit{group-dependent error} when the covariate in question is a protected attribute (e.g., gender, race) \citep{wang2021fair}. Group-dependent error \lgdelete{inherits modeling challenges arising in the class dependent case, and} has been tied to disparities in criminal justice \citep{obermeyer2019dissecting} and medical \citep{akpinar2021effect} outcomes in real-world deployments of ADS tools.    
\end{itemize}
Human-AI decision-making research also stands to benefit from existing \textit{methodologies} designed to characterize measurement error in other disciplines. Latent Class Analysis (LCA) is an approach used in psychology and political science to identify latent sub-populations in data that are believed to carry an unobserved characteristic (e.g., personality, political ideology, or disease status) \citep{weller2020latent}. LCA estimates a set of conditional probabilities mapping multiple discrete \textit{factors} (i.e., \textit{proxies}) to a binary latent variable (e.g., \textit{target outcome}). While LCA is tailored to discrete latent variables, other structural equation models (i.e., factor analysis \cite{harman1976modern}) are designed for continuous latent variables. Within biostatistics, the Hui-Walter framework is used to estimate the sensitivity and specificity of diagnostic tests in the absence of a gold standard \citep{hui1980estimating}. Given multiple proxies, Hui-Walter can therefore be adapted to estimate the sensitivity and specificity of each proxy. Like all measurement models, LCA and Hui-Walter make assumptions on the relationship between the target outcome and its proxy. Table \ref{tab:modeling_table} states these assumptions in the context of our measurement model taxonomy.
"
46,"\begin{thebibliography}{10}
\bibitem{2010_book_Schmickler}
W.~Schmickler and E.~Santos.
\newblock {\em Interfacial Electrochemistry}.
\newblock Springer, Berlin, Heidelberg, 2010.
\bibitem{2019_JChemPhys_Hoermann_Marzari}
Nicolas~G. H\""ormann, Oliviero Andreussi, and Nicola Marzari.
\newblock Grand canonical simulations of electrochemical interfaces in implicit
  solvation models.
\newblock {\em J. Chem. Phys.}, 150(4):041730, 2019.
\bibitem{2022_JChemTheoComp_Binninger}
Arthur Hagopian, Marie-Liesse Doublet, Jean-S\'ebastien Filhol, and Tobias
  Binninger.
\newblock Advancement of the homogeneous background method for the
  computational simulation of electrochemical interfaces.
\newblock {\em Journal of Chemical Theory and Computation}, 18(3):1883--1893,
  2022.
\bibitem{2022_EnergyEnvironSci_Binninger}
Tobias Binninger and Marie-Liesse Doublet.
\newblock The {Ir--OOOO--Ir} transition state and the mechanism of the oxygen
  evolution reaction on \ce{IrO2}(110).
\newblock {\em Energy Environ. Sci.}, 15:2519--2528, 2022.
\bibitem{2022_PNAS_Koper}
Kasinath Ojha, Katharina Doblhoff-Dier, and Marc T.~M. Koper.
\newblock Double-layer structure of the pt(111)--aqueous electrolyte interface.
\newblock {\em Proceedings of the National Academy of Sciences},
  119(3):e2116016119, 2022.
\bibitem{2006_PhysRevB_Otani}
M.~Otani and O.~Sugino.
\newblock First-principles calculations of charged surfaces and interfaces: A
  plane-wave nonrepeated slab approach.
\newblock {\em Phys. Rev. B}, 73:115407, Mar 2006.
\bibitem{2008_PhysRevB_Jinnouchi}
Ryosuke Jinnouchi and Alfred~B. Anderson.
\newblock Electronic structure calculations of liquid-solid interfaces:
  Combination of density functional theory and modified poisson-boltzmann
  theory.
\newblock {\em Phys. Rev. B}, 77:245417, Jun 2008.
\bibitem{2010_arXiv_Dabo}
I.~Dabo, E.~Canc\`es, Y.~L. Li, and N.~Marzari.
\newblock Towards first-principles electrochemistry.
\newblock arXiv, eprint: 0901.0096, 2010.
\bibitem{2012_PhysRevB_Letchworth-Weaver_Arias}
Kendra Letchworth-Weaver and T.~A. Arias.
\newblock Joint density functional theory of the electrode-electrolyte
  interface: Application to fixed electrode potentials, interfacial
  capacitances, and potentials of zero charge.
\newblock {\em Phys. Rev. B}, 86:075140, Aug 2012.
\bibitem{2017_PhysRevB_Otani}
S.~Nishihara and M.~Otani.
\newblock Hybrid solvation models for bulk, interface, and membrane: Reference
  interaction site methods coupled with density functional theory.
\newblock {\em Phys. Rev. B}, 96:115429, 2017.
\bibitem{2017_JChemPhys_Sundararaman_Arias}
Ravishankar Sundararaman, William~A. Goddard~III, and Tomas~A. Arias.
\newblock Grand canonical electronic density-functional theory: Algorithms and
  applications to electrochemistry.
\newblock {\em The Journal of Chemical Physics}, 146(11):114104, 2017.
\bibitem{2018_JChemPhys_Schwarz}
R.~Sundararaman, K.~Letchworth-Weaver, and K.~A. Schwarz.
\newblock Improving accuracy of electrochemical capacitance and solvation
  energetics in first-principles calculations.
\newblock {\em J. Chem. Phys.}, 148(14):144105, 2018.
\bibitem{2019_JChemPhys_Mathew_Hennig}
Kiran Mathew, V.~S.~Chaitanya Kolluru, Srinidhi Mula, Stephan~N. Steinmann, and
  Richard~G. Hennig.
\newblock Implicit self-consistent electrolyte model in plane-wave
  density-functional theory.
\newblock {\em The Journal of Chemical Physics}, 151(23):234101, 2019.
\bibitem{2019_JChemPhys_Nattino_Marzari}
Francesco Nattino, Matthew Truscott, Nicola Marzari, and Oliviero Andreussi.
\newblock Continuum models of the electrochemical diffuse layer in
  electronic-structure calculations.
\newblock {\em J. Chem. Phys.}, 150(4):041722, 2019.
\bibitem{2019_JChemPhys_Melander_Honkala}
Marko~M. Melander, Mikael~J. Kuisma, Thorbj\o{}rn Erik~K\o{}ppen Christensen,
  and Karoliina Honkala.
\newblock Grand-canonical approach to density functional theory of
  electrocatalytic systems: Thermodynamics of solid-liquid interfaces at
  constant ion and electrode potentials.
\newblock {\em J. Chem. Phys.}, 150(4):041706, 2019.
\bibitem{2021_JPhysCondMat_Tesch_Kowalski_Eikerling}
R.~Tesch, P.~M. Kowalski, and M.~H. Eikerling.
\newblock Properties of the {Pt}(111)/electrolyte electrochemical interface
  studied with a hybrid {DFT}{\textendash}solvation approach.
\newblock {\em J. Phys. Condens. Matter}, 33(44):444004, 2021.
\bibitem{1924_ZElektrochem_Stern}
Otto Stern.
\newblock {Zur Theorie der Elektrolytischen Doppelschicht}.
\newblock {\em Zeitschrift f\""ur Elektrochemie und angewandte physikalische
  Chemie}, 30(21-22):508--516, 1924.
\bibitem{1983_JElectroanalChem_Badiali}
J.P. Badiali, M.L. Rosinberg, and J.~Goodisman.
\newblock Contribution of the metal to the differential capacity of an ideally
  polarisable electrode.
\newblock {\em Journal of Electroanalytical Chemistry and Interfacial
  Electrochemistry}, 143(1):73--88, 1983.
\bibitem{1989_Kornyshev_ElectrochimActa}
A.A. Kornyshev.
\newblock Metal electrons in the double layer theory.
\newblock {\em Electrochimica Acta}, 34(12):1829--1847, 1989.
\bibitem{1996_ChemRev_Schmickler}
Wolfgang Schmickler.
\newblock Electronic effects in the electric double layer.
\newblock {\em Chemical Reviews}, 96(8):3177--3200, 1996.
\bibitem{1942_PhilosMag_Bikerman}
J.J. Bikerman.
\newblock {XXXIX.} {S}tructure and capacity of electrical double layer.
\newblock {\em The London, Edinburgh, and Dublin Philosophical Magazine and
  Journal of Science}, 33(220):384--397, 1942.
\bibitem{1997_PhysRevLett_Borukhov}
Itamar Borukhov, David Andelman, and Henri Orland.
\newblock Steric effects in electrolytes: {A} modified poisson-boltzmann
  equation.
\newblock {\em Phys. Rev. Lett.}, 79:435--438, 1997.
\bibitem{2007_JPhysChemB_Kornyshev}
Alexei~A. Kornyshev.
\newblock Double-layer in ionic liquids: {P}aradigm change?
\newblock {\em The Journal of Physical Chemistry B}, 111(20):5545--5557, 2007.
\bibitem{2011_PhysRevLett_Bazant_Kornyshev}
Martin~Z. Bazant, Brian~D. Storey, and Alexei~A. Kornyshev.
\newblock Double layer in ionic liquids: {O}verscreening versus crowding.
\newblock {\em Phys. Rev. Lett.}, 106:046102, 2011.
\bibitem{1985_JPhysChem_Gerischer}
H.~Gerischer.
\newblock An interpretation of the double layer capacity of graphite electrodes
  in relation to the density of states at the fermi level.
\newblock {\em The Journal of Physical Chemistry}, 89(20):4249--4251, 1985.
\bibitem{1988_ApplPhysLett_Luryi}
Serge Luryi.
\newblock Quantum capacitance devices.
\newblock {\em Applied Physics Letters}, 52(6):501--503, 1988.
\bibitem{2004_JApplPhys_Pulfrey}
D.~L. John, L.~C. Castro, and D.~L. Pulfrey.
\newblock Quantum capacitance in nanoscale device modeling.
\newblock {\em Journal of Applied Physics}, 96(9):5180--5184, 2004.
\bibitem{2011_EnergyEnvSci_Stoller}
Meryl~D. Stoller, Carl~W. Magnuson, Yanwu Zhu, Shanthi Murali, Ji~Won Suk,
  Richard Piner, and Rodney~S. Ruoff.
\newblock Interfacial capacitance of single layer graphene.
\newblock {\em Energy Environ. Sci.}, 4:4685--4689, 2011.
\bibitem{2015_PhysRevB_Radin_Wood}
Maxwell~D. Radin, Tadashi Ogitsu, Juergen Biener, Minoru Otani, and Brandon~C.
  Wood.
\newblock Capacitive charge storage at an electrified interface investigated
  via direct first-principles simulations.
\newblock {\em Phys. Rev. B}, 91:125415, 2015.
\bibitem{2021_PhysRevB_Binninger}
Tobias Binninger.
\newblock Piecewise nonlinearity and capacitance in the joint density
  functional theory of extended interfaces.
\newblock {\em Phys. Rev. B}, 103:L161403, 2021.
\bibitem{2005_JPhysChemB_Petrosyan_Arias}
S.~A. Petrosyan, A.~A. Rigos, and T.~A. Arias.
\newblock Joint density-functional theory: {A}b initio study of {Cr$_2$O$_3$}
  surface chemistry in solution.
\newblock {\em The Journal of Physical Chemistry B}, 109(32):15436--15444,
  2005.
\bibitem{1982_JChemPhys_Capitani}
Joseph~F. Capitani, Roman~F. Nalewajski, and Robert~G. Parr.
\newblock Non‐born–oppenheimer density functional theory of molecular
  systems.
\newblock {\em The Journal of Chemical Physics}, 76(1):568--573, 1982.
\bibitem{1998_PhysRevB_Gidopoulos}
Nikitas Gidopoulos.
\newblock Kohn-sham equations for multicomponent systems: The exchange and
  correlation energy functional.
\newblock {\em Phys. Rev. B}, 57:2146--2152, 1998.
\bibitem{2001_PhysRevLett_Gross}
T.~Kreibich and E.~K.~U. Gross.
\newblock Multicomponent density-functional theory for electrons and nuclei.
\newblock {\em Phys. Rev. Lett.}, 86:2984--2987, 2001.
\bibitem{2008_PhysRevLett_Hammes-Schiffer}
Arindam Chakraborty, Michael~V. Pak, and Sharon Hammes-Schiffer.
\newblock Development of electron-proton density functionals for multicomponent
  density functional theory.
\newblock {\em Phys. Rev. Lett.}, 101:153001, 2008.
\bibitem{1964_PhysRev_Hohenberg_Kohn}
P.~Hohenberg and W.~Kohn.
\newblock Inhomogeneous electron gas.
\newblock {\em Phys. Rev.}, 136:B864--B871, 1964.
\bibitem{1991_JElectrochemSoc_Conway}
B.~E. Conway.
\newblock Transition from {\textquotedblleft}supercapacitor{\textquotedblright}
  to {\textquotedblleft}battery{\textquotedblright} behavior in electrochemical
  energy storage.
\newblock {\em Journal of The Electrochemical Society}, 138(6):1539--1548,
  1991.
\bibitem{2016_JPhysChemC_Eikerling}
Jun Huang, Ali Malek, Jianbo Zhang, and Michael~H. Eikerling.
\newblock Non-monotonic surface charging behavior of platinum: {A} paradigm
  change.
\newblock {\em The Journal of Physical Chemistry C}, 120(25):13587--13595,
  2016.
\bibitem{2017_JElectrochemSoc_Prendergast}
Artem Baskin and David Prendergast.
\newblock Improving continuum models to define practical limits for molecular
  models of electrified interfaces.
\newblock {\em Journal of The Electrochemical Society}, 164(11):E3438--E3447,
  2017.
\bibitem{1965_PhysRev_Mermin}
N.~David Mermin.
\newblock Thermal properties of the inhomogeneous electron gas.
\newblock {\em Phys. Rev.}, 137:A1441--A1443, 1965.
\bibitem{1965_PhysRev_Kohn_Sham}
W.~Kohn and L.~J. Sham.
\newblock Self-consistent equations including exchange and correlation effects.
\newblock {\em Phys. Rev.}, 140:A1133--A1138, 1965.
\bibitem{2007_ApplPhysLett_Fang}
Tian Fang, Aniruddha Konar, Huili Xing, and Debdeep Jena.
\newblock Carrier statistics and quantum capacitance of graphene sheets and
  ribbons.
\newblock {\em Applied Physics Letters}, 91(9):092109, 2007.
\bibitem{2016_JPhysChemLett_Zhan}
Cheng Zhan and De-en Jiang.
\newblock Contribution of dielectric screening to the total capacitance of
  few-layer graphene electrodes.
\newblock {\em The Journal of Physical Chemistry Letters}, 7(5):789--794, 2016.
\bibitem{2020_JElectroanalChem_Schmickler}
Wolfgang Schmickler.
\newblock The electronic response of the metal in simulations of the electric
  double layer.
\newblock {\em Journal of Electroanalytical Chemistry}, 856:113664, 2020.
\bibitem{1957_TransFaradaySoc_Bradley}
R.~S. Bradley.
\newblock The electrical conductivity of ice.
\newblock {\em Trans. Faraday Soc.}, 53:687--691, 1957.
\bibitem{1958_ProcRSocLondA_Eigen}
Manfred Eigen and L.~De~Maeyer.
\newblock Self-dissociation and protonic charge transport in water and ice.
\newblock {\em Proc. R. Soc. Lond. A}, 247:505--533, 1958.
\bibitem{1984_ApplPhysA_Langer}
J.~J. Langer.
\newblock Protonic p-n junction.
\newblock {\em Applied Physics A}, 34:195--198, 1984.
\bibitem{2013_SciRep_Deng}
Yingxin Deng, Erik Josberger, Jungho Jin, Anita~Fadavi Roudsari, Brett~A.
  Helms, Chao Zhong, M.~P. Anantram, and Marco Rolandi.
\newblock {H}$^{+}$-type and {OH}$^{-}$-type biological protonic semiconductors
  and complementary devices.
\newblock {\em Scientific Reports}, 3:2481, 2013.
\bibitem{2013_FaradayDiscuss_Kornyshev}
Alexei~A. Kornyshev.
\newblock The simplest model of charge storage in single file metallic
  nanopores.
\newblock {\em Faraday Discuss.}, 164:117--133, 2013.
\bibitem{2020_JChemPhys_Eikerling}
Mpumelelo Matse, Peter Berg, and Michael Eikerling.
\newblock Asymmetric double-layer charging in a cylindrical nanopore under
  closed confinement.
\newblock {\em The Journal of Chemical Physics}, 152(8):084103, 2020.
\bibitem{2018_Science_Fumagalli}
L.~Fumagalli, A.~Esfandiar, R.~Fabregas, S.~Hu, P.~Ares, A.~Janardanan,
  Q.~Yang, B.~Radha, T.~Taniguchi, K.~Watanabe, G.~Gomila, K.~S. Novoselov, and
  A.~K. Geim.
\newblock Anomalously low dielectric constant of confined water.
\newblock {\em Science}, 360(6395):1339--1342, 2018.
\bibitem{2019_NatComm_Yamada}
Akira Sugahara, Yasunobu Ando, Satoshi Kajiyama, Koji Yazawa, Kazuma Gotoh,
  Minoru Otani, Masashi Okubo, and Atsuo Yamada.
\newblock Negative dielectric constant of water confined in nanosheets.
\newblock {\em Nature Communications}, 10:850, 2019.
\bibitem{2021_NatMater_Boyd}
Shelby Boyd, Karthik Ganeshan, Wan-Yu Tsai, Tao Wu, Saeed Saeed, De-en Jiang,
  Nina Balke, Adri C.~T. van Duin, and Veronica Augustyn.
\newblock Effects of interlayer confinement and hydration on capacitive charge
  storage in birnessite.
\newblock {\em Nature Materials}, 20:1689--1694, 2021.
\bibitem{2016_ChemMater_Ceder}
Aziz Abdellahi, Alexander Urban, Stephen Dacek, and Gerbrand Ceder.
\newblock Understanding the effect of cation disorder on the voltage profile of
  lithium transition-metal oxides.
\newblock {\em Chemistry of Materials}, 28(15):5373--5383, 2016.
\bibitem{1996_CompMaterSci_Kresse}
G.~Kresse and J.~Furthm\""uller.
\newblock Efficiency of ab-initio total energy calculations for metals and
  semiconductors using a plane-wave basis set.
\newblock {\em Computational Materials Science}, 6(1):15 -- 50, 1996.
\bibitem{1999_PhysRevB_Kresse}
G.~Kresse and D.~Joubert.
\newblock From ultrasoft pseudopotentials to the projector augmented-wave
  method.
\newblock {\em Phys. Rev. B}, 59:1758--1775, Jan 1999.
\bibitem{1996_PhysRevLett_PBE}
John~P. Perdew, Kieron Burke, and Matthias Ernzerhof.
\newblock Generalized gradient approximation made simple.
\newblock {\em Phys. Rev. Lett.}, 77:3865--3868, Oct 1996.
\end{thebibliography}
\begin{thebibliography}{10}
\bibitem{2010_book_Schmickler}
W.~Schmickler and E.~Santos.
\newblock {\em Interfacial Electrochemistry}.
\newblock Springer, Berlin, Heidelberg, 2010.
\bibitem{2019_JChemPhys_Hoermann_Marzari}
Nicolas~G. H\""ormann, Oliviero Andreussi, and Nicola Marzari.
\newblock Grand canonical simulations of electrochemical interfaces in implicit
  solvation models.
\newblock {\em J. Chem. Phys.}, 150(4):041730, 2019.
\bibitem{2022_JChemTheoComp_Binninger}
Arthur Hagopian, Marie-Liesse Doublet, Jean-S\'ebastien Filhol, and Tobias
  Binninger.
\newblock Advancement of the homogeneous background method for the
  computational simulation of electrochemical interfaces.
\newblock {\em Journal of Chemical Theory and Computation}, 18(3):1883--1893,
  2022.
\bibitem{2022_EnergyEnvironSci_Binninger}
Tobias Binninger and Marie-Liesse Doublet.
\newblock The {Ir--OOOO--Ir} transition state and the mechanism of the oxygen
  evolution reaction on \ce{IrO2}(110).
\newblock {\em Energy Environ. Sci.}, 15:2519--2528, 2022.
\bibitem{2022_PNAS_Koper}
Kasinath Ojha, Katharina Doblhoff-Dier, and Marc T.~M. Koper.
\newblock Double-layer structure of the pt(111)--aqueous electrolyte interface.
\newblock {\em Proceedings of the National Academy of Sciences},
  119(3):e2116016119, 2022.
\bibitem{2006_PhysRevB_Otani}
M.~Otani and O.~Sugino.
\newblock First-principles calculations of charged surfaces and interfaces: A
  plane-wave nonrepeated slab approach.
\newblock {\em Phys. Rev. B}, 73:115407, Mar 2006.
\bibitem{2008_PhysRevB_Jinnouchi}
Ryosuke Jinnouchi and Alfred~B. Anderson.
\newblock Electronic structure calculations of liquid-solid interfaces:
  Combination of density functional theory and modified poisson-boltzmann
  theory.
\newblock {\em Phys. Rev. B}, 77:245417, Jun 2008.
\bibitem{2010_arXiv_Dabo}
I.~Dabo, E.~Canc\`es, Y.~L. Li, and N.~Marzari.
\newblock Towards first-principles electrochemistry.
\newblock arXiv, eprint: 0901.0096, 2010.
\bibitem{2012_PhysRevB_Letchworth-Weaver_Arias}
Kendra Letchworth-Weaver and T.~A. Arias.
\newblock Joint density functional theory of the electrode-electrolyte
  interface: Application to fixed electrode potentials, interfacial
  capacitances, and potentials of zero charge.
\newblock {\em Phys. Rev. B}, 86:075140, Aug 2012.
\bibitem{2017_PhysRevB_Otani}
S.~Nishihara and M.~Otani.
\newblock Hybrid solvation models for bulk, interface, and membrane: Reference
  interaction site methods coupled with density functional theory.
\newblock {\em Phys. Rev. B}, 96:115429, 2017.
\bibitem{2017_JChemPhys_Sundararaman_Arias}
Ravishankar Sundararaman, William~A. Goddard~III, and Tomas~A. Arias.
\newblock Grand canonical electronic density-functional theory: Algorithms and
  applications to electrochemistry.
\newblock {\em The Journal of Chemical Physics}, 146(11):114104, 2017.
\bibitem{2018_JChemPhys_Schwarz}
R.~Sundararaman, K.~Letchworth-Weaver, and K.~A. Schwarz.
\newblock Improving accuracy of electrochemical capacitance and solvation
  energetics in first-principles calculations.
\newblock {\em J. Chem. Phys.}, 148(14):144105, 2018.
\bibitem{2019_JChemPhys_Mathew_Hennig}
Kiran Mathew, V.~S.~Chaitanya Kolluru, Srinidhi Mula, Stephan~N. Steinmann, and
  Richard~G. Hennig.
\newblock Implicit self-consistent electrolyte model in plane-wave
  density-functional theory.
\newblock {\em The Journal of Chemical Physics}, 151(23):234101, 2019.
\bibitem{2019_JChemPhys_Nattino_Marzari}
Francesco Nattino, Matthew Truscott, Nicola Marzari, and Oliviero Andreussi.
\newblock Continuum models of the electrochemical diffuse layer in
  electronic-structure calculations.
\newblock {\em J. Chem. Phys.}, 150(4):041722, 2019.
\bibitem{2019_JChemPhys_Melander_Honkala}
Marko~M. Melander, Mikael~J. Kuisma, Thorbj\o{}rn Erik~K\o{}ppen Christensen,
  and Karoliina Honkala.
\newblock Grand-canonical approach to density functional theory of
  electrocatalytic systems: Thermodynamics of solid-liquid interfaces at
  constant ion and electrode potentials.
\newblock {\em J. Chem. Phys.}, 150(4):041706, 2019.
\bibitem{2021_JPhysCondMat_Tesch_Kowalski_Eikerling}
R.~Tesch, P.~M. Kowalski, and M.~H. Eikerling.
\newblock Properties of the {Pt}(111)/electrolyte electrochemical interface
  studied with a hybrid {DFT}{\textendash}solvation approach.
\newblock {\em J. Phys. Condens. Matter}, 33(44):444004, 2021.
\bibitem{1924_ZElektrochem_Stern}
Otto Stern.
\newblock {Zur Theorie der Elektrolytischen Doppelschicht}.
\newblock {\em Zeitschrift f\""ur Elektrochemie und angewandte physikalische
  Chemie}, 30(21-22):508--516, 1924.
\bibitem{1983_JElectroanalChem_Badiali}
J.P. Badiali, M.L. Rosinberg, and J.~Goodisman.
\newblock Contribution of the metal to the differential capacity of an ideally
  polarisable electrode.
\newblock {\em Journal of Electroanalytical Chemistry and Interfacial
  Electrochemistry}, 143(1):73--88, 1983.
\bibitem{1989_Kornyshev_ElectrochimActa}
A.A. Kornyshev.
\newblock Metal electrons in the double layer theory.
\newblock {\em Electrochimica Acta}, 34(12):1829--1847, 1989.
\bibitem{1996_ChemRev_Schmickler}
Wolfgang Schmickler.
\newblock Electronic effects in the electric double layer.
\newblock {\em Chemical Reviews}, 96(8):3177--3200, 1996.
\bibitem{1942_PhilosMag_Bikerman}
J.J. Bikerman.
\newblock {XXXIX.} {S}tructure and capacity of electrical double layer.
\newblock {\em The London, Edinburgh, and Dublin Philosophical Magazine and
  Journal of Science}, 33(220):384--397, 1942.
\bibitem{1997_PhysRevLett_Borukhov}
Itamar Borukhov, David Andelman, and Henri Orland.
\newblock Steric effects in electrolytes: {A} modified poisson-boltzmann
  equation.
\newblock {\em Phys. Rev. Lett.}, 79:435--438, 1997.
\bibitem{2007_JPhysChemB_Kornyshev}
Alexei~A. Kornyshev.
\newblock Double-layer in ionic liquids: {P}aradigm change?
\newblock {\em The Journal of Physical Chemistry B}, 111(20):5545--5557, 2007.
\bibitem{2011_PhysRevLett_Bazant_Kornyshev}
Martin~Z. Bazant, Brian~D. Storey, and Alexei~A. Kornyshev.
\newblock Double layer in ionic liquids: {O}verscreening versus crowding.
\newblock {\em Phys. Rev. Lett.}, 106:046102, 2011.
\bibitem{1985_JPhysChem_Gerischer}
H.~Gerischer.
\newblock An interpretation of the double layer capacity of graphite electrodes
  in relation to the density of states at the fermi level.
\newblock {\em The Journal of Physical Chemistry}, 89(20):4249--4251, 1985.
\bibitem{1988_ApplPhysLett_Luryi}
Serge Luryi.
\newblock Quantum capacitance devices.
\newblock {\em Applied Physics Letters}, 52(6):501--503, 1988.
\bibitem{2004_JApplPhys_Pulfrey}
D.~L. John, L.~C. Castro, and D.~L. Pulfrey.
\newblock Quantum capacitance in nanoscale device modeling.
\newblock {\em Journal of Applied Physics}, 96(9):5180--5184, 2004.
\bibitem{2011_EnergyEnvSci_Stoller}
Meryl~D. Stoller, Carl~W. Magnuson, Yanwu Zhu, Shanthi Murali, Ji~Won Suk,
  Richard Piner, and Rodney~S. Ruoff.
\newblock Interfacial capacitance of single layer graphene.
\newblock {\em Energy Environ. Sci.}, 4:4685--4689, 2011.
\bibitem{2015_PhysRevB_Radin_Wood}
Maxwell~D. Radin, Tadashi Ogitsu, Juergen Biener, Minoru Otani, and Brandon~C.
  Wood.
\newblock Capacitive charge storage at an electrified interface investigated
  via direct first-principles simulations.
\newblock {\em Phys. Rev. B}, 91:125415, 2015.
\bibitem{2021_PhysRevB_Binninger}
Tobias Binninger.
\newblock Piecewise nonlinearity and capacitance in the joint density
  functional theory of extended interfaces.
\newblock {\em Phys. Rev. B}, 103:L161403, 2021.
\bibitem{2005_JPhysChemB_Petrosyan_Arias}
S.~A. Petrosyan, A.~A. Rigos, and T.~A. Arias.
\newblock Joint density-functional theory: {A}b initio study of {Cr$_2$O$_3$}
  surface chemistry in solution.
\newblock {\em The Journal of Physical Chemistry B}, 109(32):15436--15444,
  2005.
\bibitem{1982_JChemPhys_Capitani}
Joseph~F. Capitani, Roman~F. Nalewajski, and Robert~G. Parr.
\newblock Non‐born–oppenheimer density functional theory of molecular
  systems.
\newblock {\em The Journal of Chemical Physics}, 76(1):568--573, 1982.
\bibitem{1998_PhysRevB_Gidopoulos}
Nikitas Gidopoulos.
\newblock Kohn-sham equations for multicomponent systems: The exchange and
  correlation energy functional.
\newblock {\em Phys. Rev. B}, 57:2146--2152, 1998.
\bibitem{2001_PhysRevLett_Gross}
T.~Kreibich and E.~K.~U. Gross.
\newblock Multicomponent density-functional theory for electrons and nuclei.
\newblock {\em Phys. Rev. Lett.}, 86:2984--2987, 2001.
\bibitem{2008_PhysRevLett_Hammes-Schiffer}
Arindam Chakraborty, Michael~V. Pak, and Sharon Hammes-Schiffer.
\newblock Development of electron-proton density functionals for multicomponent
  density functional theory.
\newblock {\em Phys. Rev. Lett.}, 101:153001, 2008.
\bibitem{1964_PhysRev_Hohenberg_Kohn}
P.~Hohenberg and W.~Kohn.
\newblock Inhomogeneous electron gas.
\newblock {\em Phys. Rev.}, 136:B864--B871, 1964.
\bibitem{1991_JElectrochemSoc_Conway}
B.~E. Conway.
\newblock Transition from {\textquotedblleft}supercapacitor{\textquotedblright}
  to {\textquotedblleft}battery{\textquotedblright} behavior in electrochemical
  energy storage.
\newblock {\em Journal of The Electrochemical Society}, 138(6):1539--1548,
  1991.
\bibitem{2016_JPhysChemC_Eikerling}
Jun Huang, Ali Malek, Jianbo Zhang, and Michael~H. Eikerling.
\newblock Non-monotonic surface charging behavior of platinum: {A} paradigm
  change.
\newblock {\em The Journal of Physical Chemistry C}, 120(25):13587--13595,
  2016.
\bibitem{2017_JElectrochemSoc_Prendergast}
Artem Baskin and David Prendergast.
\newblock Improving continuum models to define practical limits for molecular
  models of electrified interfaces.
\newblock {\em Journal of The Electrochemical Society}, 164(11):E3438--E3447,
  2017.
\bibitem{1965_PhysRev_Mermin}
N.~David Mermin.
\newblock Thermal properties of the inhomogeneous electron gas.
\newblock {\em Phys. Rev.}, 137:A1441--A1443, 1965.
\bibitem{1965_PhysRev_Kohn_Sham}
W.~Kohn and L.~J. Sham.
\newblock Self-consistent equations including exchange and correlation effects.
\newblock {\em Phys. Rev.}, 140:A1133--A1138, 1965.
\bibitem{2007_ApplPhysLett_Fang}
Tian Fang, Aniruddha Konar, Huili Xing, and Debdeep Jena.
\newblock Carrier statistics and quantum capacitance of graphene sheets and
  ribbons.
\newblock {\em Applied Physics Letters}, 91(9):092109, 2007.
\bibitem{2016_JPhysChemLett_Zhan}
Cheng Zhan and De-en Jiang.
\newblock Contribution of dielectric screening to the total capacitance of
  few-layer graphene electrodes.
\newblock {\em The Journal of Physical Chemistry Letters}, 7(5):789--794, 2016.
\bibitem{2020_JElectroanalChem_Schmickler}
Wolfgang Schmickler.
\newblock The electronic response of the metal in simulations of the electric
  double layer.
\newblock {\em Journal of Electroanalytical Chemistry}, 856:113664, 2020.
\bibitem{1957_TransFaradaySoc_Bradley}
R.~S. Bradley.
\newblock The electrical conductivity of ice.
\newblock {\em Trans. Faraday Soc.}, 53:687--691, 1957.
\bibitem{1958_ProcRSocLondA_Eigen}
Manfred Eigen and L.~De~Maeyer.
\newblock Self-dissociation and protonic charge transport in water and ice.
\newblock {\em Proc. R. Soc. Lond. A}, 247:505--533, 1958.
\bibitem{1984_ApplPhysA_Langer}
J.~J. Langer.
\newblock Protonic p-n junction.
\newblock {\em Applied Physics A}, 34:195--198, 1984.
\bibitem{2013_SciRep_Deng}
Yingxin Deng, Erik Josberger, Jungho Jin, Anita~Fadavi Roudsari, Brett~A.
  Helms, Chao Zhong, M.~P. Anantram, and Marco Rolandi.
\newblock {H}$^{+}$-type and {OH}$^{-}$-type biological protonic semiconductors
  and complementary devices.
\newblock {\em Scientific Reports}, 3:2481, 2013.
\bibitem{2013_FaradayDiscuss_Kornyshev}
Alexei~A. Kornyshev.
\newblock The simplest model of charge storage in single file metallic
  nanopores.
\newblock {\em Faraday Discuss.}, 164:117--133, 2013.
\bibitem{2020_JChemPhys_Eikerling}
Mpumelelo Matse, Peter Berg, and Michael Eikerling.
\newblock Asymmetric double-layer charging in a cylindrical nanopore under
  closed confinement.
\newblock {\em The Journal of Chemical Physics}, 152(8):084103, 2020.
\bibitem{2018_Science_Fumagalli}
L.~Fumagalli, A.~Esfandiar, R.~Fabregas, S.~Hu, P.~Ares, A.~Janardanan,
  Q.~Yang, B.~Radha, T.~Taniguchi, K.~Watanabe, G.~Gomila, K.~S. Novoselov, and
  A.~K. Geim.
\newblock Anomalously low dielectric constant of confined water.
\newblock {\em Science}, 360(6395):1339--1342, 2018.
\bibitem{2019_NatComm_Yamada}
Akira Sugahara, Yasunobu Ando, Satoshi Kajiyama, Koji Yazawa, Kazuma Gotoh,
  Minoru Otani, Masashi Okubo, and Atsuo Yamada.
\newblock Negative dielectric constant of water confined in nanosheets.
\newblock {\em Nature Communications}, 10:850, 2019.
\bibitem{2021_NatMater_Boyd}
Shelby Boyd, Karthik Ganeshan, Wan-Yu Tsai, Tao Wu, Saeed Saeed, De-en Jiang,
  Nina Balke, Adri C.~T. van Duin, and Veronica Augustyn.
\newblock Effects of interlayer confinement and hydration on capacitive charge
  storage in birnessite.
\newblock {\em Nature Materials}, 20:1689--1694, 2021.
\bibitem{2016_ChemMater_Ceder}
Aziz Abdellahi, Alexander Urban, Stephen Dacek, and Gerbrand Ceder.
\newblock Understanding the effect of cation disorder on the voltage profile of
  lithium transition-metal oxides.
\newblock {\em Chemistry of Materials}, 28(15):5373--5383, 2016.
\bibitem{1996_CompMaterSci_Kresse}
G.~Kresse and J.~Furthm\""uller.
\newblock Efficiency of ab-initio total energy calculations for metals and
  semiconductors using a plane-wave basis set.
\newblock {\em Computational Materials Science}, 6(1):15 -- 50, 1996.
\bibitem{1999_PhysRevB_Kresse}
G.~Kresse and D.~Joubert.
\newblock From ultrasoft pseudopotentials to the projector augmented-wave
  method.
\newblock {\em Phys. Rev. B}, 59:1758--1775, Jan 1999.
\bibitem{1996_PhysRevLett_PBE}
John~P. Perdew, Kieron Burke, and Matthias Ernzerhof.
\newblock Generalized gradient approximation made simple.
\newblock {\em Phys. Rev. Lett.}, 77:3865--3868, Oct 1996.
\end{thebibliography}
"
12,"\importpackages{}
\graphicspath{ {./images/} }


\title{FedIL: Federated Incremental Learning from Decentralized Unlabeled Data with Convergence Analysis}
\author{\IEEEauthorblockN{Nan Yang, Dong Yuan, Charles Z Liu,  Yongkun Deng and Wei Bao \\}
\IEEEauthorblockA{\textit{Faculty of Engineering, The University of Sydney} \\
\{n.yang, dong.yuan, zhenzhong.liu\}@sydney.edu.au}, yden9681@uni.sydney.edu.au, wei.bao@sydney.edu.au}
        
\maketitle
\begin{abstract}
Most existing federated learning methods assume that clients have fully labeled data to train on, while in reality, it is hard for the clients to get task-specific labels due to users' privacy concerns, high labeling costs, or lack of expertise. This work considers the server with a small labeled dataset and intends to use unlabeled data in multiple clients for semi-supervised learning. We propose a new framework with a generalized model, Federated Incremental Learning (FedIL), to address the problem of how to utilize labeled data in the server and unlabeled data in clients separately in the scenario of Federated Learning (FL). FedIL uses the Iterative Similarity Fusion to enforce the server-client consistency on the predictions of unlabeled data and uses incremental confidence to establish a credible pseudo-label set in each client. We show that FedIL will accelerate model convergence by Cosine Similarity with normalization, proved by Banach Fixed Point Theorem. The code is available at \href{https://anonymous.4open.science/r/fedil}{https://anonymous.4open.science/r/fedil}.
\end{abstract}
\begin{IEEEkeywords}
Semi-Supervised Learning, Federated Learning, Incremental Learning.
\end{IEEEkeywords}
\section{Introduction}
Federated Learning (FL) is a decentralized technique where multiple clients collaborate to train a global model through coordinated communication \cite{mcmahan2017communication,zhao2018federated,chen2019communication}. This method has been successfully applied in a range of applications and offers solutions to data privacy, security, and access issues\cite{hard2018federated,yang2019ffd,brisimi2018federated}. However, previous FL research \cite{han2020robust} often assumes that clients have fully annotated data with ground-truth labels, which can be an unrealistic assumption as labeling data is a time-consuming, expensive process that often requires the participation of domain experts. A more practical scenario is to share a limited amount of labeled data on the server while assisting clients with unlabeled data in model training \cite{jeong2021federated}, as proposed in recent studies.
The samples and labels on the server side may be accurate, however, it must be acknowledged that the data on the server is limited and does not provide a complete picture. This results in the reflection of only local features, and not global features. While other clients may have more extensive data coverage, the lack of reliable label information causes the features to be unreliable and uncertain. The two main challenges in model training in an FSSL scenario are depicted in Figure \ref{challenges}. Firstly, overfitting can occur when too much reliance is placed on labeled data from the server, leading to poor model generalization. Secondly, the absence of ground-truth labels in the client data may result in the annotating of incorrect pseudo-labels, causing model mislearning and preventing convergence of model training.
Motivated by this practical scenario, a naive solution is to simply perform SSL methods using any off-the-shelf methods (e.g. FixMatch \cite{DBLP:conf/nips/SohnBCZZRCKL20}, UDA \cite{xie2020unsupervised}), while using federated learning strategies to aggregate the learned weights.
The recent method FedMatch \cite{jeong2021federated} uses existing SSL methods based on pseudo-labeling and enhances the consistency between predictions made across multiple models by deploying a second labeled dataset for validation on the server, but additionally increases the need for labeled data. 
Other approaches FedU \cite{DBLP:conf/iccv/ZhuangG0ZY21}, FedEMA \cite{zhuang2021divergence}, and Orchestra \cite{DBLP:conf/icml/LubanaTKDM22}, use self-supervised strategies to correct the training results of aggregated client models by labeled data on the server. These methods may result in a learning strategy that heavily relies on the feature information of labeled data, leading to the risk of overfitting.
Therefore, a major challenge in FSSL is finding a way to correct the training bias caused by uncertain samples and labels on clients while also learning correct feature information from clients that the server does not have. Additionally, current research in this field is based on empirical model design, with no analysis of model convergence for the FSSL scenario.
\begin{figure*}[t]
  \centering
  [width=0.95\textwidth]{challenges_in_FSSL.pdf}
  \caption{\textbf{Illustrations of Two Challenges in Federated Semi-Supervised Learning} (a) Model overfitting is caused by over-reliance on the labeled data on the server, leading to poor recognition of one class that is not similar to those in the training data. (b) Model mislearning is caused by incorrect pseudo-labels generated in clients, leading to incorrect classification in that the data belongs to one class but is labeled as another.}
  \label{challenges}
\end{figure*}
In this paper, we propose Federated Incremental Learning (FedIL), which is a novel and general framework, for addressing the problem of FSSL, aiming at the demanding challenge of how to utilize separated labeled data in the server and unlabeled data in clients. 
Inspired by the mainstream semi-supervised learning methods based on pseudo-labeling, we alternate the training of labeled data in the server and unlabeled data in selected clients and employ a siamese network for contrastive learning to ensure acquiring high-quality pseudo-labels during training. To prevent the model from forgetting what it has learned from labeled data in the server when learning unlabeled data in clients, our method uses KL loss during client training to enforce the consistency between the predictions made by clients and the server. 
In clients, FedIL selects high-confidence pseudo-labels obtained through the mechanism of Incremental Credibility Learning to establish an independent and highly credible pseudo-label set in each client, allowing each client to achieve authentic semi-supervised learning to reduce the training bias during the client training.
In the server, we screen the uploaded client weights by Cosine Similarity with normalization to accelerate the convergence of model training.
The design of FedIL is based on the specific needs of the FSSL scenario, especially in protecting the data privacy of clients. The convergence condition of the traditional FL model is the decrease of the Loss, while the Loss in FSSL is computed by pseudo-labels rather than ground-truth labels, which is unreliable, and thus cannot guarantee the model convergence. Therefore, we first propose a Theorem base on the Banach fixed point to use weight difference as a criterion rather than Loss to determine model convergence in FSSL, which reveals that the model enters a progressive convergence stage if the increment difference of weight between the server and the client decreases.
The main contributions of this work are summarized as follows:
\begin{itemize}
\item 
We first introduce \textbf{Incremental Credibility Learning} to select pseudo labels with higher confidence for joining a pseudo-label set to establish a real and reliable semi-supervised training in each client. 
We also propose \textbf{Global Incremental Learning} to use \textbf{Cosine Similarity} with normalization to select client weights that are close to the server weight, which accelerates model learning.
\item
We further propose a Theorem base on the \textbf{Banach fixed point}, a new definition of convergence for FSSL, that is based on weight difference rather than the loss in supervised FL. This Theorem reveals that the system enters a progressive convergence stage when the weight increment difference between the server and clients decreases.
\item
We propose \textbf{FedIL}, a novel FSSL framework that can enable combined with any type of semi-supervised model based on pseudo labeling and enables learning server-client consistency between supervised and unsupervised tasks. The experiment results show that FedIL outperforms most of the state-of-the-art FSSL baselines on both IID and non-IID settings.
\end{itemize}
\section{Related Work}
\subsection{Federated Learning}
Traditional Federated Learning (FL) is a distributed training technique for acquiring knowledge from decentralized clients without transmitting raw data to the server \cite{mcmahan2017communication}. A number of methods for averaging local weights at the server have been developed in recent years. FedAvg \cite{mcmahan2017communication} is the standard algorithm for FL, which averages local weights to update the global weight according to the local training size. FedProx \cite{li2020federated} uniformly averages the local weights while clients execute proximal regularisation against the global weights. The aggregation policy introduced by PFNM \cite{yurochkin2019bayesian} makes use of Bayesian non-parametric methods. FedMA \cite{wang2020federated} matches the hidden elements with comparable feature extraction signatures when averaging local weights. In addition to the research of aggregation strategies, Non-IID data is also one of the most significant problems of FL \cite{wang2020federated}, causing weight divergence and performance degradation, as explained in \cite{zhao2018federated}. Numerous solutions to this problem have been proposed, including providing a public dataset \cite{zhao2018federated}, knowledge distillation \cite{zhuang2020performance}, and regularising client training \cite{li2020federated}.
\begin{figure*}[t]
  \centering
  [width=0.9\textwidth]{Client.pdf}
  \caption{\textbf{Illustrative Running Example of Client.} The client uses the global model $\theta^{'}$ and the server model $\theta_0$ to train the unlabeled data to get the client model $\theta^{*}$. It comprises an end-to-end training pipeline with three steps: 1) Each client selects pseudo-labels with higher confidence. 2) Move the data with pseudo-labels that are continuous and stable for a certain time to the pseudo-label set in each client. 3) Use KL loss during client training to enforce the prediction consistency between the predictions made by clients and the server.}
  \label{Client}
\end{figure*}
\subsection{Semi-supervised Learning}
Semi-supervised learning (SSL) combines supervised learning and unsupervised learning, which aims to improve model performance by leveraging unlabeled data \cite{rasmus2015semi,zhou2005tri,chapelle2009semi}. 
The majority of SSL methods fall into two categories: Consistency Regularization and Self-Supervised Learning. 
Consistency regularization \cite{sajjadi2016regularization} assumes that transformation of the input instances will have no effect on the class semantics and requires that the model output be consistent across input perturbations. UDA \cite{xie2020unsupervised} and ReMixMatch \cite{DBLP:conf/iclr/BerthelotCCKSZR20} use data augmentations to enforce the consistency between the representations of two versions of unlabeled data. Pseudo-Label \cite{lee2013pseudo} is a popular and mainstream semi-supervised technique that use hard (1-hot) labels from the model's prediction as labels on unlabeled data, implicitly minimizing the prediction's entropy. FixMatch \cite{DBLP:conf/nips/SohnBCZZRCKL20} selects pseudo-labels as consistency between weak-strong augmented pairs from unlabeled data using a fixed and unified threshold. 
Self-supervised learning obtains supervisory signals from the data itself. For example, contrastive learning\cite{hadsell2006dimensionality,oord2018representation} tries to reduce the similarity of positive samples while increasing the similarity of negative samples. The negative pairs are created from either a memory bank, such as MoCo \cite{he2020momentum}, or a huge batch size, such as SimCLR \cite{chen2020simple}, but these methods strongly rely on the computation resources. Methods such as BYOL \cite{grill2020bootstrap} and SimSiam \cite{chen2021exploring}  can avoid negative pairs and only compare positive ones, which can be used when computation resources are limited. After a pre-train model is generated by self-supervised learning, a small number of labeled data will be fed to this model.
\subsection{Federated Semi-supervised Learning}
The majority of existing FL research focuses on supervised learning problems using ground-truth labels provided by clients while most clients are unlikely to be specialists in many real-world tasks, which is faced as an issue in recent studies \cite{jin2020towards}.  
FSSL introduces learning representations from unlabeled decentralized data into FL and tries to overcome the labeling issue in traditional FL. 
Several solutions have been offered to realize FSSL by incorporating classical semi-supervised learning into the framework of federated learning. 
There are currently two FSSL scenarios, one is Labels-at-Client, and the other one is Labels-at-Server. For the Labels-at-Client, RSCFed \cite{liang2022rscfed} relies on label clients to assist in learning unlabeled data in other clients, while there is still a high risk of data leakage in this scenario. For Labels-at-Server, FedMatch \cite{jeong2021federated} focused on adapting the semi-supervised models to federated learning, which introduced the inter-client consistency that aims to maximize the agreement across models trained at different clients. FedU \cite{DBLP:conf/iccv/ZhuangG0ZY21} is based on the self-supervised method BYOL \cite{grill2020bootstrap}, which aims for representation learning. FedEMA \cite{zhuang2021divergence} is an upgraded version of FedU, which adaptively updates online networks of clients with EMA of the global model. Orchestra \cite{DBLP:conf/icml/LubanaTKDM22} relies on high representational similarity for related samples and low similarity across different samples.
Our FedIL consists of novel models that enable clients with just unlabeled data to complete semi-supervised training and the novel Cosine Similarity  with normalization based aggregation that accelerates the convergence of the global model in federated learning.
\section{Method}
In the following sections, we introduce FedIL, our proposed federated incremental learning approach shown in Figure \ref{Client} and Figure \ref{Server}. Section A will describe the scenario setting of Federated Semi-Supervised Learning. In the following sections B, C, and D, we will introduce the walkthrough procedures of the Client system, which has been illustrated as shown in Figure \ref{Client}. In section E, we will introduce training and weight selection of the Server system shown in Figure \ref{Server}. The summary of the whole system will be introduced in section F.
\subsection{Modeling and Federated Mapping}
Let the whole dataset for the training be $D$ as
\begin{equation}
    D=\left[X,y\right]=
    \begin{bmatrix}
X^s & y^s\\
X^u & y^u
\end{bmatrix}=
\begin{bmatrix}
D^s\\
D^u
\end{bmatrix}
\end{equation}
where $X$ refers to the input and $y$ refers to the label, $X^s$ refers to the input with known labels $y^s$, being the dataset $D^s=[X^s,y^s]$; while $X^u$ refers to the input without labels, and $y^u$ serves as unknown labels corresponding to $X^u$. 
The main task is to obtain the estimation $\bar{y}^u$ corresponding to the input $X^u$ with the mapping $f$ built based on $D$, which satisfies
\begin{equation}
\label{mappingFormula}
\left\lbrace
\begin{array}{ll}
    \min_f \|y^u-\bar{y}^u\|\\
    \bar{y}^u=f(X^u)
\end{array}
\right.
\end{equation}
in which $\|\cdot\|$ refers to the difference measurement.
Solving the mapping $f$ that satisfies~\eqref{mappingFormula} is the process of finding a function that can be fitted to the best mapping relationship by known ${D^s}$.
However, in the practical process, the unidentified label $y^u$ is unknown, so we cannot know exactly what the label corresponding to $X^u$ is. Therefore, we introduce a semi-supervised learning strategy for solving this mapping, which can be formulated as
\begin{equation}
\label{bary}
\bar{y}^u=f(X^u,\theta)
\end{equation}
in which $\theta$ refers to the weights for the learning.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
In federated learning, the learning system will take place in a distributed system that is composed of a Server and 
multiple Clients with index $i=1,2,\ldots,n$. Each client uses a dataset $D_i^u$ with unlabeled data $X_i^u$.
The server and all the clients have the same learning model $f$. 
Each local client learns a part of the data and submits its learning weights $\theta_i$ to the server, and the server (index $i=0$) collects and integrates weights, and then performs supervised learning on labeled dataset $D^s$.
The client-mapping corresponding to \eqref{bary} can be formulated as
\begin{equation}
\bar{y}_i^u = f({X}_i^u, \theta_i) \ \ \ \ \ \ i=1,2,\ldots,n
\end{equation}
and the optimal overall solution is obtained by
\begin{equation}
y^*=f(X,\theta^*)
\end{equation}
It can be seen that the main task now has been converted into finding the optimal parameter $\theta^*$ corresponding to~\eqref{mappingFormula} as
\begin{equation}
\label{thetaSol}
    \theta^* = \arg \min_{\theta} \|f(X,\theta)- y\|\\
\end{equation}
Once the $\theta^*$ is obtained the mapping $f$ can be determined.
Since $y^u \in y$ is unknown, it is impossible to solve $\theta$ by directly calculating the measure between the predicted label and the actual label of the domain, so we propose an iterative method to estimate $\theta^*$ based on $\bar{y}_i^u$ and $\theta_i$ with incremental similarity belief. 
\subsection{Hyper-Pseudo Pattern Space}
For the dataset $D^u=[X^u, y^u]$ with unknown labels, it is not feasible to use the difference between $\bar{y}^u$ and $y^u$ to construct a loss function for learning model updates. The $y^u$ are pseudo labels generated by the model strategies, which cannot be equivalent to the ground-truth labels and suffers from certain labeling errors. Therefore, we constructed the Hyper-Pseudo Pattern Space to approximate a pattern of loss model in FSSL, which is equivalent to the loss model for centralized learning built with $y^u$ and $\bar{y}^u$, through a set of nonlinear projections based on a set of augmentations of $X^u$ and corresponding estimations to the labels.
We expect to detect more features from the data itself to guess the commonality of their labels. Therefore, we introduce pseudo-data derivation from data and weights to generate hyper context. Specifically, we introduce three derivative mappings, including weak augmentation $f_A(\cdot)$, strong augmentation $F_A(\cdot)$, and 
the server reference $f(\cdot,\theta_0)$ to derive three estimations to the labels, as shown in Figure \ref{Client}, in which weakly and strongly augmentation sub-data of $X^u$ can be obtained by
\begin{equation}
\hat{X}^u = f_A(X^u) \ \ \ \ \ \ \ \widetilde{X}^u = F_A(X^u)
\end{equation}
and the estimated $\hat{y}^u, \widetilde{y}^u$ corresponding to $\hat{X}^u, \widetilde{X}^u$ are obtained by mapping $f$ as
\begin{equation}
\label{yest}
\hat{y}^u = f(\hat{X}^u, \theta^{'}) \ \ \ \ \ \ \ \widetilde{y}^u =f(\widetilde{X}^u, \theta^{'})
\end{equation}
and the server reference label 
can be obtained by
\begin{equation}
\label{pseudoOptimal}
y_0^u = f(\hat{X}^u, \theta_0)
\end{equation}
where $\theta_0$ refers to the weight from supervised learning in the server.
The client-mapping corresponding to \eqref{yest}-\eqref{pseudoOptimal} can be formulated as
\begin{equation}
\begin{aligned}
\hat{y}_i^u = f(\hat{X}_i^u, \theta^{'})  \ \ \ \widetilde{y}_i^u =f(\widetilde{X}_i^u, \theta^{'})  \ \ \ {y_0}_i^u = f(\hat{X}_i^u, \theta_0) \\ i=1,2,\ldots,n
\end{aligned}
\end{equation}
With the estimated labels, we can select a part of the estimated $y^u$ (pseudo labels) as reference labels to enhance the likelihood of classification learning for unlabeled samples. To distinguish the unselected $X^u, y^u$, we rewrite the selected pseudo-labels and their corresponding dataset as $D^l=[X^l,\hat{l}^{*u}]$, which are described in section D.
\subsection{Iterative Similarity Fusion}
Since there is no 
known label $y^u$ to determine the final optimal solution $f$ in~\eqref{mappingFormula}, we design an iterative approach to optimize the processing of 
the most probable optimal solution. On the basis of the above hyper-pseudo space, we use a number of different similarity comparisons and fusions for final confirmation and find an optimal solution that can optimize the fitting weights in different hyper-pseudo contexts.
Let $\theta_i(t)$ be the parameter of $i$th local client at $t$ training rounds. The local optimal parameter $\theta^*_i(t)$ is formulated as
\begin{equation}
\label{ISF}
    \theta_i^*(t) = f^b(\xi_i^a(t) + \xi_i^b(t))
\end{equation}
where $f^b$ refers to the back-propagation weight updating for $f$, $\xi^a$ is the Cross-entropy of the 
$\widetilde{y}_i^u$ and $\hat{y}_i^u$, and $\xi^b$ is the Kullback–Leibler divergence to measure how one probability distribution $\hat{y}_i^u$ is different from a reference probability distribution ${y_0}_i^u$, which can be formulated as
\begin{equation}
\left\lbrace
\begin{array}{ll}
     \xi_i^a(t)= \varepsilon(\max(\hat{y}_i^u(t))-\tau) H(\widetilde{y}_i^u(t),\arg\max(\hat{y}_i^u(t)))\\
\xi_i^b(t)=KL({y_0}_i^u(t),\hat{y}_i^u(t))
\end{array}
\right.
\end{equation}
in which $\varepsilon(x)$ refers to step function as
\begin{equation}
\label{stepFun}
\varepsilon (x)=
   \left\lbrace
   \begin{array}{ll}
1 \ \ x\geq 0\\
0 \ \ else
   \end{array}
   \right.
\end{equation}
$f^b$ here is equivalent to fusing the features obtained by each context and using it to update the weights of the clients.
In the standard semi-supervised learning methods, learning on labeled and unlabeled data is simultaneously done using a shared set of weights. However, this may result in forgetting knowledge of labeled data in disjoint learning scenarios, such as FSSL. FedIL ensures consistency between the class with the highest probability predicted by the local model and the server model when updating the pseudo-label dataset. However, FedIL wants to learn information about all classes instead of treating all negative labels uniformly when training a local model with an unselected unlabeled dataset. In the output of the softmax layer, other classes carry a lot of information besides the class with the highest probability. Therefore, in addition to the regular Cross-entropy loss $\xi^a(t)$ in semi-supervised models, we propose a consistency KL loss $\xi^b(t)$ that regularizes the models learned between the server and clients to output the same estimation.
\subsection{Incremental Credibility Learning with Dataset Updating}
For unlabeled training data, it is difficult to guarantee that pseudo-labels given by the system at the beginning are correct. We, therefore, design incremental confidences to observe whether their estimated labels always stabilize at a deterministic result after successive training and validation. Each continuous identification of a classification result will increase the credibility of its label. In the current FSSL scenario, labeled data and unlabeled data are disjoint. To enable complete semi-supervised training in the client, we design a dataset updating strategy by introducing a pseudo-label set to store the selected high-confidence data.
In order to integrate the credibility into the optimization for the parameter mapping solution, we further formulate the optimal incremental credibility learning as
\begin{equation}
    \theta_i^*(t) = f^b( \xi_i^a(t) + \xi_i^b(t) + \xi_i^c(t))
\end{equation}
in which $f^b(\cdot)$ refers to the back-propagation mapping to update the weight based on the loss input $(\cdot)$ and
\begin{equation}
    \xi^{c}(t)=H(y_{i}^{l}(t),\hat{l}_{i}^{*u}(t))
\end{equation}
where ${y}_i^l(t)$ refers to the predictions generated by the input $X^l_i$ using the model trained by $D_i^l(t)$, which can be formulated as
\begin{equation}
    {y}_i^l(t)=f(X^l_i,\theta_i(t))
\end{equation}
and $\hat{l}_i^{*u}(t)$ refers to the optimal pseudo-labels which satisfied two main principles, 1) the label has been consecutively selected as a pseudo-label no less than $T$ times; 2) the count of the compliance of the rule $\arg\max({y_0}_i^u(t))==\arg\max(\hat{y}_i^u(t)))$ has been no less than $T$ times.
The first time an unlabeled image is involved in training in $D_i^l(t)$ can be formulated as
\begin{equation}
    \hat{l}_i^{*u}(t)= \alpha (\hat{y}_i^u(t), \check{l}_i^u(t))
    \arg \max(\hat{y}_i^u(t-t^1_i))
\end{equation}
\begin{equation}
    \check{l}_i^u(t)= {\mathbbm{1}} (\arg \max({y_0}_i^u(t))==\arg\max(\hat{y}_i^u(t)))
\end{equation}
in which $ {\mathbbm{1}}$ refers to the selection function and
\begin{equation}
 \alpha(\hat{y}_i^u(t), \check{l}_i^u(t))= \prod_{k=1}^T \varepsilon (\max(\hat{y}_i^u(t-t^k_i))-\tau)
 \varepsilon (\check{l}_i^u(t-t^k_i)))
  
\end{equation}
When the result is continuous and stable for a long time after a series of confidence increments accumulate to reach a threshold, the data $X_i^u(t)$ and the label $\arg \max(\hat{y}_i^u(t))$ will be used as candidate credible samples as $\Delta D_i^l(t)$ and incorporated into the known label dataset $D_i^l(t+1)$ as a reference with unchanged labels for the following training rounds. 
The federated pseudo-label set updating can be formulated as
\begin{equation}
D_i^l(t+1) = [X^l_i(t+1),\hat{l}_i^{*u}(t+1)]\\
\end{equation}
where
\begin{equation}
   \left\lbrace
   \begin{array}{rl}
   D_i^l(t)&=\{X_i^l(t),\hat{l}_i^{*u}(t)\}\\
    X_i^l(t+1)&=X_i^l(t)\cup \Delta X_i^l(t)\\
    \Delta X_i^l(t)&=\{F_A^{-1}(\widetilde{y}_i^u(t),\theta_i(t))\ |
    \ \alpha(\hat{y}_i^u(t), \check{l}_i^u(t))=1\}\\
    \hat{l}_i^{*u}(t+1)&=\hat{l}_i^{*u}(t)\cup\Delta\hat{l}_i^{*u}(t) \\
    \Delta\hat{l}_i^{*u}(t)&=f(\Delta X_i^l(t),\theta_i(t))\\
   \end{array}
   \right.
\end{equation}
in which $F_A^{-1}$ refer to the inverse mapping of $F_A$.
\subsection{Global Incremental Learning}
As an iterative update of federated learning, we propose global incremental learning by \textbf{Cosine Similarity} with weight normalization to update the learning weights of each round which is shown in Figure \ref{Server}, that is, the final learning results of each round will be aggregated into $\theta'$ through the server ($i=0$) system and assigned to each client as the weights of the next round of initial training.
The parameter updating of the server ($i=0$) can be formulated in an iterative form as
\begin{equation}
   \theta_0(t)=f^b(H(y^s, \arg\max(y_0(t))))
\end{equation}
where $y_0(t)$ refers to the prediction based on the $X^s$ with the model $f$ and parameter $\theta'$, i.e.,
\begin{equation}
   y_0(t)=f(X^s,\theta'(t))
\end{equation}
in which $D^s=[X^s,y^s]$ refers to the supervised learning dataset with known labels $y^s$.
So the process of solving the model $f$ is equivalent to the process of optimizing $\theta'(t)$.
The aggregated updating can be formulated as
\begin{equation}
\label{GIL}
    \theta'(t+1) = \theta'(t)+\Delta \theta(t)
\end{equation}
where the $\Delta \theta(t)$ can obtained by \eqref{incrementalTheta}.
The incremental $\Delta \theta(t)$ can be formulated as
\begin{equation}
\label{incrementalTheta}
\Delta \theta(t) = \frac{\sum^n_{i=1} \varepsilon (S(t)) (\theta_i^*(t)-\theta'(t))}{\sum_{i=1}^n\varepsilon (S(t))}
\end{equation}
in which $S(t)$ refers to the cosine similarity between $\theta^*_i(t)-\theta'(t)$ and $\theta_0(t)-\theta'(t)$ that
\begin{equation}
    S(t)=\cos(\theta^*_i(t)-\theta'(t),\theta_0(t)-\theta'(t))
\end{equation}
\subsection{Summarization of the Framework}
FedIL well solves the problem of disjoint between labeled data and unlabeled data and achieves complete semi-supervised training by establishing its own pseudo-label set on each client. Furthermore, Global Incremental Learning helps the server to effectively select the uploaded client weights, which accelerates the convergence of the global model. Based on the federated learning scenario, we separately demonstrate the training process on the server in Algorithm \ref{alg1} and the training process on local clients in Algorithm \ref{alg2}. 
For Server-side training in Algorithm \ref{alg1}, we aggregate the selected weights from clients in the same direction as the server weight by cosine similarity with normalization. For example, we demonstrate how to derive $\theta^{'}(t=2)$. Let $t=1$, which means the first training round starts, and initialize global model $\theta^{'}(t=1)$ randomly. Take $\theta^{'}(t=1)$ as a reference point and train on the labeled data to get $\theta_0(t=1)$, and broadcast $\theta^{'}(t=1)$ and $\theta_0(t=1)$ to clients for training. After the first training round, select the client models $\theta^{*}_i(t=1)$ that are close to the server model $\theta_0(t=1)$ by using cosine similarity $\cos(\theta^{*}_i(t=1)-\theta^{'}(t=1),\theta_0(t=1)-\theta^{'}(t=1))$. Next, calculate the increment of the difference $\Delta \theta(t=1)$ between the selected client models and the global model by (25), and use (24) to get $\theta^{'}(t=2)$.
For Client-side training in Algorithm \ref{alg1}, we use KL loss to enforce Server-Client consistency and use Incremental Credibility Learning to build high-confidence Pseudo Label Sets.
For example, when selected clients receive the server weight $\theta_0(t)$ and the global weight $\theta^{'}(t)$ from the server, unsupervised training based on pseudo labeling will start on clients and we use Cross-entropy loss and KL loss to update Client weight $\theta^*_i(t)$. For Pseudo Label Set updating, when pseudo labels $\arg\max(\hat{y}_i^u(t))$ of one image are continuous and stable for a long period of time, the data $X_i^u(t)$ and the label $\arg \max(\hat{y}_i^u(t))$ will be used as candidate credible samples in $\Delta D_i^l(t)$ and incorporated into the known label dataset $D_i^l(t+1)$ as a reference with unchanged labels for the subsequent training rounds.  
\begin{algorithm}
    \caption{FedIL in the Server}
    \label{alg1}
\begin{algorithmic}[1]
    \FOR{each training round $t = 1,2,3...$}
        \IF{$t=1$} 
            \STATE randomly initialize $\theta^{'}(t=1)$
            \STATE \textbf{Processing with} (22) to update Server weight $\theta_0(t=1)$ 
            \STATE Randomly select 5 clients from 100 clients
            \STATE broadcast $\theta^{'}(t=1)$ and $\theta_0(t=1)$ to the next selected Clients
        \ELSE
            \STATE Received $\theta^*_i(t)$ from clients \\
            \COMMENT{\textit{received the weights from clients selected in the last round.}}
            \STATE \textbf{Processing with} (24)(25)(26) to update the global weight $\theta^{'}(t+1)$
            
            \STATE  \textbf{Processing with} (22)(23) to update Server weight $\theta_0(t+1)$
            \STATE Randomly select 5 clients from 100 clients
            \STATE broadcast $\theta^{'}(t+1)$ and $\theta_0(t+1)$ to the next selected Clients
        \ENDIF
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
    \caption{FedIL in selected Clients}
    \label{alg2}
\begin{algorithmic}[1]
    \FOR{each selected Client $i$ in parallel}
        \STATE Received the Server weight $\theta_0(t)$ and the global weight $\theta^{'}(t)$ from the Server
        \STATE \textbf{Processing with} (12)(13)(14)(15)(16) to update Client weight $\theta^*_i(t)$ 
        \FOR{each Pseudo Label Set updating}
            \STATE \textbf{Processing with} (17)(18)(19)(20)(21) to update Pseudo Label Set $D_i^l(t+1)$
        \ENDFOR
        \STATE upload $\theta^*_i(t)$ to the Server
    \ENDFOR
\end{algorithmic}
\end{algorithm}
\section{Convergence Analysis of FedIL}
In this section, we provide that the proposed global incremental learning possesses the theoretical justification of fixed point convergence, which can be proved as follows.
\begin{definition}
\label{defContractMapping}
Let $(X,d)$ be a complete metric space. Then a map $ T : X \to X$ is a contraction mapping on $X$ if $\exists q\in [0,1)$ such that
\begin{equation}
    d(T(x),T(y))\leq qd(x,y) \forall x,y\in X.
\end{equation}
\end{definition}
\begin{lemma}
\label{banachTh}
Banach Fixed Point Theorem. Let $(X,d)$ be a non-empty complete metric space with a contraction mapping $T:X\to X$. Then T admits a unique fixed-point $x^{*}$ in $X$ (i.e. $T(x^{*})=x^{*})$.
\end{lemma}
\begin{remark}
\normalfont{The fixed point $x^{*}$ in the non-empty complete metric space $(X,d)$ can be found as 
\begin{equation}
    \lim _{n\to \infty }x_{n}=\lim _{n\to \infty }T^n(x_{0})=x^{*}
\end{equation}
in which $x$ starts with an arbitrary element $x_{0}\in X$ and define a sequence with the contraction mapping as $(x_{n})_{n\in {\mathbb  N}}$ by $ x_{n}=T(x_{n-1})$ for $n\geq 1$.}
\end{remark}
\begin{theorem}
\label{THDtheta}
Let $(\Theta,d)$ be a non-empty norm space of parameter set, in which
$\Theta=\{\theta\},d(\theta)=\|\theta\|$.
Under the proposed Global Incremental Learning~\eqref{GIL}~\eqref{incrementalTheta}, $\exists \theta^* \in \Theta$ that
\begin{equation}
\lim_{t\to \infty }\theta'(t)=\theta^{*}
\end{equation}
if and only if the norm of $\Delta \theta(t)$ is monotonically decreasing with $t$ that
\begin{equation}
\label{dtheta}
    \frac{d \| \Delta \theta(t) \|}{dt} \leq 0
\end{equation}
\end{theorem}
\begin{proof}
\normalfont{In $(\Theta,d)$, define mapping $T: \Theta \to \Theta$ as
\begin{equation}
\label{defT}
T(\theta'(t))=\theta'(t)+\Delta \theta(t)=\theta'(t+1)
\end{equation}
where $\Delta \theta(t)$ is obtained by~\eqref{incrementalTheta}.
Therefore, 
\begin{equation}
d(T(\theta'(t)),T(\theta'(t-1)))=\|\theta'(t+1)-\theta'(t)\|=\|\Delta \theta(t)\|
\end{equation}
similarly, $d(\theta'(t),\theta'(t-1))=\|\Delta \theta(t-1)\|$.
\begin{equation}
\frac{d(T(\theta'(t)),T(\theta'(t-1)))}{d(\theta'(t),\theta'(t-1))}=\frac{\|\Delta \theta(t)\|}{\|\Delta \theta(t-1)\|}
\end{equation}
Since
\begin{equation}
\frac{d \| \Delta \theta(t) \|}{dt} \leq 0
\end{equation}
we have $\|\Delta \theta(t)\| \leq \|\Delta \theta(t-1)\|$ therefore, $\exists q\in(0,1)$
\begin{equation}
\label{banachdT}
d(T(\theta'(t)),T(\theta'(t-1)))\leq q d(\theta'(t),\theta'(t-1))
\end{equation}
where
\begin{equation}
\label{qRef}
     \frac{\|\Delta \theta(t)\|}{\|\Delta \theta(t-1)\|} \leq q <1
\end{equation}
Therefore, with Definition \ref{defContractMapping}, $T$ defined by \eqref{defT} is a contraction mapping, so the sufficiency of Theorem~\ref{THDtheta} is proved with
lemma~\ref{banachTh}.
When~\eqref{banachdT} holds, if
\begin{equation}
\label{dthetaGre}
    \frac{d \| \Delta \theta(t) \|}{dt} > 0
\end{equation}
which yields
\begin{equation}
\frac{\|\Delta \theta(t)\|}{\|\Delta \theta(t-1)\|}=
\frac{d(T(\theta'(t)),T(\theta'(t-1))}{d(\theta'(t),\theta'(t-1))}>1
\end{equation}
in which it is impossible to find $q\in (0,1)$ to satisfy~\eqref{banachdT}, which shows the contradiction and the necessity is proved with proof by contradiction.
Therefore, Theorem~\ref{THDtheta} holds if and only if the norm of $\Delta \theta(t)$ is monotonically decreasing with $t$.}
\end{proof}
Theorem~\ref{THDtheta} reveals the relationship among global learning $\theta'(t)$, local learning $\theta_i^*(t)$, server learning $\theta_0(t)$ and gives the necessary and sufficient condition for federated learning to enter the convergence stage. It is when the incremental difference $\Delta \theta(t)$ between the distributed local learning weights and the server weights decreases according to the norm $\|\Delta \theta(t)\|$, the global incremental learning weights $\theta'(t)$ gradually converge to the fixed point $\theta^*$ in the parameter space $\Theta$. 
The condition of Theorem 1 is only $\Delta\theta$ (weight different) monotonically decreasing for each training round, and the conclusion is $\Delta\theta$ approaching 0 as time goes on. The novelty of the Theorem is the new definition of convergence for FSSL that is based on weight difference rather than the loss in supervised FL. This is because client data are unlabeled in FSSL, and the loss is computed by pseudo-labels rather than ground-truth labels, which is unreliable and thus cannot guarantee model convergence. 
We are the first to use weight difference as a criterion rather than loss to determine model convergence in FSSL.
\begin{table*}[!t]
\caption{Method-Wise Accuracy Evaluation Stats on IID and non-IID settings of MNIST, CIFAR10 and CIFAR100 datasets. Our proposed FedIL outperforms most of the state-of-the-art methods.}
\centering
\small
\renewcommand\tabcolsep{6.0pt}
\begin{tabular}{lcccccccccc}
\hline
                 & \multicolumn{4}{c}{MNIST}                                                                                                                                                                                                                                                                               & \multicolumn{4}{c}{CIFAR10}                                                                                                                                                                                                                                                                             & \multicolumn{2}{c}{CIFAR100}                                                                                                        \\ \cline{2-11} 
                 & \multicolumn{2}{c}{IID}                                                                                                             & \multicolumn{2}{c}{non-IID}                                                                                                                                        & \multicolumn{2}{c}{IID}                                                                                                                                           & \multicolumn{2}{c}{non-IID}                                                                                                          & IID                                                              & non-IID                                                           \\ \hline
Label rate       & $\gamma$=0.01                                                           & $\gamma$=0.1                                                            & $\gamma$=0.01                                                                          & $\gamma$=0.1                                                                           & $\gamma$=0.01                                                                          & $\gamma$=0.1                                                                           & $\gamma$=0.01                                                           & $\gamma$=0.1                                                            & $\gamma$=0.1                                                            & $\gamma$=0.1                                                            \\ \hline
Fully-Supervised & \multicolumn{4}{c}{99.50\%±0.02}                                                                                                                                                                                                                                                                        & \multicolumn{4}{c}{93.59\%±0.03}                                                                                                                                                                                                                                                                        & \multicolumn{2}{c}{71.71\%±0.05}                                                                                                    \\ \hline
FedMatch         & \begin{tabular}[c]{@{}c@{}}97.13\%\\ ±0.15\end{tabular}          & \begin{tabular}[c]{@{}c@{}}98.89\%\\ ±0.21\end{tabular}          & \begin{tabular}[c]{@{}c@{}}97.22\%\\ ±0.23\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}98.28\%\\ ±0.26\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}55.30\%\\ ±0.17\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}82.20\%\\ ±0.29\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}54.26\%\\ ±0.13\end{tabular}          & \textbf{\begin{tabular}[c]{@{}c@{}}79.50\%\\ ±0.15\end{tabular}} & \begin{tabular}[c]{@{}c@{}}46.43\%\\ ±0.33\end{tabular}          & \textbf{\begin{tabular}[c]{@{}c@{}}46.60\%\\ ±0.15\end{tabular}} \\ 
FedU             & \begin{tabular}[c]{@{}c@{}}95.42\%\\ ±0.23\end{tabular}          & \begin{tabular}[c]{@{}c@{}}98.13\%\\ ±0.12\end{tabular}          & \begin{tabular}[c]{@{}c@{}}96.74\%\\ ±0.22\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}98.78\%\\ ±0.08\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}54.17\%\\ ±0.08\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}60.05\%\\ ±0.13\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}58.73\%\\ ±0.15\end{tabular}          & \begin{tabular}[c]{@{}c@{}}72.36\%\\ ±0.32\end{tabular}          & \begin{tabular}[c]{@{}c@{}}30.82\%\\ ±0.26\end{tabular}          & \begin{tabular}[c]{@{}c@{}}31.51\%\\ ±0.15\end{tabular}          \\ 
FedEMA           & \begin{tabular}[c]{@{}c@{}}97.17\%\\ ±0.21\end{tabular}          & \begin{tabular}[c]{@{}c@{}}98.90\%\\ ±0.13\end{tabular}          & \begin{tabular}[c]{@{}c@{}}96.26\%\\ ±0.13\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}98.60\%\\ ±0.25\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}54.85\%\\ ±0.09\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}63.73\%\\ ±0.32\end{tabular}                         & \begin{tabular}[c]{@{}c@{}}58.44\%\\ ±0.22\end{tabular}          & \begin{tabular}[c]{@{}c@{}}72.49\%\\ ±0.18\end{tabular}          & \begin{tabular}[c]{@{}c@{}}30.25\%\\ ±0.26\end{tabular}          & \begin{tabular}[c]{@{}c@{}}31.65\%\\ ±0.23\end{tabular}          \\ 
Orchestra        & \begin{tabular}[c]{@{}c@{}}96.57\%\\ ±0.21\end{tabular}          & \begin{tabular}[c]{@{}c@{}}97.86\%\\ ±0.13\end{tabular}          & \begin{tabular}[c]{@{}c@{}}95.97\%\\ ±0.28\end{tabular} & \begin{tabular}[c]{@{}c@{}}97.74\%\\ ±0.15\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.32\%\\ ±0.17\end{tabular} & \begin{tabular}[c]{@{}c@{}}67.17\%\\ ±0.23\end{tabular} & \begin{tabular}[c]{@{}c@{}}57.92\%\\ ±0.25\end{tabular}          & \begin{tabular}[c]{@{}c@{}}65.24\%\\ ±0.17\end{tabular}          & \begin{tabular}[c]{@{}c@{}}31.15\%\\ ±0.17\end{tabular}          & \begin{tabular}[c]{@{}c@{}}31.42\%\\ ±0.26\end{tabular}          \\ \hline
\textbf{FedIL}            & \textbf{\begin{tabular}[c]{@{}c@{}}98.30\%\\ ±0.21\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}99.05\%\\ ±0.13\end{tabular}} & \textbf{\begin{tabular}[c]{@{}c@{}}98.46\%\\ ±0.08\end{tabular}}                & \textbf{\begin{tabular}[c]{@{}c@{}}99.08\%\\ ±0.25\end{tabular}}                & \textbf{\begin{tabular}[c]{@{}c@{}}61.31\%\\ ±0.25\end{tabular}}                & \textbf{\begin{tabular}[c]{@{}c@{}}82.51\%\\ ±0.14\end{tabular}}                & \textbf{\begin{tabular}[c]{@{}c@{}}58.98\%\\ ±0.16\end{tabular}} & \begin{tabular}[c]{@{}c@{}}79.14\%\\ ±0.27\end{tabular}          & \textbf{\begin{tabular}[c]{@{}c@{}}48.25\%\\ ±0.17\end{tabular}} & \begin{tabular}[c]{@{}c@{}}46.39\%\\ ±0.25\end{tabular}          \\ \hline
\end{tabular}
\label{Acc}
\end{table*}
\section{Experiment}
\subsection{Experimental Setup}
\noindent\textbf{Datasets.}
We conduct our experiments using three public datasets, including MNIST \cite{lecun1998gradient}, CIFAR10 and CIFAR100 \cite{krizhevsky2009}. The MNIST dataset is a widely used dataset in the field of computer vision and machine learning. It consists of handwritten digit images and is considered a benchmark dataset for image classification tasks. This dataset is split into a 60,000-image training set and a 10,000-image test set. There are 60,000 32x32 color images in CIFAR10 and CIFAR100, where the training set has 50,000 images and the test set has 10,000 images. The CIFAR10 and CIFAR100 datasets, on the other hand, contain natural images belonging to 10 and 100 classes respectively. These datasets contain a large number of color images, making them a popular choice for testing the performance of image classification algorithms. Both CIFAR datasets have been used extensively in the research community to evaluate the performance of various computer vision and machine learning models.
\noindent\textbf{Federated System Setting.}
The whole dataset $D$ will be randomly divided into two separate groups: a labeled set $D^s$ and an unlabeled set $D^u$. The labeled set will be held by the server, while the unlabeled set will be further divided and distributed among $K=100$ clients.
Each client will receive $\tfrac{|D^u|}{K}$ instances for the independent and identically distributed (IID) setting and 20\% of the total classes for the non-IID (Not IID) setting. The quantity of unlabeled training data is represented by $|D^u|$.
To represent the ratio of labeled data in the entire training dataset, the variable $\gamma$ is introduced. This means that there will be $\gamma * |D|$ labeled samples on the server and $\tfrac{(1-\gamma)*|D|}{K}$ unlabeled data on each client. This distribution is designed to ensure an equal distribution of labeled and unlabeled data among the clients and the server.
We set $\gamma=0.01, 0.1$ in the experiments, and only 5 local clients are working in each round. The total training rounds are 2000.
\noindent\textbf{Baselines.}
To fairly evaluate the proposed FedIL framework, we use the same backbone ResNet9 and the following state-of-the-art benchmarks. \textbf{1) Fully-Supervised}: centralized supervised training for the whole labeled datasets.
\textbf{2) FedMatch} \cite{jeong2021federated}: use FedAvg-FixMatch with inter-client consistency and parameter decomposition to train the models.
\textbf{3) FedU} \cite{DBLP:conf/iccv/ZhuangG0ZY21}: use the divergence-aware predictor updating technique with self-supervised BYOL \cite{grill2020bootstrap} to train the models. 
\textbf{4) FedEMA} \cite{zhuang2021divergence}:  use an Exponential Moving Average (EMA) of the global model to adaptively update online client networks. 
\textbf{5) Orchestra} \cite{DBLP:conf/icml/LubanaTKDM22}: use a novel clustering-based FSSL technique. 
\subsection{Experimental Results}
\noindent\textbf{Comparison with FSSL methods.} We demonstrate our experimental results in Table \ref{Acc}, which shows a performance summary using MNIST, CIFAR10 and CIFAR100 by different FSSL methods with the comparison of accuracy and parameters in the scenarios of IID and non-IID.
The method FedIL we use in our experiments is limited in the sense that it only utilizes the training dataset and the test dataset, and does not incorporate the validation dataset. This is in contrast to FedMatch, which leverages the validation dataset to optimize its weight aggregation by adjusting its parameters in the server. The validation dataset plays a crucial role in this process as it consists of labeled data, and hence allows for a supervised learning approach. By using the validation dataset, FedMatch effectively learns from two labeled datasets, namely $D^{s1}$ which is the labeled data in the training dataset, and $D^{s2}$ which is the labeled data in the validation dataset. This results in better performance compared to our method, which only utilizes a single labeled dataset.
In practice, this solution is not fully applicable, especially when the amount of labeled data is limited. In such cases, dividing the limited labeled data into separate datasets for training and validation becomes problematic as there may not be enough labeled samples to ensure a representative and balanced split. Over-fitting is a common issue in these scenarios, where the model becomes overly reliant on the limited labeled data and fails to generalize well to unseen data.
To address these limitations, our model employs a different approach to solving for parameters. It uses an incremental credibility-based process that is more in line with the practical application of semi-supervised learning. This method reduces the amount of labeled data required for training, making it more suitable for scenarios where the labeled data is limited. Additionally, our model can also be extended to incorporate validation data if available, making it more flexible and applicable in a wider range of scenarios.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\noindent\textbf{Trend of incremental $\Delta\theta(t)$.}
The proposed incremental credibility approach ensures that the learning process is gradually convergent. As illustrated in Figure~\ref{Theta}, the trend of $\Delta \theta(t)$ during the incremental learning of the proposed method can be seen, where $\overline{\Delta\theta(t)}$ represents the moving average of $\Delta\theta(t)$. Despite any perturbations that may occur during the learning process, the overall trend remains one of gradual decline, demonstrating that the proposed federated incremental strategy effectively solves the optimal mapping based on model parameter learning and leads to a convergent solution process. The experimental results further confirm the criterion outlined in Theorem~\ref{THDtheta}, indicating that the system's solution process, designed based on the criteria defined in Theorem~\ref{THDtheta}, will eventually converge to a fixed point.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\section{Ablation Study}
\subsection{The Impact of Number of Clients.}
The impact of the number of clients participating in each training round on the performance of the global model can be seen in Figure~\ref{fig5}. Our experiments on the CIFAR10 dataset, conducted over 1000 training rounds, demonstrate a clear correlation between the number of clients selected and the global model performance. As the number of clients increases from 5 to 20, we observe a corresponding improvement in the global model's performance. However, beyond a certain point, we see that further increases in the number of clients lead to a decrease in the global model's performance. This is likely due to the fact that the data on the clients is unlabeled and incorporating too much of this unknown data in the early stages of training can slow down the convergence of the model.
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
\subsection{The Influence of Number of Local Training Epochs.}
The influence of the local training epochs in each training round on the performance of the global model is demonstrated in Figure~\ref{fig6}. The data on the clients being unlabeled, and conducting too many epochs in each client during the early stages of training can have a negative effect on the performance of the model. Our experiments, conducted on the CIFAR10 dataset over 1000 training rounds, show that the optimal number of local training epochs is 5. The results suggest that it is crucial to strike a balance between the number of epochs and the stage of training in order to maintain the optimal performance of the global model.
\begin{table}[h!]
\caption{Modules Analysis in FedIL.}
\centering
\small
\renewcommand\tabcolsep{5.0pt}
\begin{tabular}{lc}
\hline
Modules                                              & Acc   \\ \hline
FedIL                                                & 61.31 \\
FedIL without pseudo-label set                       & 60.77 \\
FedIL without Cosine Similarity                      & 28.59 \\
FedIL without a pseudo-label set and Cosine Similarity & 27.21 \\ \hline
\end{tabular}
\label{ablation}
\end{table}
\subsection{The Evaluation of Different Modules}
For the necessity of each module, we also did an ablation analysis. The results, as presented in Table~\ref{ablation}, demonstrate that even without the use of the pseudo-label candidate mechanism and the similar incremental screening strategy, the proposed method still achieved a close-to-30\% accuracy within a limited number of training rounds. The incorporation of the candidate label mechanism resulted in a 1\% improvement in accuracy, while the use of the similarity adjustment screening strategy resulted in an increase to over 60\%. When both the candidate tag mechanism and the similarity incremental screening were utilized simultaneously, the accuracy reached over 61\%. This highlights the significant impact of a similar incremental strategy on performance and the contribution of the candidate labeling mechanism toward accelerating the learning process. The core of this method involves the continuous enhancement of label identification by the upper-level server calculation, and the selection of candidate labels for semi-supervised learning tasks through similar incremental learning at each local client calculation in the network, which iterates over time.
\begin{table}[h!]
\caption{Accuracy Stats of FedIL on Various Count of Successive Labeling $k$. The dataset used here is CIFAR10.}
\centering
\small
\renewcommand\tabcolsep{1.0 pt}
\begin{tabular}{lccccccc}
\hline
\multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Count of Successive \\ Labeling $k$\end{tabular}} & 2                                                     & 3                                                     & 4                                                     & 5                                                     & 6                                                     & 7                                                              & 8                                                     \\ \hline
Accuracy                                                                                      & \begin{tabular}[c]{@{}c@{}}52.95\\ ±0.13\end{tabular} & \begin{tabular}[c]{@{}c@{}}54.03\\ ±0.35\end{tabular} & \begin{tabular}[c]{@{}c@{}}54.10\\ ±0.26\end{tabular} & \begin{tabular}[c]{@{}c@{}}59.51\\ ±0.17\end{tabular} & \begin{tabular}[c]{@{}c@{}}60.27\\ ±0.26\end{tabular} & \begin{tabular}[c]{@{}c@{}}61.31\\ ±0.25\end{tabular} & \begin{tabular}[c]{@{}c@{}}62.03\\ ±0.37\end{tabular} \\ \hline
\end{tabular}
\label{tab1}
\end{table}
\subsection{Count of Successive Labeling $k$ and Threshold $\tau$}
According to the trend shown in Table~\ref{tab1}, it is evident that there is a trade-off between the reliability of the candidate tag data and calculation time. A larger value of $k$ results in more reliable candidate labels, but it also increases the calculation time. Hence, when selecting parameters, it is essential to consider not only the performance metrics but also the distribution characteristics, computational costs, and time constraints. In our study, we have chosen $k=7$ and threshold $\tau$ equal to 0.95, which is consistent with the parameters used in the Fixmatch \cite{DBLP:conf/nips/SohnBCZZRCKL20}. By balancing the competing demands of accuracy and efficiency, we aim to achieve optimal results in our experiments.
\section{Conclusion}
This paper proposes Federated Incremental Learning (FedIL) with an incremental credibility learning strategy to select highly credible pseudo-labels to join the pseudo-label set to establish complete semi-supervised training on unknown samples in each client. We utilize cosine similarity to select client weights and propose a standardized global incremental learning framework to speed up training models while ensuring server-client consistency between learning supervised and unsupervised tasks. Based on Banach's fixed point theorem, we further prove the necessary and sufficient conditions for the convergence of global incremental learning, revealing that the system enters a progressive convergence stage when the weight increment difference between server and client decreases.
Experimental validation shows that the proposed FedIL outperforms most of the FSSL baselines on common public datasets, thus demonstrating the feasibility of the proposed FedIL mechanism for performance improvement in federated semi-supervised learning by determining the estimation of unlabeled data with incremental learning. Due to the existing hardware limitations of client devices, federated learning can only be implemented with small backbones and datasets, e.g., MNIST, CIFAR10 and CIFAR100. In future work, we will investigate the applicability of large-scale datasets, e.g., ImageNet, in the FSSL scenario.
{ieeetr}
"
49,"\importpackages{}
\graphicspath{ {./images/} }


  
\maketitle
\begin{abstract}
When factorized approximations are used for variational inference (VI), they tend to understimate the uncertainty---as measured in various ways---of the distributions they are meant to approximate.
We consider two popular ways to measure the uncertainty deficit of VI: (i) the degree to which it underestimates the componentwise variance, and (ii) the degree to which it underestimates the entropy.
To better understand these effects, and the relationship between them, we examine an informative setting where they can be explicitly (and elegantly) analyzed:
the approximation of a Gaussian,~$p$, with a dense covariance matrix, by a Gaussian,~$q$, with a diagonal covariance matrix.
We prove that $q$ always underestimates both the componentwise variance and the entropy of $p$, \textit{though not necessarily to the same degree}.
Moreover we demonstrate that the entropy of $q$ is determined by the trade-off of two competing forces: it is decreased by the shrinkage of its componentwise variances (our first measure of uncertainty) but it is increased by the factorized approximation which delinks the nodes in the graphical model of $p$.
We study various manifestations of this trade-off, notably one where, as the dimension of the problem grows, the per-component entropy gap between $p$ and $q$ becomes vanishingly small even though $q$ underestimates every componentwise variance by a constant multiplicative factor.
We also use the shrinkage-delinkage trade-off to bound the entropy gap in terms of the problem dimension and the condition 
number of the correlation matrix of $p$.
Finally we present empirical results on both Gaussian and non-Gaussian targets, the former to validate our analysis and the latter to explore its limitations.
\end{abstract}
\section{Introduction}
Variational inference (VI) is a popular methodology for approximate Bayesian inference \citep{Jordan:1999, Wainwright:2008, Blei:2017}.
Given a target distribution,~$p$, VI searches for a tractable distribution, $q \in \mathcal Q$, that minimizes the Kullback-Leibler (KL) divergence to~$p$. 
A common choice for $\mathcal Q$ is to use a family of factorized distributions.
The KL-divergence can then be optimized in a scalable manner for high-dimensional distributions \citep{Wainwright:2008}, which is crucial, for instance, to train models such as variational auto-encoders over large data sets \citep{Kingma:2013}.
Factorized VI has its roots in the mean-field approximations to certain Gibbs distributions from statistical physics~\citep{Parisi:1988,Mackay:2003}. 
In this approach, the approximating distribution is modeled as
\begin{equation}
  q({\bf z}) = \prod_{i = 1}^n q(z_i).
\label{eq:FVI}
\end{equation}
In most applications, the target distribution $p({\bf z})$ does \underline{not} factorize.
By its very nature, factorized VI cannot estimate the correlations between different elements of ${\bf z}$.
A more subtle shortcoming of factorized VI is that it also fails to correctly estimate the marginal distributions, $p(z_i)$.
This failure typically manifests as an approximation $q$ with an uncertainty deficit relative to $p$, a phenomenon which has been studied both empirically and theoretically~\citep[e.g][]{Mackay:2003, Wang:2005, Bishop:2006, Turner:2011, Blei:2017, Giordano:2018}.
There exists several measures of uncertainty, and we focus on two: (i) the componentwise variance and (ii) the entropy.
The componentwise variance plays a crucial role in Bayesian modeling, especially when estimating the posterior distribution over interpretable variables.
Meanwhile the entropy provides a multivariate notion of uncertainty and, in statistical physics, can be linked to the free energy, a quantity of interest for many problems.
Intuitively, we expect factorized VI to shrink the variance of $q$ to minimize its overlap with the tails of $p$.
It is less clear how it should affect the entropy of $q$: on the one hand, this entropy is decreased by any shrinkage in the variance, but it is increased
by the factorized approximation, which delinks the nodes in the full-covariance graphical model of $p$.
Hence entropy is driven by a trade-off between two competing forces. We call this the \textit{shrinkage-delinkage trade-off}.
This trade-off hints that the adequacy of factorized VI may depend on the way we elect to measure its uncertainty deficit.
The goal of this paper is to understand the uncertainty deficit of factorized VI in the most informative setting where it can be rigorously analyzed.
To this end, we study the special case where $p$ is a Gaussian distribution over $\mathbb R^n$ with a full covariance matrix and $q$ is a Gaussian distribution over $\mathbb R^n$ with a diagonal covariance matrix.
This choice of $q$ is natural when $p$ is a multivariate distribution over ${\bf z} \in \mathbb R^n$, and leads to factorized Gaussian variational inference (FG-VI)---a popular method among practionners due notably to ``black box'' implementations such as automatic differentiation variational inference (ADVI) \citep{Kucukelbir:2017}.
Our paper expands on previous analyses of FG-VI \citep{Bishop:2006, Turner:2011} in many ways, but perhaps most significantly by identifying---and elucidating---the shrinkage-delinkage trade-off of factorized VI, which in the considered setting can be written explicitly.
Our analysis is grounded in two fundamental inequalities. First we show that if $p$ is multivariate Gaussian, and if~$q$ is the distribution (optimally) estimated by FG-VI, then
\begin{equation}
\text{Var}_q(z_i) \le \text{Var}_p(z_i). 
\label{eq:ineq1}
\end{equation}
Second, under the same assumptions, we show~that
\begin{equation}
{\cal H}(q) \le {\cal H}(p),
\label{eq:ineq2}
\end{equation}
where ${\cal H}(\cdot)$ denotes the entropy. 
This second inequality, relating the entropies of $p$ and $q$, formalizes an observation~\citep{Mackay:2003,Bishop:2006} that $q$ tends to be more ``compact'' than~$p$.
Our proofs of these inequalities hold generally for Gaussian distributions over $\mathbb{R}^n$; to the best of our knowledge, they are more direct and more general than previous demonstrations
While both inequalities reveal an uncertainty deficit, we will see that the two notions of uncertainty are not equivalent.
Indeed, we provide one example where $q$ underestimates each componentwise variance by a constant multiplicative factor, but the per-component entropy gap between $p$ and $q$ can be arbitrarily small.
 This discrepancy arises because the entropy gap in FG-VI is in fact \textit{equal} to the KL divergence minimized by FG-VI when it targets a multivariate Gaussian.
But, as we will see, this choice of objective function can harm the estimation of marginal variances.
The inequalities in eq.~(\ref{eq:ineq1}--\ref{eq:ineq2}) anchor our subsequent analysis. As shown in Figure~\ref{fig:shrinkage}, the amount of shrinkage in FG-VI depends in general on the number of components of~${\bf z}\in\mathbb{R}^n$ as well as the degree of correlation between these components. 
With this motivation, we derive an upper bound on the entropy gap in eq.~(\ref{eq:ineq2}) in terms of the problem dimensionality,~$n$, and the condition number of the true correlation matrix.
Finally we examine the relevance for some of our findings when FG-VI is applied to non-Gaussian target distributions. For these experiments, we draw on several examples from the Bayesian literature.
We find that, while the variance shrinkage (\ref{eq:ineq1}) does not hold systematically, it holds on average in the considered examples.
We do not have a reliable method to empirically estimate the entropy, but make an argument that eq.~(\ref{eq:ineq2}) may hold in the studied examples.
Our results build on those of many previous studies. \citet{Mackay:2003}, \citet{Bishop:2006}, \citet{Turner:2011}, and \citet{Blei:2017} all use a two-dimensional Gaussian to illustrate that the approximations from VI are more ``compact'' than the distributions they target. We formalize this observation in the general $n$-dimensional setting, while highlighting the difference between componentwise variance and entropy as measures of uncertainty---a difference that becomes more critical in high-dimensional settings.
Experiments on non-Gaussian models also suggest a more nuanced picture, showing for instance that FG-VI does not always underestimate every componentwise variance, though in the studied examples variance shrinkage holds \textit{on average}. 
Previous studies have also examined other measure of uncertainty, such as the frequentist intervals obtained by variational Bayes estimators \citep[e.g.][]{Wang:2005}. Finally, many have been motivated by the uncertainty deficit of factorized VI to develop new methods for inference. These include post-hoc corrections of variational approximations \citep{Giordano:2018} or, for certain models, careful decompositions of $p$ using conditional distributions to justify the assumption of factorization \citep{Agrawal:2021}.
The code for all results and figures is available on \href{https://github.com/charlesm93/variance-delinkage}{GitHub}.
\begin{figure*}
    \centering
    [width = 6in]{figures/shrinkage.pdf}
    \caption{\textit{FG-VI's approximation of a multivariate Gaussian whose correlation matrix has constant off-diagonal terms. FG-VI's variance shrinkage grows with both increasing dimensionality ($n$) and correlation ($\varepsilon$). For $n=64$, the distributions are projected onto their first two coordinates.
    Despite what the picture suggests, the entropy gap between the approximation and the target is actually quite small (Section~\ref{sec:corr}).
    In this sense, the lower-dimensional projection is misleading.
    }}
    \label{fig:shrinkage}
\end{figure*}
\section{Preliminaries}
We analyze FG-VI in the setting where $p(\mathbf{z})$ is multivariate Gaussian with mean $\boldsymbol\mu\in\mathbb{R}^n$ and covariance $\boldsymbol\Sigma\in\mathbb{R}^{n\times n}$. In this setting FG-VI has a particularly simple solution. (An earlier statement of this solution can be found in \citet{Turner:2011}.)
\begin{proposition} \label{prop:solution}
 Let $q(\mathbf{z})$ be multivariate Gaussian with mean $\boldsymbol\nu$ and diagonal covariance~$\boldsymbol\Psi$.
 Then the variational parameters minimizing $\text{KL}(q||p)$ are given by $\boldsymbol\nu=\boldsymbol\mu$ and
\begin{equation}
    \Psi_{ii} = \frac{1}{\Sigma^{-1}_{ii}},
\label{eq:Psi}
\end{equation}
where the denominator $\Sigma_{ii}^{-1}$ denotes a diagonal element of the matrix inverse~$\boldsymbol\Sigma^{-1}$.
\end{proposition}
\begin{proof}
The variational parameters $\boldsymbol\nu$ and $\boldsymbol\Psi$ are estimated by minimizing the KL-divergence
\begin{equation}
  \text{KL}(q || p) = \mathbb E_q[\log q({\bf z})] - \mathbb E_q[\log p({\bf z})],
  \label{eq:KL}
\end{equation}
where each expectation is taken with respect to the measure~$q$. Note that only the second term in eq.~(\ref{eq:KL}) depends on the variational mean $\boldsymbol\nu$, and it is given by
\begin{equation}
-\mathbb E_q[\log p({\bf z})] = \tfrac{1}{2}(\boldsymbol\nu\!-\!\boldsymbol\mu)^\top\boldsymbol\Sigma^{-1}(\boldsymbol\nu\!-\boldsymbol\mu)\, +\, \ldots
\end{equation}
where the ellipses indicate terms that do not depend on~$\boldsymbol\nu$. By minimizing this expression, it follows at once that $\boldsymbol\nu=\boldsymbol\mu$. With this substitution, eq.~(\ref{eq:KL}) simplifies to
\begin{equation}
 \text{KL}(q || p) = \tfrac{1}{2}\left[\text{trace}\big(\boldsymbol\Psi\boldsymbol\Sigma^{-1}\big) - \log\big|\boldsymbol\Psi\boldsymbol\Sigma^{-1}\big| - n\right],
\label{eq:KL-gaussian}
\end{equation}
and the result in eq.~(\ref{eq:Psi}) follows by minimizing the above expression with respect to the diagonal elements of $\boldsymbol\Psi$.
\end{proof}
In sections 2, 3, and 4 of the paper, we assume that $q$ is the factorized Gaussian distribution whose variances are given by eq.~(\ref{eq:Psi}). {\it We emphasize in general that} $\Psi_{ii}\neq\Sigma_{ii}$. However, it is true that $\boldsymbol\Psi=\boldsymbol\Sigma$ when $\boldsymbol\Sigma$ is diagonal.
Many of our results will not be expressed directly in terms of $\boldsymbol\Sigma$ and $\boldsymbol\Psi$, but in terms of two related (but dimensionless) matrices. The first is the {\it correlation matrix}~$\mathbf{C}$ with elements
\begin{equation}
C_{ij} = \frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}}.
\label{eq:C}
\end{equation}
Note that $C_{ii}=1$, a simple fact that we will often exploit, and also that $\mathbf{C}$ reduces to the identity matrix when $\boldsymbol\Sigma$ is diagonal.
At the other extreme, we may consider the case where all the off-diagonal elements of $\mathbf{C}$ are equal to some constant $\varepsilon\!>\!0$. This is explored visually in Figure.~(\ref{fig:shrinkage}). In appendix A we show that $\Psi_{ii}\rightarrow 0$ as $\varepsilon\rightarrow 1$ for fixed $n$, and that $\Psi_{ii}\rightarrow (1\!-\!\varepsilon)\Sigma_{ii}$ as $n\rightarrow\infty$ for fixed~$\varepsilon$. Note that FG-VI underestimates the variance in both limits.
In addition to the correlation matrix, we also define the diagonal \textit{shrinkage} matrix~$\mathbf{S}$ with dimensionless entries
\begin{equation}
    S_{ii} = \frac{\Sigma_{ii}}{\Psi_{ii}} = \Sigma_{ii}\,\Sigma^{-1}_{ii}.
    \label{eq:S}
\end{equation}
We will use the matrices $\mathbf{C}$ and $\mathbf{S}$ to analyze how FG-VI underestimates the uncertainty of $p$. The uncertainty in axis-aligned directions is measured by the variances $\Sigma_{ii}$, but a multivariate measure of uncertainty is provided by the entropy
\begin{equation}
  \mathcal H(p) = - \mathbb E_p \log p({\bf z}).
\label{eq:entropy}
\end{equation}
To what extent does FG-VI underestimate this entropy? As shown next, the answer is very naturally expressed in terms of the correlation matrix $\mathbf{C}$ and the shrinkage matrix $\mathbf{S}$.
\begin{proposition} 
Let $p$ and $q$ be defined as above. Then their difference in entropy is given by
    \begin{equation}
      \mathcal H(p) - \mathcal H(q)
      = \tfrac{1}{2} \log |\mathbf{S}| - \tfrac{1}{2} \log |\mathbf{C}|^{-1}.
      \label{eq:entropy-loss}
    \end{equation}
\end{proposition}
\begin{proof}
A standard calculation for multivariate Gaussian distributions~\citep{Cover:2006} gives 
\mbox{$\mathcal H(p) = \frac{1}{2} \log |\Sigma|(2\pi e)^n$}, and
an analogous result holds for~$\mathcal H(q)$. Let $\Delta\mathcal H = \mathcal H(p)-\mathcal H(q)$. Then we see that
\begin{equation}
  \Delta\mathcal H 
    = \tfrac{1}{2} \log|\mathbf{\Sigma}| - \tfrac{1}{2}|\log{\mathbf{\Psi}}| 
    = \tfrac{1}{2}\log \left|\mathbf{\Psi}^{-\frac{1}{2}} \mathbf{\Sigma} \mathbf{\Psi}^{-\frac{1}{2}}\right|.
    \label{eq:entropy-loss2}
  \end{equation}
Now from the definitions in eqs.~(\ref{eq:C}--\ref{eq:S}), it can be verified by direct substitution that
    $\boldsymbol\Psi^{-\frac{1}{2}} \boldsymbol\Sigma\, \boldsymbol\Psi^{-\frac{1}{2}} = 
    \mathbf{S}^\frac{1}{2}\mathbf{C}\,\mathbf{S}^\frac{1}{2}$.
It follows from the basic properties of determinants that
\begin{equation*}
  \Delta\mathcal H 
  = \tfrac{1}{2} \log \big|\mathbf{S}^\frac{1}{2} \mathbf{C} \mathbf{S}^\frac{1}{2}\big| = 
  \tfrac{1}{2}\log|\mathbf{S}| - \tfrac{1}{2}\log|\mathbf{C}|^{-1}.
\end{equation*}
\end{proof}
 
\section{\mbox{\!\!Shrinkage-Delinkage Trade-off}} \label{sec:trade-off}
In this section we prove that FG-VI systematically underestimates the variance and entropy of a multivariate Gaussian distribution. We will see, however, that a large shrinkage in {\it all} componentwise variances does not imply a correspondingly large shrinkage in the entropy.
\subsection{Fundamental inequalities}
  
\begin{theorem}[Variance shrinkage]
\label{thm:shrinkage-variance}
The solution for FG-VI in eq.~(\ref{eq:Psi}) underestimates the variance; that is,
\begin{equation}
    \Psi_{ii} \le \Sigma_{ii},
\label{eq:variance-shrinkage}
\end{equation}
and the inequality is strict for some component of the variance (i.e., $\Psi_{ii}<\Sigma_{ii}$) if $\mathbf{\Sigma}$ is not purely diagonal.
\end{theorem}
  
\begin{proof}
Let $\mathbf{C}$ denote the correlation matrix in eq.~(\ref{eq:C}).
It can be verified by direct calculation that
\begin{equation}
 C^{-1}_{ij} = \Sigma^{-1}_{ij}\sqrt{\Sigma_{ii}\Sigma_{jj}}.
 \label{eq:invC}
\end{equation}
As further notation, let $\lambda_1,\ldots,\lambda_n$ denote the eigenvalues of $\mathbf{C}$, and
let $\mathbf{e}_i$ denote the unit vector along the $i^{\rm th}$ axis. Then from the solution in eq.~(\ref{eq:Psi}), it follows that
\begin{eqnarray}
  \frac{\Sigma_{ii}}{\Psi_{ii}}
    &=& C^{-1}_{ii}, \label{eq:Sii} \\
    &=& C^{-1}_{ii} + C_{ii} - 1, \\ 
    &=& \mathbf{e}_i^\top(\mathbf{C}^{-1}\! + \mathbf{C}\,)\,\mathbf{e}_i - 1, \\
    &\geq& \min_{\|\mathbf{e}\|=1} [\mathbf{e}^\top(\mathbf{C}^{-1}\! + \mathbf{C}\,)\,\mathbf{e} - 1], 
    \label{eq:eigen-ineq} \\
    &=& \min_i\, (\lambda_i^{-1} + \lambda_i - 1), \\
    &\geq& \min_{\lambda>0}\, (\lambda^{-1} + \lambda - 1) = 1, 
    
\end{eqnarray}
where in the last step we have used the fact that the correlation matrix $\mathbf{C}$ has strictly positive eigenvalues. This proves eq.~(\ref{eq:variance-shrinkage}). Now suppose that $\mathbf{\Sigma}$ is not purely diagonal. Then~$\mathbf{C}$ is also not diagonal; hence there must be some unit vector~$\mathbf{e}_i$ that is not an eigenvector of $\mathbf{C}$. In this case the inequality in eq.~(\ref{eq:eigen-ineq}) is strict, showing that $\Sigma_{ii}>\Psi_{ii}$.
\end{proof}
Next we examine the difference in entropy given by eq.~(\ref{eq:entropy-loss}). First we show that this difference is determined by the trade-off of competing entropic forces.
\begin{theorem}[The Shrinkage-Delinkage Tradeoff]
Consider the entropy difference in eq.~(\ref{eq:entropy-loss}) from FG-VI:
     \begin{equation*}
        \mathcal H(p) - \mathcal H(q) = \tfrac{1}{2} \log|\mathbf{S}| - \tfrac{1}{2} \log |\mathbf{C}|^{-1}.
      \end{equation*}
      Both terms in this difference are nonnegative: that is,
      \begin{align}
      \log |\mathbf{S}|\ &\geq\ 0 \label{eq:logS},\\
      \log \frac{1}{|\mathbf{C}|} &\geq\ 0. \label{eq:logC}
      \end{align}
      \label{thm:trade-off}
  \end{theorem}
\vspace{-3ex}
Before proving the theorem we consider the meaning of these inequalities. Conceptually, the first inequality shows that any shrinkage of variances (from Theorem~\ref{thm:shrinkage-variance}) reduces the entropy of $q$ and thus contributes to a larger difference in eq.~(\ref{eq:entropy-loss}). The second inequality shows that the factorization of $q$ acts as a counterbalance to this effect: the entropy of~$p$ is necessarily reduced by the presence of correlations, but such correlations cannot be modeled by~$q$. Thus the factorization of $q$ must (to some extent) oppose the entropy difference in eq.~(\ref{eq:entropy-loss}), and the net difference is determined by the trade-off of these forces. Visually the factorization of $q$ is represented by the delinkage of nodes in the full-covariance graphical model for $p$. This is the essence of the {\it shrinkage-delinkage}~tradeoff for FG-VI.
\begin{proof}
The bound on $\log|\mathbf{S}|$ in eq.~(\ref{eq:logS}) follows at once from Theorem~\ref{thm:shrinkage-variance}:
  \begin{equation}
    \log|\mathbf{S}| = \sum_{i = 1}^n \log \frac{\Sigma_{ii}}{\Psi_{ii}} \ge 0.
  \end{equation}
As before, let $\lambda_1,\ldots,\lambda_n$ denote the eigenvalues of $\mathbf{C}$ so that $\log|\mathbf{C}| = \sum_i \log\lambda_i$. From Jensen's inequality, we~have:
\begin{equation}
\sum_{i=1}^n \log \lambda_i
    \leq n\log\bigg[\tfrac{1}{n}\sum_{i=1}^n\lambda_i\bigg]
    = n\log\tfrac{1}{n}\,\text{trace}(\mathbf{C}) = 0,
\end{equation}
which proves eq.~(\ref{eq:logC}).
\end{proof}
To prove that $q$ underestimates the entropy of $p$, we need the following result which is important in its own right.
\begin{proposition}
The entropy gap between $p$ and $q$ is equal to the KL divergence minimized by FG-VI:
    \begin{equation}
    \label{eq:gapKL}
        \mathcal H(p) - \mathcal H(q) = {\rm KL}(q||p). 
    \end{equation}
\end{proposition}
\begin{proof}
    The identity follows by substituting the solution from eq.~(\ref{eq:Psi}) into the KL divergence in eq.~(\ref{eq:KL}). This yields the entropy gap, namely ${\rm KL}(q,p) = \frac{1}{2}\log|\boldsymbol\Sigma| - \frac{1}{2}\log|\boldsymbol{\Psi}|$,  computed in eq.~(\ref{eq:entropy-loss2}).
\end{proof}
It follows that FG-VI is minimizing the entropy gap between $p$ and $q$ when it targets a multivariate Gaussian. As suggested by the trade-off in Theorem~\ref{thm:trade-off}, however, the entropy gap can be minimized despite a large shrinkage in componentwise variances. 
The nonnegativity of the KL divergence in eq.~(\ref{eq:gapKL}) also leads to the other fundamental inequality of this section.
\begin{theorem}[Entropy gap] \label{thm:shrinkage-entropy}
The solution for FG-VI in eq.~(\ref{eq:Psi}) underestimates the entropy; that is,
\begin{equation}
\mathcal{H}(q) \leq \mathcal{H}(p),
\label{eq:entropy-shrinkage}
\end{equation}
and this inequality is strict if $\mathbf{\Sigma}$ is not purely diagonal.
\end{theorem}
An immediate implication of this theorem is that the shrinkage term in eq.~(\ref{eq:logS}) dominates the shrinkage-delinkage trade-off in Theorem~\ref{thm:trade-off}.
\begin{remark}
We see also from Theorem~\ref{thm:shrinkage-entropy} that $|\mathbf{\Psi}| \leq |\mathbf{\Sigma}|$. This inequality can be viewed as a multivariate analog of the result, in Theorem~\ref{thm:shrinkage-variance}, that $\Psi_{ii} \leq \Sigma_{ii}$.
\end{remark}
\subsection{Demonstration of the trade-off}
\label{sec:corr}
Figure~\ref{fig:entropy_loss} illustrates the shrinkage-delinkage trade-off in \mbox{FG-VI}, and how it is resolved, for multivariate Gaussian distributions with two types of covariance matrices:
 \begin{itemize}
 
       \item \textit{Squared exponential kernel:} 
       This type of covariance matrix arises in models involving Gaussian processes~\citep[e.g.][Chapter 2]{Rasmussen:2006}. For this example we sampled a random input ${\bf x} \sim \text{uniform}(0, 200)^n$ and set the covariance matrix via the kernel function 
       \mbox{$\Sigma_{ij} = \exp(-(x_i\!-\!x_j)^2/\rho^2)$}.
      We use the hyperparameter $\rho\!>\!0$ to vary the degree of correlation.
      
      \item \textit{Constant off-diagonal:} 
      The posterior distributions of models with exchangeable data~\citep[chapter 5]{Gelman:2013} can generate such covariance matrices, or at least covariance matrices whose subblocks have the described structure. For this example we set $\Sigma_{ii}\!=\!1$ along the diagonal and $\Sigma_{ij}\!=\!\varepsilon$ for all $i\!\neq\!j$, and we used the hyperparameter $\varepsilon\!>\!0$ to vary the degree of correlation.
  \end{itemize}
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
    Figure~\ref{fig:entropy_loss} plots the opposing contributions from the shrinkage and delinkage terms in eq.~(\ref{eq:entropy-loss}) using solid and dashed lines. In each panel, the difference between these curves reveals the degree to which FG-VI underestimates the entropy of the multivariate Gaussian distribution it is being used to approximate. 
    It can also be seen that FG-VI manages the shrinkage-delinkage trade-off differently for different types of covariance matrices. 
    While in the squared exponential kernel case the entropy gap is large, it is smaller when the covariance matrix has constant off-diagonal terms: there, the shrinkage and delinkage terms in eq.~(\ref{eq:entropy-loss}) are almost perfectly balanced.
    
    This last finding may come as a surprise in light of earlier results, shown in Figure~\ref{fig:shrinkage}, where the variational approximation is clearly too ``compact.'' But the two-dimensional projections in Figure~\ref{fig:shrinkage} are misleading.
    In higher dimensions, the approximating sphere of FG-VI gains more in volume than its target ellipse; this discrepancy arises because each added component is independent for $q$ but strongly correlated for~$p$. The overall effect is that the opposing terms in eq.~(\ref{eq:entropy-loss}) are nearly balanced. Hence even when FG-VI hardly underestimates the (per-component) entropy, it may still grossly underestimate the componentwise variance.
    This contrast becomes more acute in the asymptotic limit of $n$.
    
    \begin{theorem}  \label{thm:constant-off-diag}
        Suppose $\boldsymbol\Sigma$ has constant off-diagonal terms, $\varepsilon>0$.
        Then the per-component entropy gap vanishes in the limit $n\rightarrow\infty$, whereas every componentwise variance shrinks by a constant factor:
        \begin{align}
            \underset{n \to \infty} \lim \ \tfrac{1}{n} \left(\mathcal H(q) - \mathcal H(q)\right) &= 0. \\
            \underset{n \to \infty} \lim \ (\Psi_{ii}/\Sigma_{ii}) &= 1 - \varepsilon.
        \end{align}
      \end{theorem}
    
    The proof is given in Appendix~A. The theorem also shows that the average of the diagonal elements in the shrinkage matrix also converges to a constant factor:
        \begin{equation}
            \underset{n \to \infty} \lim \ \tfrac{1}{n} \mathrm{trace}({\bf S}) = (1 - \varepsilon)^{-1}.
        \end{equation}
        
    This example highlights the roots of FG-VI in mean-field approximations from statistical physics~\citep{Parisi:1988}. As is well known, the mean-field approximation for the free energy becomes exact in the limit $n\!\rightarrow\!\infty$ for certain spin systems with infinite-range interactions. The infinite-range interactions in these systems are analogous, for the Gaussian models we study here, to the assumption of constant off-diagonal terms in the covariance matrix~\citep[e.g][]{Mukherjee:2018, Margossian:2021}.
    An important takeaway is that factorized approximations can work well to estimate the entropy---to wit, minimizing the KL-divergence with FG-VI on a Gaussian target is equivalent to minimizing the entropy gap---but still fail to accurately compute the componentwise variances.
    This can become an important limitation as we apply VI beyond problems in statistical physics and more broadly to Bayesian modeling, where estimation of the variances is critical.
    
\section{Bounds on ${\bf log}|\mathbf{S}|$ and ${\bf log}|\mathbf{C}|$}
In the last section, we saw that the shrinkage-delinkage trade-off played out differently for different types of covariance matrices; we also proved certain asymptotic results that depended on the detailed structure of the covariance matrix (e.g., constant off-diagonal). 
In this section, we derive more general bounds on the terms in this trade-off that depend only
the problem dimensionality, $n$, and the condition number, $R$, of the correlation matrix, ${\bf C}$.
\subsection{Optimizations for upper bounds}
Consider the space of all correlation matrices with condition number $R$. We denote this space by the set
\begin{equation}
    \label{eq:CR}
    \mathcal{C}_R = \{\mathbf{C}\in\mathcal{S}^{n}_+\, |\, C_{ii}\!=\!1\ \forall i,\, \lambda_{\rm max}(\mathbf{C})\!=\! R\lambda_{\rm min}(\mathbf{C})\}.
\end{equation}
The set contains the intersection of those $n\!\times\! n$ matrices that are positive semidefinite (i.e., lying in the cone $\mathcal{S}_+^n$), whose diagonal elements are equal to unity, and whose largest eigenvalue is $R$ times larger than its smallest one.
If the condition number of the correlation matrix $\mathbf{C}$ is known to be $R$, then we can (in principle) compute the following upper bounds on the terms in eq.~(\ref{eq:entropy-loss}):
\begin{align}
\log|\mathbf{S}| &\leq \max_{\mathbf{C}\in\mathcal{C}_R}\left[\sum_{i=1}^n \log C_{ii}^{-1}\right],
\label{eq:boundS1} \\
\log|\mathbf{C}| &\leq \max_{\mathbf{C}\in\mathcal{C}_R}\left[\sum_{i=1}^n \log \lambda_i(\mathbf{C})\right].
\label{eq:boundC1}
\end{align} 
In eq.~(\ref{eq:boundS1}), we have used the fact from eq.~(\ref{eq:Sii}) that \mbox{$S_{ii} \!=\! C^{-1}_{ii}$}, while in eq.~(\ref{eq:boundC1}), we have written the determinant of a matrix as the product of its eigenvalues.
In practice, however, it is difficult to perform the optimizations over the set $\mathcal{C}_R$ in eq.~(\ref{eq:boundS1}-\ref{eq:boundC1}). Instead we consider a more tractable relaxation; the essential idea is to optimize over a larger set of matrices, one that is characterized only in terms of constraints on its eigenvalues. We denote this constrained set of eigenvalues by
\begin{equation}
\label{eq:LR}
\hspace{-1.75ex}\Lambda_R = \left\{\boldsymbol{\lambda}\!\in\!\mathbb R^n_+\, \bigg|\, \lambda_1\!\geq\!\ldots\geq\!\lambda_n=R\lambda_1,\, \sum_{i=1}^n \lambda_i \!=\! n\right\}\!.
\end{equation}
Note that the set ${\cal C}_R$ of correlation matrices is contained strictly within the set of matrices with eigenvalues in $\Lambda_R$. In particular, a matrix in ${\cal C}_R$ is constrained to have ones along its diagonal, while a matrix with eigenvalues in $\Lambda_R$ is only constrained to have a trace equal to $n$. With the above relaxation, we obtain the following upper bounds
on the terms $\log|\mathbf{S}|$ and $\log|\mathbf{C}|$ in eq.~(\ref{eq:entropy-loss}). 
\begin{proposition} 
Suppose that the correlation matrix $\mathbf{C}$ has condition number $R$. Then
\begin{align}
\log |\mathbf{S}| &\leq n \log \frac{1}{n}\!\left[\max_{\boldsymbol\lambda\in\Lambda_R}
  \sum_{i=1}^n \lambda_i^{-1}\right]
\label{eq:boundS2} \\
\log |\mathbf{C}| &\leq \max_{\boldsymbol\lambda\in\Lambda_R}\left[\sum_{i=1}^n \log \lambda_i\right].
\label{eq:boundC2}
\end{align}
\end{proposition}
\begin{proof}
The second bound is immediate from eq.~(\ref{eq:boundC1}) and the relaxation in eq.~(\ref{eq:LR}). 
For the first bound, recall that $S_{ii} = C^{-1}_{ii}$, and note from Jensen's equality that
\begin{equation*}
\tfrac{1}{n}\mbox{$\sum_i$} \log C_{ii}^{-1} \leq \log\tfrac{1}{n}\mbox{$\sum_i$}\, C^{-1}_{ii} = \log\left[\tfrac{1}{n}\mbox{$\sum_i$}\, \lambda_i^{-1}(\mathbf{C})\right].
\end{equation*}
The bound in eq.~(\ref{eq:boundS2}) follows from the above in concert with the relaxion in eq.~(\ref{eq:LR}).
\end{proof}
\subsection{Solutions from symmetry}
The optimizations over $\Lambda_R$ in eqs.~(\ref{eq:boundS2}--\ref{eq:boundC2}) have a great deal of structure that we can exploit to compute their solutions. We analyze each of these optimizations in turn.
\begin{lemma} \label{lemma:symmetry}
  Let $\boldsymbol\lambda\in\Lambda_R$ be the solution that maximizes the right side of eq.~(\ref{eq:boundS2}). Then at most one $\lambda_i$ is not equal to either $\lambda_1$ or~$\lambda_n$.
\end{lemma}
\begin{proof}
We prove the lemma by contradiction. Suppose there exists a solution with intermediate elements $\lambda_i$ and $\lambda_j$ that satisfy
$\lambda_1\! >\! \lambda_i\! >\! \lambda_j\! >\! \lambda_n$.
Consider the effect on this solution of a perturbation that adds some small amount $\delta\!>\!0$ to $\lambda_i$ and subtracts the same amount from $\lambda_j$. Note that for sufficiently small~$\delta$, this perturbation will not leave the set~$\Lambda_R$; however, it will {\it expand} the separation of $\lambda_i$ from~$\lambda_j$. As a result the objective $\sum_i \lambda_i^{-1}$ has a gain
\begin{equation}
  f(\delta) = \frac{1}{\lambda_i+\delta} - \frac{1}{\lambda_i} + \frac{1}{\lambda_j-\delta} - \frac{1}{\lambda_j}.
\end{equation}
Next we evaluate the derivative $f'(\delta)$ at $\delta=0$; doing so we find $f'(0) = \lambda_j^{-2}\! -\! \lambda_i^{-2} > 0$. But this yields a contradiction, because any solution must be maximal, and hence stationary (i.e., $f'(0)\!=\!0$), with respect to small perturbations.
\end{proof}
The above lemma greatly restricts the form of the solutions that we must consider for the optimization in eq.~(\ref{eq:boundS2}). The next lemma does the same for the optimization in eq.~(\ref{eq:boundC2}).
\begin{lemma} \label{lemma:symmetry2}
  Let $\boldsymbol\lambda\in\Lambda_R$ be the solution that maximizes the right side of eq.~(\ref{eq:boundC2}). Then $\lambda_i\!=\!\lambda_j$ whenever $1\!<\!i\!<\!j\!<\!n$.
\end{lemma}
\begin{proof}
We prove this lemma in similar fashion. Suppose there exists a solution
with intermediate elements $\lambda_i$ and $\lambda_j$ that satisfy
$\lambda_1\! \geq\! \lambda_i\! >\! \lambda_j\! \geq\! \lambda_n$.
Consider the effect on this solution of a perturbation that adds some small amount $\delta\!>\!0$ to $\lambda_j$ and subtracts the same amount from~$\lambda_i$. Again, for sufficiently small~$\delta$, this perturbation will not leave the set~$\Lambda_R$; however, it will {\it diminish} the separation of~$\lambda_i$ from~$\lambda_j$. As a result the objective $\sum_i \log\lambda_i$ has a gain
\begin{equation}
  g(\delta) = \log(\lambda_i-\delta) +\log(\lambda_j+\delta) -\log(\lambda_i) - \log(\lambda_j).
  \end{equation}
Evaluating the derivative, we find $g'(0) = \lambda_j^{-1}\! -\! \lambda_i^{-} > 0$. As before this yields a contradiction, because any solution must be maximal, and hence stationary (i.e., $g'(0)\!=\!0$), with respect to small perturbations.
\end{proof}
\begin{figure*}
    [width=3in]{figures/entropy_bound_n_10.pdf}
    [width=3in]{figures/entropy_bound_n_100.pdf}
    \caption{\textit{Upper bounds on the competing terms in the shrinkage-delinkage tradeoff, eq.~(\ref{eq:entropy-loss}), for a correlation matrix with condition number $R$. The actual values for these terms are shown for the correlation matrices introduced in section~\ref{sec:corr}. 
    Note that the entropy gap is minimal at $n\!=\!100$ for the case of a constant off-diagonal correlation matrix.
    }}
    \label{fig:entropy_bound}
\end{figure*}
Now let us consider, at a high level, how these lemmas simplify the optimizations in eqs.~(\ref{eq:boundS2}--\ref{eq:boundC2}).
The lemmas show that for each optimization, there exist {\it three} elements---the maximum element $\lambda_1$, the minimum element $\lambda_n$, and some intermediate element $\lambda_k$ for $1\!<\!k\!<\!n$---from which the remaining $n\!-\!3$ elements of the solution can be deduced by symmetry. Note also that any solution in $\Lambda_R$ must satisfy the {\it two additional} constraints that $\sum_i\lambda_i\!=\!n$ and $\lambda_1\!=\!R\lambda_n$. 
In Appendix B, we show that by exploiting these symmetries and constraints in concert, we can reduce the optimizations in eqs.~(\ref{eq:boundS2}--\ref{eq:boundC2}) to a sequence of one-dimensional problems for which we have closed-form solutions. 
    Figure~\ref{fig:entropy_bound} plots the bounds on the shrinkage and delinkage terms as a function of the condition number, $R$, for problems with dimensionalities $n\!=\!10$ (\textit{left}) and $n\!=\!100$ (\textit{right}). These bounds provide envelopes between which the actual values of the competing terms in eq.~(\ref{eq:entropy-loss}) must lie. To illustrate this, the figure also shows
    the corresponding values of these terms for the squared-exponential-kernel and constant-off-diagonal covariance matrices introduced in the previous section. (Notice that in this figure, unlike Figure~\ref{fig:entropy_loss}, these values are plotted against the condition number of the correlation matrix rather than the hyperparameters $\rho$ or $\varepsilon$.) 
    Using similar methods, it also possible to upper-bound the entropy gap and the trace of the shrinkage matrix (which reflects the average shrinkage in componentwise variance).
    The derivations of these additional bounds are relegated to Appendices~C and D.  
 
  \section{Non-Gaussian models} \label{sec:examples}
 Do our results extend in any way when FG-VI is applied to non-Gaussian models?
  In this section we suppose that~$p$ is a \textit{non-Gaussian} target with covariance~$\boldsymbol\Sigma$.
 Our previous analysis of FG-VI targets was based on the variance estimator,
  \begin{equation*}
      (\Psi_G)_{ii} := 1 / (\Sigma^{-1})_{ii},
  \end{equation*}
  
  and the corresponding shrinkage matrix with diagonal elements $(S_G)_{ii} = \Sigma_{ii} / (\Psi_G)_{ii}$.
  
  But neither $\mathbf{\Psi}_G$ nor $\mathbf{S}_G$ will be returned by FG-VI when it is applied to a non-Gaussian target with covariance $\boldsymbol\Sigma$. 
To explore these issues, we applied ADVI~\citep{Kucukelbir:2017} with a factorized Gaussian approximation to study the posterior distributions in several Bayesian models as well as one ``adversarial'' target (Table~\ref{tab:models}).
  These test targets represent a diversity of applications.
  The GLM and 8-schools models are taken from the model data base PosteriorDB \citep{Magnusson:2022}, while the disease map Gaussian process model and sparse kernel interaction model \citep{Agrawal:2019} are studied by \citep{Margossian:2020}.
  We also included a mixture of well-separated spherical Gaussians; for this target, the approximation by FG-VI collapses to one of the modes, so that FG-VI can underestimate the componentwise variances by an arbitrarily large amount (e.g., if the modes are widely separated). 
  Note that in all cases, before applying ADVI, we transformed any constrained (e.g., nonnegative) variables of the target distribution to be unconstrained variables over $\mathbb{R}$.
    \begin{table*}[!htb]
  \small
      \centering
      \renewcommand{\arraystretch}{1.1}
      \begin{tabular}{l r l r}
         \rowcolor{Gray} {\bf Call}  &  $n$ & {\bf Description} & $\frac{1}{n} \log|\boldsymbol \Sigma / \boldsymbol \Psi|$ \\
         \texttt{glm\_binomial} & 3 & General linear model with a binomial likelihood. &  0.291\\
         \rowcolor{Gray} \texttt{8schools\_nc} & 10 & Hierarchical model with a non-centered parameterization. & 0.011 \\
         \texttt{8schools\_pool} & 9 & Same model but with a small, fixed population variance value to enforce & 0.339 \\
         & & strong partial pooling and create a high posterior correlation. \\
         \rowcolor{Gray} \texttt{disease\_map} & 102 & Gaussian process model with Poisson likelihood. Applied to disease & 0.066 \\
        \rowcolor{Gray} & & map of Finland using 100 randomly sampled counties (out of 911). & \\
        \texttt{SKIM} & 305 & Sparse kernel interaction model, applied to a Prostate cancer microarray & 0.033 \\
        & & data set on a subset of 200 SNPS. \\
        \rowcolor{Gray} \texttt{Mixture} & 2 & Mixture of well-separated Gaussians with spherical covariance matrices. & 3.051
      \end{tabular}
      \caption{\textit{Non-Gaussian targets for numerical experiments.}}
      \label{tab:models}
  \end{table*}
  
  We estimated the posterior covariance using long runs of Markov chain Monte Carlo,
  specifically 16,000 draws using the software Stan \citep{Carpenter:2017}, except in the mixture example, where the covariance was calculated analytically.
  We then estimated (i) the shrinkage matrix, $\mathbf{S}$, when targeting the posterior and (ii) the shrinkage matrix,~$\mathbf{S}_G$, when targeting a Gaussian with the same covariance as the posterior.
  For non-Gaussian posteriors, we observe that FG-VI does \textit{not} always underestimate the componentwise variance; see Figure~\ref{fig:8schools_shrinkage}.
  On the other hand, for all models in Table~\ref{tab:models}, we see that $\frac{1}{n} \text{trace}(\mathbf{S})\! >\! 1$, meaning that the componentwise variances are underestimated \textit{on average} (Figure~\ref{fig:model_S}).
  In addition, for the Bayesian models, we observe that \mbox{$\text{trace}(\mathbf{S})\! \approx\! \text{trace}(\mathbf{S}_G)$}.
  The mixture target, however, provides a counter-example where $\text{trace}(\mathbf{S}) \gg \text{trace}(\mathbf{S}_G)$.
  
    
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
  
\begin{figure}[h]
\includegraphics[]{image_name}
\label{fig:label}
\end{figure}
  
 
  In addition to the shrinkage in componentwise variances, we would have also liked to evaluate the entropy gap in these models.
  It is easy to obtain an upper bound on this gap by observing that the Gaussian maximizes the entropy among all continuous distributions with a given covariance~$\boldsymbol \Sigma$~\citep{Cover:2006}. Thus we have
  \begin{equation}  \label{eq:non-gaussian-bound}
      \mathcal H(p) - \mathcal H(q) \le \tfrac{1}{2}(\log|\boldsymbol \Sigma| - \log|\boldsymbol \Psi|).
  \end{equation}
   We observed this upper bound to be positive for all the models in Table~\ref{tab:models}. But we also know that FG-VI can overestimate the entropy in non-Gaussian models: \citet{Turner:2011} demonstrated this for a mixture of largely overlapping 1-dimensional Gaussians.
  It is an open question to understand the general conditions under which FG-VI underestimates the entropy.
  We next note that our upper-bound on the entropy gap does not immediately apply to \eqref{eq:non-gaussian-bound}, because it is unclear how $\log |\boldsymbol \Psi|$ and $\log |\boldsymbol \Psi_G|$ compare.
   To evaluate the entropy gap empirically in a non-Gaussian model---as would be required to investigate this question further---it is necessary to estimate the normalizing constant of the posterior.
  Candidate methods for this, such as bridge sampling \citep[e.g][]{Gronau:2020, Meng:2002}, rely on a proposal distribution which (roughly) approximates the target.
  Typically, a Gaussian-like approximation is used for these proposals, but this is precisely the assumption we want to relax. In other words, we do not wish to compare a theory for Gaussian targets to an empirical benchmark which relies on a Gaussian approximation. We leave this issue to future work.
\section{Discussion}
In this paper we have shown that FG-VI underestimates the componentwise variance and joint entropy of a multivariate Gaussian distribution.
Furthermore we expressed the entropy gap as a trade-off between two competing terms and observed that it was equal to the KL divergence minimized by FG-VI.
Our analysis helps to understand why FG-VI can greatly underestimate the componentwise variances even when it effectively minimizes the entropy gap and KL divergence.
Our results also suggest that better estimates of variance may be obtained by changing the objective function or using a different family of approximations.
An open question is whether a shrinkage-delinkage tradeoff operates when FG-VI is applied to non-Gaussian targets.
For such targets, we have produced a counter-example showing that FG-VI can overestimate
a particular componentwise variance. On the other hand, we have observed that these variances are underestimated on average, and moreover that the shrinkage term, $\log |{\bf S}|$, remains positive.
It requires further investigation to make more general statements about the entropy gap.
Finally, it would also be interesting to extend our analysis beyond FG-VI---for instance to approximations based on a diagonal plus low-rank covariance matrix, rather than a strictly diagonal one \citep[e.g][]{Zhang:2022}. 
\section{Acknowledgments}
We thank David Blei, Bob Carpenter, Justin Domke and Chirag Modi for feedback on this manuscript.
\appendix
\section*{Appendix}
In appendix~\ref{app:cov}, we provide further details on FG-VI when the covariance matrix has constant off-diagonal terms. Next appendix~\ref{app:alg} provides an algorithm to efficiently compute bounds on the shrinkage and delinkage terms, using techniques developped in section 4.
In appendix~\ref{app:traceS}, we show how to derive bounds on the trace of the shrinkage matrix.
In appendix~\ref{app:KL}, we obtain an upper bound on the KL divergence---equivalently the entropy gap---between $q$ and~$p$.
\appendix
\section{FG-VI for covariance with constant off-diagonal terms}
\label{app:cov}
Let $p$ be a multivariate Gaussian distribution over $\mathbb{R}^n$ with covariance matrix $\boldsymbol\Sigma$, and let $q$ be the solution of FG-VI with diagonal covariance matrix $\boldsymbol\Psi$, where
\begin{equation}
    \Psi_{ii} = \frac{1}{\Sigma^{-1}_{ii}}
\label{eq:psi}
\end{equation} as in eq.~(4). The elements of the correlation matrix are related to those of the covariance matrix by
\begin{equation}
    C_{ij}=\frac{\Sigma_{ij}}{\sqrt{\Sigma_{ii}\Sigma_{jj}}}.
\label{eq:corr}
\end{equation} 
In this section we assume that the correlation matrix has constant off-diagonal terms, and we use $\varepsilon\in[0,1)$ to denote the value of these terms. Note that we require $\varepsilon\geq 0$ since three or more random variables cannot all be mutually anti-correlated. Also we require $\varepsilon\!<\!1$ since otherwise $\mathbf{C}$ (and hence $\boldsymbol\Sigma$) would not be positive-definite.
We now prove Theorem~3.5, broken up into a statement about the estimated variance (Proposition~\ref{prop:asymptotic-shrinkage}) and a statement about the entropy gap (Proposition~\ref{prop:asymptotic-entropy}).
\begin{proposition} \label{prop:asymptotic-shrinkage}
    If the correlation matrix in eq.~(\ref{eq:corr}) has constant off-diagonal terms, then the solution for FG-VI in eq.~(\ref{eq:psi}) obeys the following limits:
\begin{eqnarray}
   \underset{n \to \infty}{\lim} \Psi_{ii} & = & (1\! -\! \varepsilon)\Sigma_{ii} \\
    \underset{\varepsilon \to 1}{\lim} \Psi_{ii} & = & 0 \\
    \underset{n \to \infty}{\lim} \frac{1}{n} \mathrm{trace}({\bf S}) & = & \frac{1}{1 - \varepsilon}
 \end{eqnarray}
 where $\varepsilon\in[0,1)$ denotes the value of $C_{ij}$ for $i\neq j$.
\end{proposition}
\begin{proof}
Let $\mathbf{1}\in\mathbb{R}^n$ denote the vector of all ones. Then the correlation matrix can be written as
 \begin{equation}
      \mathbf{C} = (1 - \varepsilon) \mathbf{I} + \varepsilon {\bf 1} {\bf 1}^\top.
      \label{eq:corr-epsilon}
  \end{equation}
  One can verify by direct substitution that the inverse correlation matrix has elements
  \begin{equation}
      \mathbf{C}^{-1} = \tfrac{1}{1 - \varepsilon}\left[{\bf I} - \tfrac{\varepsilon}{1 + (n - 1) \varepsilon} {\bf 1}{\bf 1}^\top\right].
   \label{eq:Cinv}
  \end{equation}
Recall from eq.~(15) that $\Psi_{ii} = \Sigma_{ii}/C_{ii}^{-1}$. With some algebra, it follows from eq.~(\ref{eq:Cinv}) that
\begin{equation}
    \Psi_{ii} = \left[\frac{(1-\varepsilon)(1+(n\!-\!1)\varepsilon)}{1+(n\!-\!2)\varepsilon}\right]\Sigma_{ii}.
    \label{eq:psi-epsilon}
\end{equation}
It is straightforward to take the limits of eq.~(\ref{eq:psi-epsilon}) as $n\rightarrow\infty$ or $\varepsilon\rightarrow 1$, and these limits yield the results of the proposition.
Finally, recalling that $\text{trace}({\bf S}) = \sum_{i = 1}^n \Sigma_{ii} / \Psi_{ii}$, we immediately get
\begin{equation}
    \underset{n \to \infty}{\lim} \frac{1}{n} \text{trace}({\bf S}) = \frac{1}{1 - \varepsilon}.
\end{equation}
\end{proof}
 \begin{proposition}  \label{prop:asymptotic-entropy}
     If the correlation matrix in eq.~(\ref{eq:corr}) has constant off-diagonal terms, then the per-component entropy gap from FG-VI vanishes in the limit $n\rightarrow\infty$; that is,
      \begin{equation}
          \underset{n \to \infty}{\lim} \ \frac{1}{n} \left [ \mathcal H(p) - \mathcal H(q) \right ] = 0.
      \end{equation}
  \end{proposition}
\begin{proof}
Recall from Theorem 3.2 of the main paper that the entropy gap for FG-VI is given by
\begin{equation}
    \mathcal{H}(p)-\mathcal{H}(q) = \tfrac{1}{2}\log|\mathbf{S}| + \tfrac{1}{2}\log|\mathbf{C}|,
    \label{eq:gap}
\end{equation}
where $\mathbf{S}$ is the diagonal shrinkage matrix with elements $S_{ii} = \Sigma_{ii}/\Psi_{ii}$. We consider each term on the right side of this equation in turn. It follows at once from eq.~(\ref{eq:psi-epsilon}) that
\begin{equation}
\log|\mathbf{S}| = n\left[\log\frac{1+(n\!-\!2)\varepsilon}{(1\!-\!\varepsilon)(1+(n\!-\!1)\varepsilon)}\right],
\label{eq:logS}
\end{equation}
where $\varepsilon\!>\!0$ denotes the amount of off-diagonal correlation. Next we show how to evaluate $\log|\mathbf{C}|$. From eq.~(\ref{eq:corr-epsilon}), we rewrite the correlation matrix as
   \begin{equation}
        \mathbf{C} = (1\! -\! \varepsilon) {\bf I} + n \varepsilon \left (\tfrac{1}{\sqrt n} {\bf 1} \right) \left (\tfrac{1}{\sqrt n} {\bf 1} \right)^T.
        \label{eq:eigC}
    \end{equation}
    
    Note that the second term on the right side of eq.~(\ref{eq:eigC}) is a rank-one matrix whose one nonzero eigenvalue is equal to $n\varepsilon$. By adding the first term---which is a multiple of the identity matrix---we obtain a new matrix whose eigenvalues are shifted by a uniform amount. It follows that this new matrix (namely, $\mathbf{C}$) has $n\!-\!1$ eigenvalues at $1\!-\!\varepsilon$ and one eigenvalue at $1+(n\!-\!1)\varepsilon$, so that
    \begin{equation}
        \log|\mathbf{C}| = (n\!-\!1)\log(1\!-\!\varepsilon) + \log (1 + (n\!-\!1) \varepsilon).
        \label{eq:logC}
    \end{equation}
The entropy gap is related to the sum of $\log|\mathbf{S}|$ and $\log|\mathbf{C}|$ by eq.~(\ref{eq:gap}). Adding the results in eq.~(\ref{eq:logS}) and eq.~(\ref{eq:logC}), we find that
\begin{align}
\log|\mathbf{S}| + \log|\mathbf{C}|
    &=  -\log(1\! -\! \varepsilon) + \log (1 + (n\! -\! 1) \varepsilon) \nonumber \\
    &\mbox{\hspace{2ex}}\hspace{3ex} +\ n \log \left [ \tfrac{1 + (n - 2) \varepsilon}{1 + (n - 1) \varepsilon} \right].
    \label{eq:sum}
\end{align}
Note that the first term on the right side is $\mathcal O(1)$, the second term is $\mathcal O(\log n)$, and the third term can be 
written as
    \begin{equation}
        n \log \left [ \tfrac{1 + (n - 2) \varepsilon}{1 + (n -1) \varepsilon} \right]
          = n \log \left [ 1 - \tfrac{\varepsilon}{1 + (n - 1) \varepsilon}  \right].
    \end{equation}
    
    For large $n$, the log term in this equation is $\mathcal O(\frac{1}{n})$ so that the entire expression is $\mathcal O(1)$. From eq.(\ref{eq:sum}), it therefore follows that the entropy gap in eq.~(\ref{eq:gap}) is $\mathcal O(\log n)$. Dividing by $n$, we see that the per-component entropy gap vanishes in the limit $n\rightarrow\infty$, thereby completing the proof.
  \end{proof}
  
  
  From \eqref{eq:sum}, we also see that the entropy gap becomes infinite as $\varepsilon \to 1$ (for fixed $n$).
  Additional limits can be considered with respect to both $\varepsilon$ and $n$, but we do not pursue those here.
  
  \section{Solutions for the bounds on the shrinkage and delinkage terms}
  \label{app:alg}
  By exploiting the symmetries that we proved in Section 4.2, we can efficiently compute bounds on the terms $\log |\mathbf{S}|$ and $\log|\mathbf{C}|$; these are the terms that arise, respectively, from the effects of shrinkage and delinkage.
  \subsection{Upper bound on $\log |{\bf S}|$} 
  
  First we show how to compute the upper bound on $\log |\mathbf{S}|$. Recall that to do so, we must solve the optimization problem
  
  \begin{equation}
      \max_{\boldsymbol\lambda\in\Lambda_R} \sum_{i=1}^n \lambda_i^{-1}.
      \label{eq:optS}
  \end{equation}
  
 From Lemma 4.2, we know that all the elements of the solution assume the edge values of $\lambda_1$ or $\lambda_n$
  save for at most one which we denote $\lambda_k$. At a high level, we solve the optimization by
 exhaustively computing the optimal solution for each candidate value of $k \in \{1,\ldots, n\}$,
  then choosing the particular value of $k$ whose solution maximizes the overall objective function.
  It remains only to show how to compute the solution for a particular candidate value of $k$.
  Recall the constraints that $\sum_{i = 1}^n \lambda_i = n$ and $\lambda_1 = R \lambda_n$.
 It follows that
  \begin{equation}
      \lambda_k = n - \left [ (k - 1) R + n - k \right] \lambda_n.
  \end{equation}
  
  Using the constraints to eliminate $\lambda_1$ and $\lambda_k$, we can write the
 objective function entirely in terms of $\lambda_n$. In this way we find
  {\small
  \begin{equation} \label{eq:objective-simple}
      \sum_{i = 1}^n \lambda_i^{-1} = \frac{1}{n - \left [ R (k - 1) + n - k \right] \lambda_n} + \frac{(k - 1)}{R \lambda_n} + \frac{n - k}{\lambda_n}.
  \end{equation}
  }
  
Crucially, we also need to enforce the boundary conditions $\lambda_n \le \lambda_k \le \lambda_1$, or equivalently
  
  \begin{equation} \label{eq:boundary}
      \frac{n}{R k + n - k} \le \lambda_n \le \frac{n}{R (k - 1) + n - k + 1}.
  \end{equation}
  
  Note that the simplified objective in \eqref{eq:objective-simple} for fixed $k$ is convex in $\lambda_n$; hence the maximizer must lie at one of the boundary values in \eqref{eq:boundary}. By computing the objective for each boundary value of $\lambda_k$, we find the optimal solution for this candidate value of~$k$. Finally, we obtain the overall solution to eq.~(\ref{eq:optS}) by considering all $n$ candidate values of $k$ and choosing the best one.
\subsection{Upper Bound on $\log |{\bf C}|$}
Next we show how to compute the upper bound on $\log|\mathbf{C}|$. Recall that to do so,
we must solve the optimization problem
  \begin{equation}
      \max_{\boldsymbol\lambda\in\Lambda_R}\left[\sum_{i=1}^n \log \lambda_i\right].
  \end{equation}
  
  From Lemma~4.3, we know that all eigenvalues other than~$\lambda_1$ and $\lambda_n$ must have the same value; we denote this value  by~$\lambda_k$.
  From the constraint $\sum_{i = 1}^n \lambda_i = n$, it follows that
  \begin{equation}
      \lambda_k = \frac{n - (1 + R) \lambda_n}{n - 2}.
  \end{equation}
  Again, using the constraints to eliminate $\lambda_1$ and $\lambda_k$, we can write the objective function entirely in terms of $\lambda_n$. In this way we find
  \begin{equation}
      \sum_{i=1}^n \log \lambda_i = (n\! -\! 2) \log \frac{n - (1 + R) \lambda_n}{n - 2} + \log R \lambda_n + \log \lambda_n.
      \label{eq:concave}
  \end{equation}
  
This objective is concave in $\lambda_n$, so we can locate the maximum by setting its derivative with respect to $\lambda_n$ equal to zero. Some straightforward algebra shows that this derivative vanishes when
  \begin{equation}
     
      \lambda_n = \frac{2}{1+R}.
  \end{equation}
  
Finally we need to check that this solution does not violate the boundary conditions of the problem; in particular, we require that \mbox{$\lambda_n \ge \lambda_k \ge R \lambda_n$}, or equivalently that
  \begin{equation}
      \frac{n}{1 + R (n - 1)} \le \lambda_n \le \frac{n}{n - 1 + R}.
  \end{equation}
  
These conditions are always satisfied for $n\geq 3$.
  Hence we obtain an analytical solution for the upper bound on $\log|\mathbf{C}|$.
  Finally, note that while the solution for $\lambda_n$ does not depend on $n$,
  the optimized objective function does depend on $n$ through eq.~(\ref{eq:concave}).
 Algorithm~\ref{alg:bounds} provides an implementation of the above-described method.
\begin{algorithm}[!b]
    \DontPrintSemicolon
    
    \caption{Upper bounds on $\log|\mathbf{S}|$ and $\log|\mathbf{C}|^{-1}$}
    \label{alg:bounds}
    \setstretch{1}
    {\bf Input:} $R, n$ \;\;
    \SetKwFunction{Fh}{ObjF}
    \SetKwProg{Fn}{Function}{:}{}
    \Fn{\Fh{$\lambda_n$, $k$}}{
        \KwRet $ \left (n - k + \frac{k - 1}{R} \right)\frac{1}{\lambda_n} + \frac{1}{n - [R(k - 1) + n - k] \lambda_n}$
    } \;
    \For {$k$ in $\{2, \cdots, n - 1\}$} {
      $\lambda_a \leftarrow \frac{n}{Rk + n - k}$ \;
      $\lambda_b \leftarrow \frac{n}{R(k-1) + n-k + 1}$ \;
      $F_k \leftarrow \text{max}(\text{\texttt{ObjF}} \;(\lambda_a), \text{\texttt{ObjF}}(\lambda_b))$ \;
      {\bf if} ($k = 1$) $F \leftarrow F_k$ \;
      {\bf else} $F \leftarrow \text{max}(F, F_k)$\;
    }
    $U_s \leftarrow n\log(F/n)$\;\;
    
    $\lambda_n \leftarrow \frac{2}{1+R}$ \;
    $U_c \leftarrow  \log \frac{1}{\lambda_n} + \log\frac{1}{R \lambda_n} + (n\!-\!2) \log \frac{n-2}{n-(1+R)\lambda_n}$\; \;
    {\bf Return:} $U_s$, $U_c$\;
    
  \end{algorithm}
  
\section{Bounds on the average variance shrinkage}
\label{app:traceS}
We can also derive bounds on the {\it average} shrinkage in componentwise variance in terms of the problem dimensionality, $n$, and the condition number, $R$, of the correlation matrix. The average in this case is performed over the different components of $\mathbf{z}$. Recall that the shrinkage in each componentwise variance is given by $S_{ii} = \Sigma_{ii}/\Psi_{ii}$. Hence we can also express this bound in terms of the trace of the shrinkage matrix, $\text{trace}({\bf S})$.
\begin{proposition}
Suppose that the correlation matrix $\mathbf{C}$ has condition number $R$. Then the solution for FG-VI in section 2 satisfies
\begin{equation}
 \min_{\lambda\in\Lambda_R}\sum_{i=1}^n \lambda_i^{-1} \leq {\rm trace}(\mathbf{S}) \leq \max_{\lambda\in\Lambda_R}\sum_{i=1}^n \lambda_i^{-1},
\label{eq:shrink-eig-bound}
\end{equation}
\end{proposition}
where $\Lambda_R$ is the set defined in section 4.
\begin{proof}
We showed in the proof of Theorem 3.1 that 
\begin{equation}
    S_{ii} = \frac{\Sigma_{ii}}{\Psi_{ii}}= C_{ii}^{-1}.
\end{equation}
It follows that ${\rm trace}(\mathbf{S}) = {\rm trace}(\mathbf{C}^{-1}) = \sum_i \lambda_i^{-1}$, where $\lambda_1,\ldots,\lambda_n$ are the eigenvalues of $\mathbf{C}$. The bound then follows from the relaxtion from the set $\mathcal{C}_R$ to the set $\Lambda_R$ in section~4.
\end{proof}
\subsection{Lower bound on $\text{trace}({\bf S})$}
The optimization implied by eq.~(\ref{eq:shrink-eig-bound}) is convex, since both the set $\Lambda_R$ and the objective function $\sum_i \lambda_i^{-1}$ are convex. In fact, this bound can be evaluated in closed form by using similar methods as in section 4.2.
  \begin{lemma} \label{lemma:trace-symmetry1}
  Let $\boldsymbol\lambda\in\Lambda_R$ be the solution that minimizes the left side of eq.~(\ref{eq:shrink-eig-bound}). Then $\lambda_i\!=\!\lambda_j$ whenever $1\!<\!i\!<\!j\!<\!n$.
\end{lemma}
\begin{proof}
This proof follows the same argument as the proof (by contradiction) for Lemma 4.3. 
Suppose there exists a solution with intermediate elements $\lambda_i$ and $\lambda_j$ that satisfy
$\lambda_1\! \geq\! \lambda_i\! >\! \lambda_j\! \geq\! \lambda_n$.
Consider the effect on this solution of a perturbation that adds some small amount $\delta\!>\!0$ to~$\lambda_j$ and subtracts the same amount from~$\lambda_i$. For sufficiently small~$\delta$, this perturbation will not leave the set~$\Lambda_R$; however, it will {\it diminish} the separation of~$\lambda_i$ from~$\lambda_j$. As a result the objective $\sum_k (1/\lambda_k)$ experiences a change
\begin{equation}
  g(\delta) = \frac{1}{\lambda_i-\delta} -\frac{1}{\lambda_i} +\frac{1}{\lambda_j+\delta} - \frac{1}{\lambda_j}.
  \end{equation}
Evaluating the derivative, we find $g'(0) = \lambda_i^{-2}\! -\! \lambda_j^{-2} < 0$, so that the objective function is decreased for some $\delta>0$. As before this yields a contradiction, because any solution must be maximal, and hence stationary (i.e., $g'(0)\!=\!0$), with respect to small perturbations.
\end{proof}
With the above lemma, the $n$-dimensional optimization over $\Lambda_R$ can be reduced to a one-dimensional optimization that can be solved in closed form. The methods are identical to those in the previous appendix.
First we rewrite the constraint, $\lambda_n \le \lambda_k \le R \lambda_n$, as
\begin{equation}  \label{eq:trace-constraint}
    \frac{n}{R(n - 1) + 1} \le \lambda_n \le \frac{n}{R  + n - 1}.
\end{equation}
Since the minimization problem is convex, a minima can be found at a stationary point of the objective function
\begin{equation}  \label{eq:trace-objective}
    \sum_{i = 1}^n \frac{1}{\lambda_i} = \frac{(n - 2)^2}{n - (1 + R) \lambda_n} + \frac{1}{\lambda_n} + \frac{1}{R \lambda_n},
\end{equation}
which now only depends on $\lambda_n$.
Differentiating and setting to 0, we obtain the root-finding problem,
\begin{equation}
    \left [R(n - 2)^2 - (1 + R)^2 \right] \lambda^2_n + 2n (1 + R) \lambda_n - n^2 = 0,
\end{equation}
which can be solved exactly.
It remains to check whether the roots violate the constraints in~\eqref{eq:trace-constraint}, and pick the non-offending root which maximizes the objective in~\eqref{eq:trace-objective}.
If both roots violate the constraints then, by convexity of the problem, the solution must lie at one of the boundary terms in~\eqref{eq:trace-constraint}.
\subsection{Upper bound on \text{trace}(S)}
A similar approach gives us an upper bound on $\text{trace}({\bf S})$.
In fact, we have already solved the problem of maximizing the right side of~\eqref{eq:shrink-eig-bound} when upper-bounding $\log |{\bf S}|$.
It remains to apply the same strategy.
\section{Tighter upper bound on entropy gap}
\label{app:KL}
In Proposition 4.1 we derived separate upper bounds on the individual terms $\log|\mathbf{S}|$ and $\log|\mathbf{C}|$. One upper bound on the entropy gap (or equivalently, on ${\rm KL}(q,p)$) is obtained simply by adding these separate bounds. However, a tighter upper bound is obtained by replacing the separate optimizations in Proposition 4.1 by a single joint optimization:
\begin{equation}
{\rm KL}(q,p)\ \leq\ 
  \frac{1}{2}\max_{\boldsymbol\lambda\in\Lambda_R}\left[
    n \log \frac{1}{n}
  \sum_{i=1}^n \lambda_i^{-1} + \sum_{i=1}^n \log \lambda_i\right].
\label{eq:combined-opt}
\end{equation}
In this appendix we sketch how to solve this optimization and evaluate this bound in closed form. The first step is to make the change of variables,
\begin{equation}
    \omega_i = \frac{\lambda_i^{-1}}{\sum_{j=1}^n \lambda_j^{-1}},
\end{equation}
and to translate the domain of optimization accordingly. Under this change of variables, the original domain $\Lambda_R$ in section 4 is mapped onto the set
\begin{equation}
    \Omega_R = \left\{\boldsymbol\omega\in \mathbb R_+^n\, |\,\omega_n\geq\ldots\geq\omega_1 = \frac{1}{R} \omega_n, \sum_{i=1}^n\omega_i = 1\right\}.
\end{equation}
Likewise, a little algebra shows that the optimization in eq.~(\ref{eq:combined-opt}) is equivalent to the following:
\begin{equation}
{\rm KL}(q,p)\ \leq\ 
  \frac{1}{2}\max_{\boldsymbol\omega\in\Omega_R}\left[
    \sum_{i=1}^n \log\frac{1}{\omega_i} - n\log n\right].
\label{eq:omega-opt}
\end{equation}
Now we can make a similar argument as in the proof of Lemma 4.2 to simplify this optimization.
\begin{lemma} 
\label{lemma:symmetry-omega}
  Let $\boldsymbol\omega\in\Omega_R$ be the solution that maximizes the right side of eq.~(\ref{eq:omega-opt}). Then at most one $\omega_i$ is not equal to either $\omega_1$ or~$\omega_n$.
\end{lemma}
\begin{proof}
We prove the lemma by contradiction. Suppose there exists a solution with intermediate elements $\omega_i$ and $\omega_j$ that satisfy
$\omega_n\! >\! \omega_i\! >\! \omega_j\! >\! \omega_1$.
Consider the effect on this solution of a perturbation that adds some small amount $\delta\!>\!0$ to $\omega_i$ and subtracts the same amount from $\omega_j$. Note that for sufficiently small~$\delta$, this perturbation will not leave the set~$\Omega_R$; however, it will {\it expand} the separation of $\omega_i$ from~$\omega_j$. As a result the objective in eq.~(\ref{eq:omega-opt}) changes by an amount
\begin{equation}
  f(\delta) = \frac{1}{2}\left[\log\frac{1}{\omega_i\!+\!\delta} - \log\frac{1}{\omega_i} + \log\frac{1}{\omega_j\!-\!\delta} - \log\frac{1}{\omega_j}\right].
\end{equation}
Next we evaluate the derivative $f'(\delta)$ at $\delta=0$; doing so we find $f'(0) = \omega_j^{-1}\! -\! \omega_i^{-1} > 0$, so that the objective is increased for some $\delta>0$. But this yields a contradiction, because any solution must be maximal, and hence stationary (i.e., $f'(0)\!=\!0$), with respect to small perturbations.
\end{proof}
With the above lemma, we can reduce the $n$-dimensional optimization over $\Omega_R$ to a one-dimensional optimization that can be solved in closed form; the methods are identical to those in the previous appendix.
In details, let $\omega_k$ be the one variable which (potentially) does not go to $\omega_1$ or $\omega_n$.
Given $\sum_{i = 1}^n \omega_i = 1$,
\begin{equation}
    \omega_k = 1 - (k - 1 + R(n -k)) \omega_1.
\end{equation}
The objective is then
\begin{eqnarray}
    \sum_{i = 1}^n \log \frac{1}{\omega_i} =&  - (k - 1) \log \omega_1 - (n - k) \log R\omega_1 \nonumber \\ & - \log (1 - [k - 1 + R(n -k) \omega_1]). \ 
\end{eqnarray}
Since we are trying to maximize a convex function, the solution does not lie at a stationary point, rather at a boundary set by the constraint, $\omega_1 \le \omega_k \le \omega_n$, or equivalently
\begin{equation}
    \frac{1}{k - 1 + R(n - k + 1} \le \omega_1 \le \frac{1}{k + R(n -k)}.
\end{equation}
It remains to test each candidate boundary for each choice of $k$ to obtain a maximizer.
  
"
